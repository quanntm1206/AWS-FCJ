[{"uri":"https://quanntm1206.github.io/AWS-FCJ/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: ‚ÄúAI-Driven Development Life Cycle: Reimagining Software Engineering‚Äù Event Objectives To explore the significant transformation in software development propelled by generative AI. To introduce the AI-Driven Development Life Cycle (AI-DLC) along with its fundamental concepts. To demonstrate the capabilities of Kiro and Amazon Q Developer. Speakers Toan Huynh ‚Äì Specialist SA, PACE My Nguyen - Sr. Prototyping Architect, Amazon Web Services - ASEAN Key Highlights The session centered on the AI-DLC framework, a system where AI orchestrates the development process‚Äîincluding planning, task decomposition, and architectural suggestions‚Äîwhile developers retain ultimate responsibility for validation, decision-making, and oversight. - AI-DLC Core Concept: This approach is distinctly human-centric, with AI acting as a collaborator to augment developer capabilities. This partnership leads to accelerated delivery, reducing cycles from weeks or months to merely hours or days.\n- AI-DLC Workflow: The process follows an iterative loop comprising AI Tasks (such as creating and implementing plans or seeking clarification) and Human Tasks (providing clarification and implementing plans). In this workflow, the AI repeatedly asks clarifying questions and only proceeds with solutions following human validation.\n- AI-DLC Stages: The lifecycle is divided into Inception, Construction, and Operation, with each stage establishing a richer context for the subsequent one:\nInception: Involves building context, elaborating on intent through User Stories, and planning via Units of Work.\nConstruction: Encompasses domain modeling, code generation and testing, the addition of architectural components, and deployment using Infrastructure as Code (IaC) and tests.\nOperation: Focuses on production deployment and incident management.\n- Challenges AI-DLC Aims to Solve:\nScaling AI development: AI coding tools often struggle when applied to complex projects.\nLimited control: Current tools make collaborating with and managing AI agents difficult.\nCode quality: Maintaining strict quality control when transitioning from a proof-of-concept to production remains a significant hurdle.\nDeep Dive: Kiro - The AI IDE for Prototype to Production Kiro is an AI-first Integrated Development Environment (IDE) designed to support the AI-DLC, with a specific focus on Spec-driven development. - Spec-driven Development: Kiro transforms high-level prompts (e.g., \u0026ldquo;I want to create a chat application like Slack\u0026rdquo;) into precise requirements (requirements.md), system designs (design.md), and discrete tasks (tasks.md). This fundamentally shifts development from \u0026ldquo;vibe coding\u0026rdquo; to a structured, traceable process where developers collaborate with Kiro on these specifications, which then serve as the single source of truth.\n- Agentic Workflows: Kiro\u0026rsquo;s AI agents execute specifications while ensuring the human developer remains in control. Key features include:\n+ Implementation Plan: Kiro generates a comprehensive implementation plan containing start tasks and sub-tasks (such as \u0026ldquo;Implement user registration and login endpoints\u0026rdquo; or \u0026ldquo;Implement JWT middleware\u0026rdquo;), linking them directly to specific requirements for validation.\n+ Agent Hooks: These hooks delegate tasks to AI agents triggered by events like a \u0026ldquo;file save.\u0026rdquo; They execute autonomously in the background based on predefined prompts, helping to scale productivity by generating documentation, writing unit tests, or optimizing code performance.\nKey Takeaways - AI Ensures Production Readiness: By creating detailed design documents‚Äîsuch as data flow diagrams and API contracts‚Äîand generating unit tests before code is written, Kiro ensures that AI-generated code is maintainable and production-ready, rather than just a quick prototype.\n- Human Control via Artifacts: Developers maintain control not by writing the majority of the code, but by validating and refining the artifacts‚Äîspecifically the requirements, design, and task plan‚Äîbefore AI agents proceed with implementation.\nApplying to Work - Integrate Amazon Q Developer/Similar Tools: I intend to integrate AI coding assistants into my academic projects to automate boilerplate code and routine tasks, thereby boosting overall productivity.\n- Focus on High-Value Tasks: By allowing AI to handle undifferentiated heavy lifting, I can dedicate my time to mastering higher-value, creative activities such as Domain Modeling and Architectural Design, which remain crucial human-centric elements in the Construction phase.\nEvent Experience Attending the AI-Driven Development Life Cycle: Reimagining Software Engineering event offered a fascinating glimpse into the future of software development. It became evident that Generative AI is not merely a coding assistant; it is poised to become the core orchestrator of the entire development process. The session was well-structured, progressing logically from the overarching concept of AI-DLC to specific demonstrations of Amazon Q Developer and Kiro. The Kiro demo was particularly impactful, illustrating how a single text prompt can be transformed into a comprehensive, executable, and traceable development plan directly within the IDE.\nLessons learned The three primary challenges associated with current AI development‚Äîscaling, limited control, and code quality‚Äîmade the structured, human-validated approach of AI-DLC appear both highly necessary and well-conceived. Some event photos "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Democratizing Quantum Resources: University of Michigan and AWS The University of Michigan is breaking new ground in quantum technology accessibility through an innovative collaboration with Amazon Web Services (AWS). Led by Dr. Zheshen Zhang, this partnership aims to transform the university‚Äôs QREAL (Quantum Receiver Enhanced by Adaptive Learning) platform‚Äîfunded by the National Science Foundation (NSF)‚Äîinto a cloud-accessible quantum testbed.\n‚ÄúThe quantum community faces a significant challenge‚Äîall quantum resources are spread out and fragmented,‚Äù explained Dr. Zhang. ‚ÄúResearch groups developing quantum computer units, quantum interconnects, transduction sensors are scattered globally. Advancing quantum information science and technology requires a unified framework capable of connecting these different functionalities.‚Äù\nThe solution uses AWS serverless technology, including AWS Fargate for container orchestration, AWS Lambda for serverless computing, and Amazon Relational Database Service (Amazon RDS) for database management. Through these AWS cloud services, users can remotely send instructions to configure, connect, and route information between different quantum hardware components.\nTransforming quantum education The project addresses two critical challenges in quantum education:\nGrasping Concepts: It helps students grasp counterintuitive quantum concepts through interactive demonstrations using real experimental data.\n‚ÄúUnlike traditional disciplines, quantum concepts like superposition and entanglement are highly counterintuitive,‚Äù noted Dr. Zhang. ‚ÄúOur demos visualize these concepts using pre-processed data from actual quantum experiments.‚Äù\nBridging Theory and Practice: Traditional quantum education focuses heavily on theoretical concepts, often disconnected from experimental reality. The testbed‚Äôs virtual labs give students hands-on experience with physical quantum systems from anywhere.\nAccelerating research through unified access The testbed addresses a fundamental challenge in quantum research by creating a fabless model for the quantum industry.\n‚ÄúThink of the semiconductor industry,‚Äù explains Dr. Zhang. ‚ÄúCompanies like NVIDIA and AMD don‚Äôt run their own foundry facilities‚Äîthey outsource manufacturing to commercial foundries. We want to develop something similar for the quantum industry.‚Äù\nThis approach enables researchers from diverse backgrounds to advance quantum innovation:\nComputer scientists: Can access physical quantum hardware instead of relying solely on simulations (e.g., Dr. Jianqing Liu‚Äôs team at NC State studying QOS for entanglement routing). Quantum physicists: Can develop new sensing modules by configuring connections between different devices, such as entangled-light sources and AI processing units. Engineers: Can prototype and validate fundamental functionalities before transitioning to larger-scale integration and packaging with industrial partners. The testbed collects and assembles data in a unified library, which is then disseminated to the broader research community. This collaborative approach helps break down barriers between different disciplines‚Äîphysics, computer science, electrical engineering, materials science, and chemistry.\nFuture developments Looking ahead, the project will expand to include new quantum resources, such as squeeze-light sources (technology similar to what‚Äôs used in LIGO). The platform will also fully integrate with AWS Braket, providing users access to various quantum technologies from multiple vendors, including circuit simulators and different types of QPUs.\n‚ÄúWe‚Äôre not just adding stand-alone quantum modules,‚Äù said Dr. Zhang. ‚ÄúWe‚Äôre standardizing them so each new module can inherently connect with existing ones in the testbed, creating an interoperable and expandable system.‚Äù\nThe collaboration with AWS is strategic for its global reach and robust infrastructure. ‚ÄúAWS cloud offers unparalleled capabilities for worldwide access, data management, and security‚Äîessential functionalities for a sustainable testbed facility,‚Äù Dr. Zhang emphasized.\nAs quantum technology continues to evolve rapidly, this partnership creates a foundation for democratizing access to quantum computing, simulation, sensing, and networking resources.\nSource: Chris Edwards, Guangyi (Leo) Li, Kailu Zhou, Rifkhan Anoor, Visuttha Manthamkarn, and Dr. Zheshen Zhang | 16 SEP 2025\n"},{"uri":"https://quanntm1206.github.io/AWS-FCJ/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Cloud Fundamentals: Understand AWS Global Infrastructure, Pricing, Support Plans, and basic security (IAM). Networking Core: Master VPC architecture (Subnets, Routing, Security Groups/NACLs) and Load Balancing. Hands-on Labs: Account setup, Cost management, VPC construction, and troubleshooting VPN connections. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Tue (9/9) Module 1 \u0026amp; Labs 1, 7, 9: - Theory: Cloud Computing concepts, Regions/AZs, Cost Optimization (Budgets), Support Plans. - Practice: + Create \u0026amp; Secure AWS Account (MFA, IAM User). + Configure AWS Budgets (Cost, Usage, RI). + Open Support Cases. 09/09/2025 09/09/2025 https://youtu.be/HxYZAK1coOI?si=4xf1yZmepR82emMd https://youtu.be/IK59Zdd1poE?si=X5Qao9OxMLUvpBYg https://youtu.be/HSzrWGqo3ME?si=r478_UihmyCTKcQ3 https://youtu.be/pjr5a-HYAjI?si=_QqGDOBJy_1c6WUe https://youtu.be/2PQYqH_HkXw?si=DKIEZLPrSbzEtRvh https://youtu.be/IY61YlmXQe8?si=taziN3scWc-corP6 https://youtu.be/Hku7exDBURo?si=715J5Yl4OjkLVEfj https://000001.awsstudygroup.com/ https://000007.awsstudygroup.com/ https://000009.awsstudygroup.com/ Thu (11/9) Module 2 \u0026amp; Lab 3: - Theory: VPC, Subnets, Route Tables, IGW/NGW, Security (SG vs NACL), ELB (ALB/NLB/GLB). - Practice: + Build a VPC network from scratch. + Launch EC2 (Amazon Linux 2023). + Successfully resolved connectivity issues. 09/11/2025 09/11/2025 https://youtu.be/O9Ac_vGHquM?si=1bFj633qq7EvfLhi https://youtu.be/BPuD1l2hEQ4?si=LrrKAww4vVeK74eE https://youtu.be/CXU8D3kyxIc?si=ju2OidhffNpSfWZ6 https://000003.awsstudygroup.com/ Sat (13/9) Lab 4: VPN Site-to-Site Setup - Configure Customer Gateway \u0026amp; VPN Tunnel. - Troubleshooting: + Install libreswan for Amazon Linux 2023. + Adapt commands: use systemctl instead of service. + Note: Identified mismatch between guide and Console. 09/13/2025 09/13/2025 https://000004.awsstudygroup.com/ Week 1 Achievements: Mastered Cloud Fundamentals \u0026amp; Security:\nUnderstood the \u0026ldquo;Pay-as-you-go\u0026rdquo; model and AWS Global Infrastructure (Region, AZ, Edge Locations). Secured the Root account with MFA and established IAM users for daily tasks. Categorized Support Plans (Basic to Enterprise) and utilized AWS Budgets for cost control. Deep Dived into Networking (VPC):\nDesigned logically isolated networks using VPC, Subnets (Public/Private), and Route Tables. Differentiated between Security Groups (Stateful) and NACLs (Stateless). Understood connectivity options: IGW (Internet), NGW (Outbound only), and VPC Peering/Transit Gateway. Practical Troubleshooting Experience:\nSuccessfully adapted to Amazon Linux 2023 environment (switching from Amazon Linux 2). Resolved service management differences (using systemd-networkd / systemctl). Gained experience in configuring Site-to-Site VPN components (Customer Gateway). "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Week 1: Cloud Fundamentals, VPC Networking and Basic Security\nWeek 2: Compute Deep Dive, RDS Deployment \u0026amp; IAM Policies\nWeek 3: Hybrid Storage, AWS Backup \u0026amp; Lift-and-Shift Migration\nWeek 4: Observability (CloudTrail/CloudWatch), Security Analytics \u0026amp; Serverless Intro\nWeek 5: Advanced IAM (ABAC, Trust Policies) \u0026amp; Cloud Compliance\nWeek 6: Content Delivery (CloudFront), Static Hosting \u0026amp; Incident Management\nWeek 7: Advanced Security, Serverless R\u0026amp;D \u0026amp; Resilience Architectures\nWeek 8: Advanced Networking, Cost Optimization \u0026amp; Governance\nWeek 9: Building Serverless ETL Pipeline \u0026amp; Log Analytics Architecture\nWeek 10: Pipeline Refactoring, ChatOps Integration \u0026amp; Data Engineering\nWeek 11: ETL Automation, Event-Driven Architecture \u0026amp; Ingestion R\u0026amp;D\nWeek 12: Real-time Streaming Transition, Network Security \u0026amp; NLP Research\nWeek 13: Advanced Analytics Optimization, Agentic AI \u0026amp; Capstone\n"},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.11-appendices/5.11.1-cloudtrail-etl/","title":"CloudTrail ETL Code","tags":[],"description":"","content":" import json import boto3 import gzip import re import os from datetime import datetime, timezone s3 = boto3.client(\u0026#34;s3\u0026#34;) firehose= boto3.client(\u0026#34;firehose\u0026#34;) # -------------------------------------------------- # CONFIG # -------------------------------------------------- SOURCE_PREFIX = \u0026#34;exportedlogs/vpc-dns-logs/\u0026#34; FIREHOSE_STREAM_NAME = os.environ.get(\u0026#34;FIREHOSE_STREAM_NAME\u0026#34;) VPC_RE = re.compile(r\u0026#34;/(vpc-[0-9A-Za-z\\-]+)\u0026#34;) ISO_TS_RE = re.compile(r\u0026#34;^\\d{4}-\\d{2}-\\d{2}T\u0026#34;) def read_gz(bucket, key): obj = s3.get_object(Bucket=bucket, Key=key) with gzip.GzipFile(fileobj=obj[\u0026#34;Body\u0026#34;]) as f: return f.read().decode(\u0026#34;utf-8\u0026#34;, errors=\u0026#34;replace\u0026#34;) def flatten_once(d): out = {} for k, v in (d or {}).items(): if isinstance(v, dict): for k2, v2 in v.items(): out[f\u0026#34;{k}_{k2}\u0026#34;] = v2 else: out[k] = v return out def safe_int(x): try: return int(x) except: return None def parse_dns_line(line): raw = line.strip() if not raw: return None json_part = raw prefix_ts = None if ISO_TS_RE.match(raw): try: prefix_ts, rest = raw.split(\u0026#34; \u0026#34;, 1) json_part = rest except: pass if not json_part.startswith(\u0026#34;{\u0026#34;): idx = json_part.find(\u0026#34;{\u0026#34;) if idx != -1: json_part = json_part[idx:] try: obj = json.loads(json_part) except: return None flat = flatten_once(obj) if prefix_ts: flat[\u0026#34;_prefix_ts\u0026#34;] = prefix_ts return flat def lambda_handler(event, context): print(f\u0026#34;Received S3 Event. Records: {len(event.get(\u0026#39;Records\u0026#39;, []))}\u0026#34;) firehose_records = [] for record in event.get(\u0026#34;Records\u0026#34;, []): if \u0026#34;s3\u0026#34; not in record: continue bucket = record[\u0026#34;s3\u0026#34;][\u0026#34;bucket\u0026#34;][\u0026#34;name\u0026#34;] key = record[\u0026#34;s3\u0026#34;][\u0026#34;object\u0026#34;][\u0026#34;key\u0026#34;] if not key.startswith(SOURCE_PREFIX) or not key.endswith(\u0026#34;.gz\u0026#34;): print(f\u0026#34;Skipping file: {key}\u0026#34;) continue print(f\u0026#34;Processing S3 file: {key}\u0026#34;) # Extract VPC ID from file path vpc_id_match = VPC_RE.search(key) vpc_id = vpc_id_match.group(1) if vpc_id_match else \u0026#34;unknown\u0026#34; # Read and process file content content = read_gz(bucket, key) if not content: continue for line in content.splitlines(): r = parse_dns_line(line) if not r: continue # Create flattened JSON record out = { \u0026#34;version\u0026#34;: r.get(\u0026#34;version\u0026#34;), \u0026#34;account_id\u0026#34;: r.get(\u0026#34;account_id\u0026#34;), \u0026#34;region\u0026#34;: r.get(\u0026#34;region\u0026#34;), \u0026#34;vpc_id\u0026#34;: r.get(\u0026#34;vpc_id\u0026#34;, vpc_id), \u0026#34;query_timestamp\u0026#34;: r.get(\u0026#34;query_timestamp\u0026#34;), \u0026#34;query_name\u0026#34;: r.get(\u0026#34;query_name\u0026#34;), \u0026#34;query_type\u0026#34;: r.get(\u0026#34;query_type\u0026#34;), \u0026#34;query_class\u0026#34;: r.get(\u0026#34;query_class\u0026#34;), \u0026#34;rcode\u0026#34;: r.get(\u0026#34;rcode\u0026#34;), \u0026#34;answers\u0026#34;: json.dumps(r.get(\u0026#34;answers\u0026#34;), ensure_ascii=False), \u0026#34;srcaddr\u0026#34;: r.get(\u0026#34;srcaddr\u0026#34;), \u0026#34;srcport\u0026#34;: safe_int(r.get(\u0026#34;srcport\u0026#34;)), \u0026#34;transport\u0026#34;: r.get(\u0026#34;transport\u0026#34;), \u0026#34;srcids_instance\u0026#34;: r.get(\u0026#34;srcids_instance\u0026#34;), \u0026#34;timestamp\u0026#34;: (r.get(\u0026#34;query_timestamp\u0026#34;) or r.get(\u0026#34;timestamp\u0026#34;) or r.get(\u0026#34;_prefix_ts\u0026#34;)) } # Add newline for JSONL format json_row = json.dumps(out, ensure_ascii=False) + \u0026#34;\\n\u0026#34; firehose_records.append({\u0026#39;Data\u0026#39;: json_row}) # Send to Firehose in batches of 500 if firehose_records: total_records = len(firehose_records) print(f\u0026#34;Sending {total_records} records to Firehose...\u0026#34;) batch_size = 500 for i in range(0, total_records, batch_size): batch = firehose_records[i:i + batch_size] try: response = firehose.put_record_batch( DeliveryStreamName=FIREHOSE_STREAM_NAME, Records=batch ) if response[\u0026#39;FailedPutCount\u0026#39;] \u0026gt; 0: print(f\u0026#34;Warning: {response[\u0026#39;FailedPutCount\u0026#39;]} records failed\u0026#34;) except Exception as e: print(f\u0026#34;Firehose error: {e}\u0026#34;) return {\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;, \u0026#34;total_records\u0026#34;: len(firehose_records)} "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.5-processing-setup/5.5.1-create-kinesis-data-firehose/","title":"Create Kinesis Data Firehose","tags":[],"description":"","content":"Create Kinesis Data Firehose Delivery Streams Create cloudtrail-firehose-stream Open Kinesis Console ‚Üí Delivery streams ‚Üí Create delivery stream\nConfigure:\nSource: Direct PUT Destination: Amazon S3 Stream name: cloudtrail-firehose-stream S3 bucket: processed-cloudtrail-logs-ACCOUNT_ID-REGION Prefix: processed-cloudtrail/date=!{timestamp:yyyy-MM-dd}/ Error prefix: processed-cloudtrail/errors/date=!{timestamp:yyyy-MM-dd}/error-type=!{firehose:error-output-type}/ Buffer size: 10 MB Buffer interval: 300 seconds Compression: GZIP IAM role: CloudTrailFirehoseRole Create delivery stream\nCreate vpc-dns-firehose-stream Stream name: vpc-dns-firehose-stream S3 bucket: processed-cloudwatch-logs-ACCOUNT_ID-REGION Prefix: vpc-logs/date=!{timestamp:yyyy-MM-dd}/ Error prefix: vpc-logs/errors/date=!{timestamp:yyyy-MM-dd}/error-type=!{firehose:error-output-type}/ IAM role: CloudWatchFirehoseRole (Same buffer/compression settings as above) Create vpc-flow-firehose-stream Stream name: vpc-flow-firehose-stream S3 bucket: processed-cloudwatch-logs-ACCOUNT_ID-REGION Prefix: eni-flow-logs/date=!{timestamp:yyyy-MM-dd}/ Error prefix: eni-flow-logs/errors/date=!{timestamp:yyyy-MM-dd}/error-type=!{firehose:error-output-type}/ IAM role: CloudWatchFirehoseRole (Same buffer/compression settings as above) "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.3-foundation-setup/5.3.3-create-iam-roles-and-policies/5.3.3.1-create-lambda-excecution-roles/","title":"Create Lambda Execution Roles","tags":[],"description":"","content":"Create CloudTrailETLLambdaServiceRole Open the IAM Console:\nNavigate to https://console.aws.amazon.com/iam/ Or: AWS Management Console ‚Üí Search for \u0026ldquo;IAM\u0026rdquo; ‚Üí Click \u0026ldquo;IAM\u0026rdquo; Navigate to Roles:\nIn the left sidebar, click \u0026ldquo;Roles\u0026rdquo; Click \u0026ldquo;Create role\u0026rdquo;\nSelect trusted entity:\nTrusted entity type: Select \u0026ldquo;AWS service\u0026rdquo; Use case: Select \u0026ldquo;Lambda\u0026rdquo; Click \u0026ldquo;Next\u0026rdquo; Add permissions:\nIn the search box, type AWSLambdaBasicExecutionRole Check the box next to \u0026ldquo;AWSLambdaBasicExecutionRole\u0026rdquo; Click \u0026ldquo;Next\u0026rdquo; Name, review, and create:\nRole name: Enter CloudTrailETLLambdaServiceRole Description: Enter Execution role for CloudTrail ETL Lambda function Click \u0026ldquo;Create role\u0026rdquo; Add inline policy:\nAfter creation, you\u0026rsquo;ll be on the role details page Click on the \u0026ldquo;Permissions\u0026rdquo; tab Click \u0026ldquo;Add permissions\u0026rdquo; ‚Üí \u0026ldquo;Create inline policy\u0026rdquo; Create inline policy:\nClick on the \u0026ldquo;JSON\u0026rdquo; tab Paste the following policy (replace ACCOUNT_ID and REGION): { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;firehose:PutRecord\u0026#34;, \u0026#34;firehose:PutRecordBatch\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:firehose:REGION:ACCOUNT_ID:deliverystream/cloudtrail-firehose-stream\u0026#34; } ] } Click \u0026ldquo;Next\u0026rdquo;\nPolicy name:\nPolicy name: Enter CloudTrailETLPolicy Click \u0026ldquo;Create policy\u0026rdquo; Verify role creation:\nYou should see the role with both managed and inline policies attached Create Remaining Lambda Roles Follow the same process for each role below (steps 3-11):\nGuardDutyETLLambdaServiceRole\nRole name: GuardDutyETLLambdaServiceRole Description: Execution role for GuardDuty ETL Lambda function Managed policy: AWSLambdaBasicExecutionRole Inline policy name: GuardDutyETLPolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/*\u0026#34;, \u0026#34;arn:aws:s3:::processed-guardduty-findings-ACCOUNT_ID-REGION/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;kms:Decrypt\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:kms:REGION:ACCOUNT_ID:key/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:CreatePartition\u0026#34;, \u0026#34;glue:GetPartition\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:glue:REGION:ACCOUNT_ID:catalog\u0026#34;, \u0026#34;arn:aws:glue:REGION:ACCOUNT_ID:database/security_logs\u0026#34;, \u0026#34;arn:aws:glue:REGION:ACCOUNT_ID:table/security_logs/processed_guardduty\u0026#34; ] } ] } CloudWatchETLLambdaServiceRole\nRole name: CloudWatchETLLambdaServiceRole Description: Execution role for VPC DNS logs ETL Lambda Managed policy: AWSLambdaBasicExecutionRole Inline policy name: CloudWatchETLPolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;firehose:PutRecord\u0026#34;, \u0026#34;firehose:PutRecordBatch\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:firehose:REGION:ACCOUNT_ID:deliverystream/vpc-dns-firehose-stream\u0026#34; } ] } CloudWatchENIETLLambdaServiceRole\nRole name: CloudWatchENIETLLambdaServiceRole Description: Execution role for VPC Flow logs ETL Lambda Managed policy: AWSLambdaBasicExecutionRole Inline policy name: CloudWatchENIETLPolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;firehose:PutRecord\u0026#34;, \u0026#34;firehose:PutRecordBatch\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:firehose:REGION:ACCOUNT_ID:deliverystream/vpc-flow-firehose-stream\u0026#34; } ] } CloudWatchExportLambdaServiceRole\nRole name: CloudWatchExportLambdaServiceRole Description: Execution role for CloudWatch log export Lambda Managed policy: AWSLambdaBasicExecutionRole Inline policy name: CloudWatchExportPolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateExportTask\u0026#34;, \u0026#34;logs:DescribeExportTasks\u0026#34;, \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:REGION:ACCOUNT_ID:log-group:*\u0026#34;, \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/*\u0026#34; ] } ] } ParseFindingsLambdaServiceRole\nRole name: ParseFindingsLambdaServiceRole Description: Execution role for parsing GuardDuty findings Managed policy: AWSLambdaBasicExecutionRole No inline policy needed IsolateEC2LambdaServiceRole\nRole name: IsolateEC2LambdaServiceRole Description: Execution role for isolating compromised EC2 instances Managed policy: AWSLambdaBasicExecutionRole Inline policy name: IsolateEC2Policy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:DescribeInstances\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } QuarantineIAMLambdaServiceRole\nRole name: QuarantineIAMLambdaServiceRole Description: Execution role for quarantining compromised IAM users Managed policy: AWSLambdaBasicExecutionRole Inline policy name: QuarantineIAMPolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:AttachUserPolicy\u0026#34;, \u0026#34;iam:ListAttachedUserPolicies\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:iam::ACCOUNT_ID:user/*\u0026#34;, \u0026#34;arn:aws:iam::ACCOUNT_ID:policy/IrQuarantineIAMPolicy\u0026#34; ] } ] } AlertDispatchLambdaServiceRole\nRole name: AlertDispatchLambdaServiceRole Description: Execution role for dispatching alerts via SNS/SES/Slack Managed policy: AWSLambdaBasicExecutionRole Inline policy name: AlertDispatchPolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;sns:Publish\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:sns:REGION:ACCOUNT_ID:IncidentResponseAlerts\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ses:SendEmail\u0026#34;, \u0026#34;ses:SendRawEmail\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguyen Tran Minh Quan\nPhone Number: 0908351595\nEmail: nguyentranminhquanb@gmail.com\nUniversity: FPT University\nMajor: Artificial Intelligent\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 09/09/2025 to 09/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.7-dashboard-setup/5.7.2-setup-lambda/5.7.2.1-create-iam-role-and-policy-for-lambda/","title":"Lambda IAM Role and Policy setup","tags":[],"description":"","content":"In this guide, you will setup IAM Role and Policy for Lambda.\nCreate IAM Role for Lambda Open the IAM Console\nNavigate to https://console.aws.amazon.com/iam/ Or: AWS Management Console ‚Üí Services ‚Üí IAM Create Role:\nChoose the Role option on the left menu panel. Then click Create role. Select trusted entity:\nTrusted entity type: AWS Service Use case: Lambda Click \u0026ldquo;Next\u0026rdquo; Attach permissions policies:\nIn the search box, type AWSLambdaBasicExecutionRole Check the box next to \u0026ldquo;AWSLambdaBasicExecutionRole\u0026rdquo; Click \u0026ldquo;Next\u0026rdquo; Name, review, and create:\nRole name: Enter dashboard-query-role Description: Enter Execution role for Lambda function Click \u0026ldquo;Create role\u0026rdquo; Add inline policy:\nAfter creation, you\u0026rsquo;ll be on the role details page Click on the \u0026ldquo;Permissions\u0026rdquo; tab Click \u0026ldquo;Add permissions\u0026rdquo; ‚Üí \u0026ldquo;Create inline policy\u0026rdquo; Create inline policy:\nClick on the \u0026ldquo;JSON\u0026rdquo; tab Paste the following policy: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AthenaActions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;athena:StartQueryExecution\u0026#34;, \u0026#34;athena:GetQueryExecution\u0026#34;, \u0026#34;athena:GetQueryResults\u0026#34;, \u0026#34;athena:StopQueryExecution\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;GlueCatalogRead\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:GetDatabase\u0026#34;, \u0026#34;glue:GetDatabases\u0026#34;, \u0026#34;glue:GetTable\u0026#34;, \u0026#34;glue:GetTables\u0026#34;, \u0026#34;glue:GetPartitions\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;S3SourceAndResultAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::vel-athena-results\u0026#34;, \u0026#34;arn:aws:s3:::vel-athena-results/*\u0026#34;, \u0026#34;arn:aws:s3:::vel-processed-cloudtrail-logs\u0026#34;, \u0026#34;arn:aws:s3:::vel-processed-cloudtrail-logs/*\u0026#34;, \u0026#34;arn:aws:s3:::vel-processed-guardduty\u0026#34;, \u0026#34;arn:aws:s3:::vel-processed-guardduty/*\u0026#34;, \u0026#34;arn:aws:s3:::cloudwatch-formatted\u0026#34;, \u0026#34;arn:aws:s3:::cloudwatch-formatted/*\u0026#34; ] } ] } Click \u0026ldquo;Next\u0026rdquo;\nPolicy name:\nPolicy name: Enter lambda-query-policy Click \u0026ldquo;Create policy\u0026rdquo; Verify role creation:\nYou should see the role with both managed and inline policies attached "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.1-workshop-overview/","title":"Overview","tags":[],"description":"","content":"System Components Auto Incident Response and Forensics is an architecture that uses automation services to ingest, process, and automatically respond to security findings, minimizing the time required for human intervention and aids security personel in visualizing and analyzing logs. This system is built around AWS Security Services (CloudTrail, GuardDuty, VPC Flow Logs, CloudWatch) feeding data into a Centralized Data Lake (S3/Glue/Athena) for analysis. The core automation is driven by AWS EventBridge rules triggering AWS Step Functions workflows, which then execute AWS Lambda functions to perform isolation and alerting actions. Workshop overview In this workshop, you will deploy a multi-phase system to achieve end-to-end security automation. This includes:\nFoundation Setup: Creating dedicated S3 buckets and IAM roles to support all services. Monitoring Setup: Enabling and configuring key security logs (CloudTrail, GuardDuty, VPC Flow Logs) to direct data to the central log ingestion point. Processing Setup: Deploying Kinesis Firehose, Lambda ETLs, and Glue/Athena tables to transform raw logs into an easily queryable security data lake. Automation Setup: Creating the Isolation Security Group, SNS Topic, Incident Response Lambda Functions, and the Step Functions State Machine that executes automatic quarantine actions when GuardDuty detects findings. Dashboard Setup: Hosting a secure, S3-based static web interface accelerated by CloudFront and protected by Cognito to provide analysts with real-time visualization of forensic data and direct query capabilities via API Gateway. "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.2-prerequiste/","title":"Prerequisites","tags":[],"description":"","content":"Required Access and Information Before proceeding with the setup of the Automated AWS Incident Response and Forensics System, ensure you have gathered the required access credentials and information below.\nüîë Access \u0026amp; Identifiers AWS Account with Administrative Access You need full administrative permissions to create resources across multiple AWS services. Access to the AWS Management Console. Your AWS Account ID Format: 12-digit number (e.g., 123456789012). Placeholder: Replace ACCOUNT_ID throughout the guide. Target AWS Region Choose the region where you\u0026rsquo;ll deploy the system (e.g., us-east-1). Placeholder: Replace REGION throughout the guide. VPC ID A VPC with at least one subnet is required for VPC Flow Logs. Placeholder: Replace YOUR_VPC_ID in the guide. Amazon SES Verified Email Address Required for sending and recieving email alerts. Verify this address in the SES Console. Placeholder: Replace YOUR_VERIFIED_EMAIL@example.com. Slack Webhook URL (Optional) If you want Slack notifications, obtain a webhook URL from your Slack workspace. Placeholder: Replace YOUR_SLACK_WEBHOOK_URL. "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.3-foundation-setup/5.3.1-set-up-s3-buckets/","title":"Set up S3 buckets","tags":[],"description":"","content":"In this section, you will create 5 S3 buckets that serve as the foundation for the Auto Incident Response system.\nImportant: Replace ACCOUNT_ID with your AWS Account ID and REGION with your target region (e.g., us-east-1) in all bucket names.\nBucket Names incident-response-log-list-bucket-ACCOUNT_ID-REGION - Primary log collection bucket processed-cloudtrail-logs-ACCOUNT_ID-REGION - Stores processed CloudTrail logs athena-query-results-ACCOUNT_ID-REGION - Stores Athena query results processed-cloudwatch-logs-ACCOUNT_ID-REGION - Stores processed CloudWatch logs processed-guardduty-findings-ACCOUNT_ID-REGION - Stores processed GuardDuty findings Bucket Creation Instructions Open the Amazon S3 Console Navigate to https://console.aws.amazon.com/s3/ Or: AWS Management Console ‚Üí Services ‚Üí S3 Click on \u0026ldquo;Create bucket\u0026rdquo; General configuration: Bucket name: Enter incident-response-log-list-bucket-ACCOUNT_ID-REGION Example: incident-response-log-list-bucket-123456789012-us-east-1 AWS Region: Select your target region (e.g., US East (N. Virginia) us-east-1) Object Ownership:\nKeep default: ACLs disabled (recommended) Block Public Access settings for this bucket:\nCheck \u0026ldquo;Block all public access\u0026rdquo; Ensure all 4 sub-options are checked: ‚úì Block public access to buckets and objects granted through new access control lists (ACLs) ‚úì Block public access to buckets and objects granted through any access control lists (ACLs) ‚úì Block public access to buckets and objects granted through new public bucket or access point policies ‚úì Block public and cross-account access to buckets and objects through any public bucket or access point policies Bucket Versioning:\nSelect \u0026ldquo;Enable\u0026rdquo; Tags (optional):\nAdd tags if desired Example: Key=Purpose, Value=IncidentResponse Default encryption:\nEncryption type: Select \u0026ldquo;Server-side encryption with Amazon S3 managed keys (SSE-S3)\u0026rdquo; Bucket Key: Keep default (Enabled) Advanced settings:\nKeep all defaults Click \u0026ldquo;Create bucket\u0026rdquo;\nVerify bucket creation:\nYou should see a success message The bucket should appear in your S3 buckets list Repeat steps 2-10 for the remaining 4 buckets:\nprocessed-cloudtrail-logs-ACCOUNT_ID-REGION athena-query-results-ACCOUNT_ID-REGION processed-cloudwatch-logs-ACCOUNT_ID-REGION processed-guardduty-findings-ACCOUNT_ID-REGION Verify all 5 buckets are created:\nNavigate to S3 Console You should see all 5 buckets listed "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.7-dashboard-setup/5.7.1-setup-s3/","title":"Setup S3 Bucket for Dashboard","tags":[],"description":"","content":"In this guide, you will setup a S3 to contain web files and folder. Important: Replace ACCOUNT_ID with your AWS Account ID and REGION with your target region (e.g., us-east-1) in all bucket names.\nBucket Names static-dashboard-bucket-ACCOUNT_ID-REGION - Store builded web files and folder\nBucket Creation Instructions Open the Amazon S3 Console\nNavigate to https://console.aws.amazon.com/s3/ Or: AWS Management Console ‚Üí Services ‚Üí S3 Click on \u0026ldquo;Create bucket\u0026rdquo;\nBucket create setting:\nKeep the setting like default: Bucket name: Enter static-dashboard-bucket-ACCOUNT_ID-REGION Example: static-dashboard-bucket-123456789012-us-east-1 Ownership: ACLs disabled Block Public Access: Block all public access Bucket versioning: Disable Tags(Optional): Add if you want Encryption: SSE-S3 Bucket key: Enable Click Create bucket Verify bucket creation:\nYou should see a success message The bucket should appear in your S3 buckets list Upload files and folder:\nGo to Github to get the web content and upload to S3 "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/2-proposal/","title":"Proposal","tags":[],"description":"","content":"\nAutomated AWS Incident Response and Forensics System Proposal Link (Google Docs): Proposal 1. Executive Summary Our team is building an automated incident response and forensics solution as part of the AWS First Cloud Journey internship program. The idea is straightforward‚Äîwhen a security issue happens in AWS, we want the system to respond automatically without waiting for manual intervention.\nWe\u0026rsquo;re creating a platform that automatically detects security findings from GuardDuty, isolates affected resources, captures forensic evidence through comprehensive data collection, and provides analytics and dashboards where security teams can investigate what happened. Everything is built using Infrastructure-as-Code with AWS CDK, so customers can easily deploy it into their own AWS accounts.\n2. Problem Statement What‚Äôs the Problem? The increasing frequency and sophistication of cyber threats pose significant risks to organizations relying on cloud infrastructure. Manual incident response processes are often slow, inconsistent, and prone to human error, which can lead to prolonged system downtime, data breaches, and financial losses. The project aims to address these challenges by developing an automated, reliable, and scalable incident response system that minimizes response time, enhances forensic capabilities, and reduces operational costs.\nThe Solution The main use cases include detecting unauthorized AWS credential use, identifying compromised EC2 instances, and ensuring forensic data is properly collected, processed, and stored for investigation. Our architecture integrates VPC Flow Logs, CloudTrail, CloudWatch, and GuardDuty to detect threats, while Step Functions orchestrates the automated response workflow including EC2 isolation, ASG detachment, Create Snapshot and IAM quarantine. All evidence is collected and processed through custom ETL Lambda and Data Firehose, using Athena for forensic analysis. The system also includes alert dispatching, notification via messaging and email, and provides dashboards and analytics for security teams to investigate what happened.\nBenefits and Return on Investment Rapid threat detection: Automated response reduces the window of vulnerability. Comprehensive evidence gathering: Automated forensic data collection facilitates faster investigations. Cost-effective deployment: Leveraging AWS serverless services minimizes infrastructure expenses. Improved security posture: Continuous monitoring and real-time alerts. Actionable insights: Dashboards and analytics empower security teams. Scalability: Adaptable to organizations of various sizes and incident volumes. 3. Solution Architecture Our solution uses a comprehensive multi-stage architecture for automated incident response and forensics:\nAWS Services Used Amazon GuardDuty: Continuously monitors for security threats and suspicious activity. AWS Step Functions: Orchestrates the incident response workflow. AWS Lambda: Runs automation code for isolation and data processing. Amazon EventBridge: Routes findings from GuardDuty to Step Functions. Amazon S3: Stores forensic evidence and hosts static dashboard. Amazon Athena: Enables SQL queries against forensic datasets. Amazon API Gateway: Facilitates communication between dashboard and backend. Amazon Cognito: Secures access for dashboard users. Amazon CloudFront: Accelerates dashboard delivery across the globe. Amazon SNS \u0026amp; SES: Handles notifications via messaging and email. AWS CloudTrail: Logs all actions for auditing. Amazon CloudWatch: Monitoring and dashboards. Amazon EC2: Optional instances for analysis. AWS KMS: Key management for encryption. Amazon Kinesis Data Firehose: Streams data to S3. Component Design Data Collection \u0026amp; Detection Layer: Collects events from VPC Flow Logs, CloudTrail, CloudWatch, EC2, and GuardDuty. Event Processing Layer: Alert Dispatch, EventBridge routes findings to Step Functions; events are classified by type. Automated Response Orchestration: Step Functions handle parsing, decision making, EC2 isolation, termination protection, ASG detachment, snapshot creation, and IAM quarantine. Alerting \u0026amp; Notification Layer: SNS, Slack \u0026amp; SES handles notifications via messaging and email, Alert Dispatch. Data Processing \u0026amp; Analytics Layer: ETL pipeline with Lambda and Data Firehose processes raw logs into S3; Athena queries the data. Dashboard \u0026amp; Analysis Layer: S3-hosted React dashboard with Cognito auth, consuming data via API Gateway and Athena. 4. Technical Implementation Implementation Phases We use Agile Scrum with 1-week sprints over 6 weeks:\nSprint 1: Foundation \u0026amp; Setup (VPC, Security Groups, Training). Sprint 2: Core Orchestration (Step Functions, Lambda, GuardDuty integration). Sprint 3: Data \u0026amp; Analytics (S3, Athena, ETL pipeline). Sprint 4: Dashboard \u0026amp; UI (Static site, API Gateway, CloudFront). Sprint 5: Testing \u0026amp; Optimization (Cognito, Performance testing, Simulations). Sprint 6: Documentation \u0026amp; Handover (Guides, Demos, Final Polish). Technical Requirements Frontend \u0026amp; Dashboards: Custom HTML/CSS/JS hosted on S3, served via CloudFront. Backend \u0026amp; Processing: Python 3.12 for Lambda, Step Functions for orchestration. Data \u0026amp; Storage: S3 for evidence, Athena for querying, Firehose for streaming. Infrastructure: All defined in AWS CDK (Python). Security: GuardDuty for detection, IAM for least privilege, KMS for encryption. 5. Timeline \u0026amp; Milestones Project Timeline Project Timeline\nWeek 6-7 (Foundation \u0026amp; Setup) Activities: Team training on GuardDuty/Step Functions, architecture design review, VPC and security setup. Deliverables: Architecture document v1, team training completion, GitHub repository established. Week 7-9 (Core Orchestration) Activities: Step Functions workflow development, Lambda function coding for all response actions, EventBridge integration, SNS/SES setup, integration testing. Deliverables: Step Functions state machine definition, 7+ Lambda functions with documentation, GuardDuty integration, notification system, API Gateway. Week 10 (Data \u0026amp; Analytics) Activities: S3 forensic storage setup, Athena table creation, ETL pipeline development, SQL query library. Deliverables: 15+ Athena queries documented, forensic analysis runbooks, processed data storage. Week 11 (Dashboard \u0026amp; UI) Activities: Static dashboard development, Cognito authentication, API Gateway setup, CloudFront CDN configuration, dashboard integration. Deliverables: S3-hosted dashboard, authentication system, query interface, real-time results integration. Week 12 (Testing \u0026amp; Validation \u0026amp; Optimization) Activities: Manual testing, security scanning including simulated incident scenarios (5+ workflows), performance testing, attack simulation. Optimize data with Athena query and Data Firehose. Deliverables: Security scan results, incident simulation videos, data optimization. Week 13 (Documentation \u0026amp; Handover) Activities: Deployment guide, API documentation, knowledge transfer sessions, final demo, GitHub cleanup. Deliverables: Complete GitHub repository (public), deployment guide instructions, live workshop demonstration. 6. Budget Estimation You can find the detailed budget estimation on the AWS Pricing Calculator.\nInfrastructure Costs Typical monthly deployment cost (Free Tier / Low scale): ~$5.01\nGuardDuty: ~$1.80/month S3: ~$1.07/month KMS: ~$1.12/month CloudTrail: ~$0.55/month Athena: ~$0.29/month Amazon Simple Email Service (SES): ~$0.09/month Amazon API Gateway: ~$0.05/month Amazon Data firehose: ~$0.04/month Lambda, Step Functions, SNS: Generally within Free Tier limits for typical usage. Note: Costs assume typical usage of 20-150 incidents per month.\n7. Risk Assessment Risk Matrix Performance Bottlenecks: High data volume slowing down queries. Security Breaches: Compromise of the forensic data itself. Cost Overruns: Unchecked logging or infinite loops. Mitigation Strategies Performance: Monitor Athena/Firehose; optimize queries; dynamic resource adjustment. Security: Encryption (KMS), strict IAM roles, audit logging, compliance checks. Cost: AWS budget alerts, cost anomaly detection, auto-scaling limits. Disaster Recovery: Backups, failover procedures, and redundancy measures. 8. Expected Outcomes Technical Improvements Automated Response: Zero-touch isolation of compromised resources. Speed: Reduction of investigation time from hours to minutes. Reliability: Consistent, repeatable evidence collection without human error. Long-term Value Scalable Architecture: Foundation for future security automation. Knowledge: Team competency in advanced AWS security and serverless concepts. Reusable Asset: A deployable solution for other AWS customers or teams. Status: Ready for Review \u0026amp; Approval Project Code: AWS-FCJ-IR-FORENSICS-2025\n"},{"uri":"https://quanntm1206.github.io/AWS-FCJ/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: ‚ÄúAWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS‚Äù Event Objectives To provide an introduction to AI, Machine Learning, and Generative AI within the AWS ecosystem. Speakers Lam Tuan Kiet ‚Äì Sr DevOps Engineer, FPT Software Danh Hoang Hieu Nghi - AI Engineer, Renova Cloud Dinh Le Hoang Anh - Cloud Engineer Trainee, First Cloud AI Journey Van Hoang Kha - Cloud Security Engineer, AWS Community Builder Key Highlights Explored Generative AI with Amazon Bedrock: - Foundation Models: Unlike traditional models, these serve as versatile tools adaptable to a multitude of tasks. Bedrock provides access to fully managed models from leading AI companies such as OpenAI, Claude, and Anthropic. - Prompt Engineering: The art of crafting and refining instructions to optimize model output. + Zero-Shot Prompting: Providing a prompt without any prior context or examples. + Few-shot Prompting: Including a limited amount of context and examples within the prompt to guide the model. + Chain of Thought: Encouraging the model to outline its reasoning process and steps to derive the final answer. - Retrieval Augmented Generation (RAG): A method for retrieving relevant information from specific data sources. + R: Retrieval - Gathers relevant information from a knowledge base or external data sources. + A: Augmented - Incorporates the retrieved information as additional context in the user\u0026rsquo;s prompt before input. + G: Generation - Produces responses from the model based on the enhanced, augmented prompt. + Use cases: Applications include enhancing content quality, creating contextual chatbots, enabling personalized search, and facilitating real-time data summarization.\n- Amazon Titan Embedding: A lightweight model designed to efficiently translate text into numerical representations (embeddings). It excels in high-accuracy retrieval tasks and supports over 100 languages.\n- Pretrained AI Services:¬†+ Amazon Rekognition: Facilitates image and video analysis. + Amazon Translate: Detects and translates text across languages. + Amazon Textract: Extracts text and layout data from documents. + Amazon Transcribe: Converts speech into text. + Amazon Polly: Converts text into lifelike speech. + Amazon Comprehend: Derives insights and identifies relationships within text. + Amazon Kendra: An intelligent enterprise search service. + Amazon Lookout: Detects anomalies in business metrics, equipment, and images. + Amazon Personalize: Tailors recommendations to individual users.\n- Demo: AMZPhoto: A demonstration of face recognition capabilities using AI on images.\n- Amazon Bedrock AgentCore: A comprehensive platform designed to address the challenges of deploying agents into production: + Securely executing and scaling agent code. + Incorporating memory to retain past interactions and facilitate learning. + Implementing identity and access controls for both agents and tools. + Enabling agentic tool use for complex workflows. + Discovering and connecting with custom tools and resources. + Understanding and auditing every interaction (observability). + Foundational Services: Categorized services that ensure agents run securely at scale.\n+ Enhance with tools \u0026amp; memory: Includes components such as Memory, Gateway, Browser tools, and Code Interpreter.\n+ Deploy securely at scale: Covers Runtime and Identity management.\n+ Gain operational insights: Focuses on Observability.\n+ Enabling Agents at Scale (Architecture): Connects the AgentCore Gateway (via MCP) to Memory, Identity, Observability, Browser, and Code Interpreter components.\n+ Frameworks for Building Agents: Supports frameworks such as CrewAI, Google ADK, LangGraph/LangChain, LlamaIndex, OpenAI Agents SDK, and Strands Agents SDK.\nKey Takeaways Bedrock is the GenAI Hub: Amazon Bedrock serves as a central hub, offering fully managed Foundation Models from top industry players for a wide array of tasks.\nCustomization via Prompts and Data: Users can customize interactions through various prompting techniques (Zero-Shot, Few-Shot, CoT) and utilize RAG to inject context for superior model responses.\nEmbeddings Power Search: Amazon Titan Embeddings plays a critical role in translating text to numerical data, thereby facilitating high-accuracy retrieval tasks (such as RAG).\nPretrained Models: AWS offers a suite of ready-to-deploy AI services for common requirements, such as Amazon Rekognition for imagery and Textract for documentation.\nAgentCore Solves Production Issues: Amazon Bedrock AgentCore is a comprehensive platform that addresses the complexities of operating AI Agents at scale, managing critical components like Memory, Identity, and Observability.\nApplying to Work The insights gained will be highly beneficial for the team\u0026rsquo;s upcoming projects, which are likely to incorporate AI Foundation Models into our architecture. Event Experience The speakers were articulate and provided highly informative content. Q\u0026amp;A: A team member raised a project-critical question, despite it being slightly tangential to the main topic. + Q: Our architecture utilizes SNS to process GuardDuty findings, but we face bottlenecks when over 1000 alerts occur simultaneously. How can this be resolved? + A: Implementing SQS to queue the events ensures that no alerts are missed during high-volume periods. I secured a position in the top 10 during the concluding Kahoot quiz and had the opportunity to take a photo with the speakers. We established an unofficial group named \u0026ldquo;M√®o Cam ƒêeo KhƒÉn,\u0026rdquo; representing a collaboration between my group, \u0026ldquo;The Ballers,\u0026rdquo; and \u0026ldquo;Vinhomies.\u0026rdquo; Some event photos "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Amazon MSK Replicator and MirrorMaker2: Choosing the right replication strategy Customers need to replicate data from their Apache Kafka clusters for a variety of reasons, such as compliance requirements, cluster migrations, and disaster recovery (DR) implementations. However, the right replication strategy can vary depending on the application context.\nIn this post, we walk through the different considerations for using Amazon MSK Replicator over Apache Kafka‚Äôs MirrorMaker 2, and help you choose the right replication solution for your use case.\nChallenges with choosing DR strategies Customers create business continuity plans and DR strategies to maximize resiliency for their applications, because downtime or data loss can result in losing revenue or halting operations. For customers using Kafka, planning for DR is essential to meeting Recovery Time Objective (RTO) and Recovery Point Objective (RPO) goals.\nAmazon MSK Availability Features Multi-AZ: Amazon MSK provides high availability by distributing brokers across multiple Availability Zones. Intra-cluster replication: A replication factor of 3 and min-ISR of 2, combined with acks=all, provides robust protection against single broker or single-AZ failures. Express brokers: These offer pay-as-you-go storage and faster recovery times, increasing resilience. However, if an issue impacts an entire Region, a multi-Region architecture is required.\nS3 Backups vs. Multi-Region Replication For companies that can withstand a longer RTO but require a lower RPO, backing up data to Amazon S3 (using Amazon MSK Connect) is a valid DR plan. However, this approach has challenges:\nRestoration can take a long time depending on data volume. Handling consumer group offsets is complex. Therefore, most streaming use cases rely on setting up MSK clusters in multiple Regions and configuring data replication between clusters to provide the required business resilience.\nChoosing the right replication solution: MSK Replicator vs MirrorMaker 2 AWS recommends two primary solutions for cross-Region Kafka replication. Understanding when to use each is crucial.\n1. MSK Replicator: For most MSK cluster replications in the same account MSK Replicator is a fully managed, serverless Kafka replication service. It is the recommended solution for replicating data within the same AWS account.\nBenefits:\nReplication between MSK clusters: Supports active-active or active-passive DR architectures. No infrastructure management: Fully serverless with automatic scaling. Built-in monitoring: Integrated with Amazon CloudWatch metrics and logs. Built-in high availability: Offers fault tolerance across Availability Zones. 2. MirrorMaker 2: For migrations and complex/hybrid scenarios MirrorMaker 2 (MM2) is a utility bundled with Kafka that uses the Kafka Connect framework. It remains the preferred solution for specific use cases requiring flexibility.\nRecommended for:\nCross-account replication: Replicating between MSK clusters in different AWS accounts. Migrations to Amazon MSK: Moving from on-premises, other clouds, or self-managed EC2. Cross-cloud or hybrid cloud scenarios: For DR or analytics across different environments. Using mTLS or SASL/SCRAM authentication: When IAM authentication cannot be enabled. Custom replication policies: Advanced topic naming or transformation requirements. MSK Replicator solution overview The following diagram illustrates the architecture for using MSK Replicator for Disaster Recovery.\nWe create two MSK clusters‚Äîone in the primary Region and a standby cluster in the secondary Region. MSK Replicator is deployed in the secondary region to replicate topics, ACLs, data, and consumer group offsets.\nScenario: Single-direction replication for active-passive DR (can be extended to active-active). Failover: Kafka clients connect to the primary cluster and switch to the secondary upon failover. MirrorMaker2 solution overview The following diagram illustrates the architecture for using MirrorMaker 2 for Migration.\nWe create an MSK cluster in the primary Region alongside an existing on-premises Kafka cluster (or self-managed/other cloud).\nScenario: Single-direction replication for cluster migration. Process: Clients interact with the on-premises cluster and are migrated to run on AWS interacting with the MSK cluster. Automated Deployment: Rather than manual configuration, AWS recommends using automated deployment resources (Terraform, Docker images optimized for AWS) to deploy MirrorMaker 2 on Amazon ECS with Fargate for scalable, serverless container deployment.\nConclusion Choosing the right replication solution depends on your specific requirements:\nSolution Primary Use Case Amazon MSK Replicator Replicating from one MSK cluster to another within the same account; fully managed DR solution. MirrorMaker 2 Migrations to Amazon MSK, hybrid environments, cross-account replication, or complex custom policies. These approaches provide customizable options to ensure data redundancy and business continuity, helping meet regulatory compliance while minimizing operational overhead.\nSource: Mazrim Mehrtens | 16 SEP 2025\n"},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.10-cleanup/5.10.2-cdk-cleanup/","title":"CDK Cleanup","tags":[],"description":"","content":"Clean up (CDK) This guide ensures you correctly decommission all resources provisioned by the AWS CDK stack and clean up manually created data to avoid ongoing charges.\nPhase 1: Manual Data Cleanup (Before CDK Destroy) The CDK automatically deletes most resources failed in deleting content from S3 buckets. You must empty the contents of these buckets before running the cdk destroy command.\nResource Name Purpose Action Required incident-response-log-list-bucket Primary Log Source Empty Contents processed-cloudwatch-logs ETL Destination Empty Contents processed-guardduty-findings ETL Destination Empty Contents processed-cloudtrail-logs ETL Destination Empty Contents athena-query-results Athena Query Results Empty Contents aws-incident-response-automation-dashboard React Dashboard S3 Bucket Empty Contents Instructions for Emptying Buckets:\nOpen the Amazon S3 Console in your browser. For each of the buckets listed above (look for the names based on your AWS Account ID and Region): Click on the bucket name. Navigate to the \u0026ldquo;Objects\u0026rdquo; tab. Click the \u0026ldquo;Empty\u0026rdquo; button. Follow the prompts to confirm the permanent deletion of all objects. Phase 2: CDK Stack Destruction This step uses the CDK CLI to destroy all resources provisioned by the CloudFormation stack.\nEnsure Virtual Environment is Active\nIf you deactivated your Python environment, re-activate it (e.g., source .venv/bin/activate). Navigate to the Project Root\nEnsure you are in the main directory where the cdk.json file is located. Execute the Destroy Command\nRun the command to destroy all deployed stacks. When prompted, type y to approve the deletion. $ cdk destroy --all Phase 3: Post-Destruction Cleanup This step addresses remaining manual cleanup of lingering resources.\nDelete Remaining S3 Buckets\nThe cdk destroy command should remove the empty S3 buckets. If any remain (due to final checks or service protections), delete them now via the S3 Console. Disable Amazon GuardDuty\nGo to GuardDuty Console ‚Üí Settings ‚Üí General. Verify the service is disabled to ensure billing stops. Remove Cognito User and Pool\nGo to Cognito Console ‚Üí User pools. Delete the test user you created. Delete the User Pool created for the dashboard. Remove SES Identity\nGo to Amazon SES Console ‚Üí Verified Identities. Delete the sender email identity (sender_email) you verified. Deactivate Virtual Environment\nDeactivate the Python virtual environment: $ deactivate "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.5-processing-setup/5.5.2-create-aws-glue-database-and-tables/","title":"Create AWS Glue Database and Tables","tags":[],"description":"","content":"Create AWS Glue Database and Tables Create Database Open Glue Console ‚Üí Databases ‚Üí Add database\nDatabase name: security_logs\nCreate database\nCreate Tables (Using Athena DDL) Open Athena Console\nSet query result location: s3://athena-query-results-ACCOUNT_ID-REGION/\nSelect database: security_logs\nCreate processed_cloudtrail Table Run this DDL in Athena (replace ACCOUNT_ID and REGION):\nCREATE EXTERNAL TABLE IF NOT EXISTS security_logs.processed_cloudtrail ( `eventtime` string, `eventname` string, `eventsource` string, `awsregion` string, `sourceipaddress` string, `useragent` string, `useridentity` struct\u0026lt; type:string, invokedby:string, principalid:string, arn:string, accountid:string, accesskeyid:string, username:string, sessioncontext:struct\u0026lt; attributes:map\u0026lt;string,string\u0026gt;, sessionissuer:struct\u0026lt; type:string, principalid:string, arn:string, accountid:string, username:string \u0026gt; \u0026gt;, inscopeof:struct\u0026lt; issuertype:string, credentialsissuedto:string \u0026gt; \u0026gt;, `requestparameters` string, `responseelements` string, `resources` array\u0026lt;struct\u0026lt;arn:string,type:string\u0026gt;\u0026gt;, `recipientaccountid` string, `serviceeventdetails` string, `errorcode` string, `errormessage` string, `hour` string, `usertype` string, `username` string, `isconsolelogin` boolean, `isfailedlogin` boolean, `isrootuser` boolean, `isassumedrole` boolean, `ishighriskevent` boolean, `isprivilegedaction` boolean, `isdataaccess` boolean, `target_bucket` string, `target_key` string, `target_username` string, `target_rolename` string, `target_policyname` string, `new_access_key` string, `new_instance_id` string, `target_group_id` string, `identity_principalid` string ) PARTITIONED BY ( `date` string ) ROW FORMAT SERDE \u0026#39;org.openx.data.jsonserde.JsonSerDe\u0026#39; WITH SERDEPROPERTIES ( \u0026#39;serialization.format\u0026#39; = \u0026#39;1\u0026#39; ) LOCATION \u0026#39;s3://processed-cloudtrail-logs-ACCOUNT_ID-REGION/processed-cloudtrail/\u0026#39; TBLPROPERTIES ( \u0026#39;projection.enabled\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;projection.date.type\u0026#39; = \u0026#39;date\u0026#39;, \u0026#39;projection.date.format\u0026#39; = \u0026#39;yyyy-MM-dd\u0026#39;, \u0026#39;projection.date.range\u0026#39; = \u0026#39;2025-01-01,NOW\u0026#39;, \u0026#39;projection.date.interval\u0026#39; = \u0026#39;1\u0026#39;, \u0026#39;projection.date.interval.unit\u0026#39; = \u0026#39;DAYS\u0026#39;, \u0026#39;storage.location.template\u0026#39; = \u0026#39;s3://processed-cloudtrail-logs-ACCOUNT_ID-REGION/processed-cloudtrail/date=${date}/\u0026#39;, \u0026#39;classification\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;compressionType\u0026#39; = \u0026#39;gzip\u0026#39; ); Create processed_guardduty Table Run this DDL in Athena:\nCREATE EXTERNAL TABLE IF NOT EXISTS security_logs.processed_guardduty ( `finding_id` string, `finding_type` string, `title` string, `severity` double, `account_id` string, `region` string, `created_at` string, `event_last_seen` string, `remote_ip` string, `remote_port` int, `connection_direction` string, `protocol` string, `dns_domain` string, `dns_protocol` string, `scanned_ip` string, `scanned_port` int, `aws_api_service` string, `aws_api_name` string, `aws_api_caller_type` string, `aws_api_error` string, `aws_api_remote_ip` string, `target_resource_arn` string, `instance_id` string, `instance_type` string, `image_id` string, `instance_tags` string, `resource_region` string, `access_key_id` string, `principal_id` string, `user_name` string, `s3_bucket_name` string, `service_raw` string, `resource_raw` string, `metadata_raw` string ) PARTITIONED BY ( `date` string ) ROW FORMAT SERDE \u0026#39;org.openx.data.jsonserde.JsonSerDe\u0026#39; WITH SERDEPROPERTIES ( \u0026#39;serialization.format\u0026#39; = \u0026#39;1\u0026#39; ) LOCATION \u0026#39;s3://processed-guardduty-findings-ACCOUNT_ID-REGION/processed-guardduty/\u0026#39; TBLPROPERTIES ( \u0026#39;classification\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;compressionType\u0026#39; = \u0026#39;gzip\u0026#39;, \u0026#39;projection.enabled\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;projection.date.type\u0026#39; = \u0026#39;date\u0026#39;, \u0026#39;projection.date.range\u0026#39; = \u0026#39;2025-01-01,NOW\u0026#39;, \u0026#39;projection.date.format\u0026#39; = \u0026#39;yyyy-MM-dd\u0026#39;, \u0026#39;projection.date.interval\u0026#39; = \u0026#39;1\u0026#39;, \u0026#39;projection.date.interval.unit\u0026#39; = \u0026#39;DAYS\u0026#39;, \u0026#39;storage.location.template\u0026#39; = \u0026#39;s3://processed-guardduty-findings-ACCOUNT_ID-REGION/processed-guardduty/date=${date}/\u0026#39; ); Create vpc_logs Table Run this DDL in Athena:\nCREATE EXTERNAL TABLE IF NOT EXISTS security_logs.vpc_logs ( `version` string, `account_id` string, `region` string, `vpc_id` string, `query_timestamp` string, `query_name` string, `query_type` string, `query_class` string, `rcode` string, `answers` string, `srcaddr` string, `srcport` int, `transport` string, `srcids_instance` string, `timestamp` string ) PARTITIONED BY ( `date` string ) ROW FORMAT SERDE \u0026#39;org.openx.data.jsonserde.JsonSerDe\u0026#39; WITH SERDEPROPERTIES ( \u0026#39;serialization.format\u0026#39; = \u0026#39;1\u0026#39;, \u0026#39;ignore.malformed.json\u0026#39; = \u0026#39;true\u0026#39; ) LOCATION \u0026#39;s3://processed-cloudwatch-logs-ACCOUNT_ID-REGION/vpc-logs/\u0026#39; TBLPROPERTIES ( \u0026#39;projection.enabled\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;projection.date.type\u0026#39; = \u0026#39;date\u0026#39;, \u0026#39;projection.date.format\u0026#39; = \u0026#39;yyyy-MM-dd\u0026#39;, \u0026#39;projection.date.range\u0026#39; = \u0026#39;2025-01-01,NOW\u0026#39;, \u0026#39;projection.date.interval\u0026#39; = \u0026#39;1\u0026#39;, \u0026#39;projection.date.interval.unit\u0026#39; = \u0026#39;DAYS\u0026#39;, \u0026#39;storage.location.template\u0026#39; = \u0026#39;s3://processed-cloudwatch-logs-ACCOUNT_ID-REGION/vpc-logs/date=${date}/\u0026#39;, \u0026#39;classification\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;compressionType\u0026#39; = \u0026#39;gzip\u0026#39; ); Create eni_flow_logs Table Run this DDL in Athena:\nCREATE EXTERNAL TABLE IF NOT EXISTS security_logs.eni_flow_logs ( `version` int, `account_id` string, `interface_id` string, `srcaddr` string, `dstaddr` string, `srcport` int, `dstport` int, `protocol` int, `packets` bigint, `bytes` bigint, `start_time` bigint, `end_time` bigint, `action` string, `log_status` string, `timestamp_str` string ) PARTITIONED BY ( `date` string ) ROW FORMAT SERDE \u0026#39;org.openx.data.jsonserde.JsonSerDe\u0026#39; WITH SERDEPROPERTIES ( \u0026#39;serialization.format\u0026#39; = \u0026#39;1\u0026#39; ) LOCATION \u0026#39;s3://processed-cloudwatch-logs-ACCOUNT_ID-REGION/eni-flow-logs/\u0026#39; TBLPROPERTIES ( \u0026#39;projection.enabled\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;projection.date.type\u0026#39; = \u0026#39;date\u0026#39;, \u0026#39;projection.date.format\u0026#39; = \u0026#39;yyyy-MM-dd\u0026#39;, \u0026#39;projection.date.range\u0026#39; = \u0026#39;2025-01-01,NOW\u0026#39;, \u0026#39;projection.date.interval\u0026#39; = \u0026#39;1\u0026#39;, \u0026#39;projection.date.interval.unit\u0026#39; = \u0026#39;DAYS\u0026#39;, \u0026#39;storage.location.template\u0026#39; = \u0026#39;s3://processed-cloudwatch-logs-ACCOUNT_ID-REGION/eni-flow-logs/date=${date}/\u0026#39;, \u0026#39;classification\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;compressionType\u0026#39; = \u0026#39;gzip\u0026#39; ); "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.7-dashboard-setup/5.7.2-setup-lambda/","title":"Create IAM Roles and Policies","tags":[],"description":"","content":"In this section, you will create IAM role and Policy for Lambda. After that you will create Lambda Function to execute query\nContent Create Lambda Execution Roles and Policy Create Lambda Function "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.3-foundation-setup/5.3.3-create-iam-roles-and-policies/5.3.3.2-create-service-roles/","title":"Create Service Roles","tags":[],"description":"","content":"Create Firehose Roles Create CloudTrailFirehoseRole Open IAM Console ‚Üí Roles ‚Üí Create role\nSelect trusted entity:\nTrusted entity type: AWS service Use case: Select \u0026ldquo;Kinesis\u0026rdquo; ‚Üí \u0026ldquo;Kinesis Firehose\u0026rdquo; Click \u0026ldquo;Next\u0026rdquo; Add permissions:\nSkip adding managed policies (we\u0026rsquo;ll add inline policy) Click \u0026ldquo;Next\u0026rdquo; Name and create:\nRole name: CloudTrailFirehoseRole Description: Allows Firehose to write CloudTrail logs to S3 Click \u0026ldquo;Create role\u0026rdquo; Add inline policy:\nPolicy name: FirehosePolicy Policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::processed-cloudtrail-logs-ACCOUNT_ID-REGION\u0026#34;, \u0026#34;arn:aws:s3:::processed-cloudtrail-logs-ACCOUNT_ID-REGION/*\u0026#34; ] } ] } Create CloudWatchFirehoseRole Role name: CloudWatchFirehoseRole Description: Allows Firehose to write CloudWatch logs to S3 Trusted entity: Kinesis Firehose Inline policy name: FirehosePolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::processed-cloudwatch-logs-ACCOUNT_ID-REGION\u0026#34;, \u0026#34;arn:aws:s3:::processed-cloudwatch-logs-ACCOUNT_ID-REGION/*\u0026#34; ] } ] } Create Step Functions Role Create StepFunctionsRole Create role:\nTrusted entity: Step Functions Role name: StepFunctionsRole Description: Execution role for Incident Response Step Functions Add TWO inline policies:\nPolicy 1: LambdaInvokePolicy\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:lambda:REGION:ACCOUNT_ID:function:ir-isolate-ec2-lambda\u0026#34;, \u0026#34;arn:aws:lambda:REGION:ACCOUNT_ID:function:ir-parse-findings-lambda\u0026#34;, \u0026#34;arn:aws:lambda:REGION:ACCOUNT_ID:function:ir-quarantine-iam-lambda\u0026#34; ] } ] } Policy 2: EC2AutoScalingPolicy\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;autoscaling:DescribeAutoScalingInstances\u0026#34;, \u0026#34;autoscaling:DetachInstances\u0026#34;, \u0026#34;autoscaling:UpdateAutoScalingGroup\u0026#34;, \u0026#34;ec2:CreateSnapshot\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:DescribeVolumes\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Create EventBridge Role Create IncidentResponseStepFunctionsEventRole Role name: IncidentResponseStepFunctionsEventRole Description: Allows EventBridge to trigger Step Functions Trusted entity: EventBridge Inline policy name: StartStepFunctionsPolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;states:StartExecution\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:REGION:ACCOUNT_ID:stateMachine:IncidentResponseStepFunctions\u0026#34; } ] } Create VPC Flow Logs Role Create FlowLogsIAMRole Create role:\nTrusted entity: EC2 (will edit trust policy) Role name: FlowLogsIAMRole Edit trust relationship to:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;vpc-flow-logs.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } Add inline policy: Policy name: FlowLogsPolicy Policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:DescribeLogStreams\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Create Glue Role Create GlueCloudWatchRole Role name: GlueCloudWatchRole Description: Allows Glue to access S3 and CloudWatch Logs Trusted entity: Glue Managed policies (attach 3): AWSGlueServiceRole CloudWatchLogsReadOnlyAccess AmazonS3FullAccess No inline policies needed "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.11-appendices/5.11.2-guardduty-etl/","title":"GuardDuty ETL Code","tags":[],"description":"","content":" import json import boto3 import gzip import os from datetime import datetime from urllib.parse import unquote_plus s3_client = boto3.client(\u0026#39;s3\u0026#39;) DATABASE_NAME = os.environ.get(\u0026#34;DATABASE_NAME\u0026#34;, \u0026#34;security_logs\u0026#34;) TABLE_NAME_GUARDDUTY = os.environ.get(\u0026#34;TABLE_NAME_GUARDDUTY\u0026#34;, \u0026#34;processed_guardduty\u0026#34;) S3_LOCATION_GUARDDUTY = os.environ.get(\u0026#34;S3_LOCATION_GUARDDUTY\u0026#34;, \u0026#34;s3://vel-processed-guardduty/processed-guardduty/\u0026#34;) DESTINATION_BUCKET = os.environ.get(\u0026#34;DESTINATION_BUCKET\u0026#34;, \u0026#34;vel-processed-guardduty\u0026#34;) def promote_network_details(finding_service): if not finding_service: return {} action = finding_service.get(\u0026#39;action\u0026#39;, {}) net_conn_action = action.get(\u0026#39;networkConnectionAction\u0026#39;, {}) if net_conn_action: remote_ip = net_conn_action.get(\u0026#39;remoteIpDetails\u0026#39;, {}).get(\u0026#39;ipAddressV4\u0026#39;) or \\ net_conn_action.get(\u0026#39;remoteIpDetails\u0026#39;, {}).get(\u0026#39;ipAddressV6\u0026#39;) return { \u0026#39;remote_ip\u0026#39;: remote_ip, \u0026#39;remote_port\u0026#39;: net_conn_action.get(\u0026#39;remotePortDetails\u0026#39;, {}).get(\u0026#39;port\u0026#39;), \u0026#39;connection_direction\u0026#39;: net_conn_action.get(\u0026#39;connectionDirection\u0026#39;), \u0026#39;protocol\u0026#39;: net_conn_action.get(\u0026#39;protocol\u0026#39;), } dns_action = action.get(\u0026#39;dnsRequestAction\u0026#39;, {}) if dns_action: return {\u0026#39;dns_domain\u0026#39;: dns_action.get(\u0026#39;domain\u0026#39;), \u0026#39;dns_protocol\u0026#39;: dns_action.get(\u0026#39;protocol\u0026#39;)} port_probe_action = action.get(\u0026#39;portProbeAction\u0026#39;, {}) if port_probe_action and port_probe_action.get(\u0026#39;portProbeDetails\u0026#39;): detail = port_probe_action[\u0026#39;portProbeDetails\u0026#39;][0] return { \u0026#39;scanned_ip\u0026#39;: detail.get(\u0026#39;remoteIpDetails\u0026#39;, {}).get(\u0026#39;ipAddressV4\u0026#39;), \u0026#39;scanned_port\u0026#39;: detail.get(\u0026#39;localPortDetails\u0026#39;, {}).get(\u0026#39;port\u0026#39;), } return {} def promote_api_details(finding_service): if not finding_service: return {} action = finding_service.get(\u0026#39;action\u0026#39;, {}) aws_api_action = action.get(\u0026#39;awsApiCallAction\u0026#39;, {}) if aws_api_action: return { \u0026#39;aws_api_service\u0026#39;: aws_api_action.get(\u0026#39;serviceName\u0026#39;), \u0026#39;aws_api_name\u0026#39;: aws_api_action.get(\u0026#39;api\u0026#39;), \u0026#39;aws_api_caller_type\u0026#39;: aws_api_action.get(\u0026#39;callerType\u0026#39;), \u0026#39;aws_api_error\u0026#39;: aws_api_action.get(\u0026#39;errorCode\u0026#39;), \u0026#39;aws_api_remote_ip\u0026#39;: aws_api_action.get(\u0026#39;remoteIpDetails\u0026#39;, {}).get(\u0026#39;ipAddressV4\u0026#39;), } return {} def promote_resource_details(finding_resource): if not finding_resource: return {} instance_details = finding_resource.get(\u0026#39;instanceDetails\u0026#39;, {}) if instance_details: return { \u0026#39;target_resource_arn\u0026#39;: instance_details.get(\u0026#39;arn\u0026#39;), \u0026#39;instance_id\u0026#39;: instance_details.get(\u0026#39;instanceId\u0026#39;), \u0026#39;resource_region\u0026#39;: instance_details.get(\u0026#39;awsRegion\u0026#39;), \u0026#39;instance_type\u0026#39;: instance_details.get(\u0026#39;instanceType\u0026#39;), \u0026#39;image_id\u0026#39;: instance_details.get(\u0026#39;imageId\u0026#39;), \u0026#39;instance_tags\u0026#39;: instance_details.get(\u0026#39;tags\u0026#39;) } access_key_details = finding_resource.get(\u0026#39;accessKeyDetails\u0026#39;, {}) if access_key_details: return { \u0026#39;access_key_id\u0026#39;: access_key_details.get(\u0026#39;accessKeyId\u0026#39;), \u0026#39;principal_id\u0026#39;: access_key_details.get(\u0026#39;principalId\u0026#39;), \u0026#39;user_name\u0026#39;: access_key_details.get(\u0026#39;userName\u0026#39;), } s3_details = finding_resource.get(\u0026#39;s3BucketDetails\u0026#39;, []) if s3_details: return { \u0026#39;target_resource_arn\u0026#39;: s3_details[0].get(\u0026#39;arn\u0026#39;), \u0026#39;s3_bucket_name\u0026#39;: s3_details[0].get(\u0026#39;name\u0026#39;), } return {} def process_guardduty_log(bucket, key): response = s3_client.get_object(Bucket=bucket, Key=key) if key.endswith(\u0026#39;.gz\u0026#39;): content = gzip.decompress(response[\u0026#39;Body\u0026#39;].read()).decode(\u0026#39;utf-8\u0026#39;) else: content = response[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8\u0026#39;) processed_findings = [] for line in content.splitlines(): if not line: continue try: finding = json.loads(line) except json.JSONDecodeError: print(f\u0026#34;Skipping malformed JSON line in {key}\u0026#34;); continue finding_type = finding.get(\u0026#39;type\u0026#39;, \u0026#39;UNKNOWN\u0026#39;) finding_service = finding.get(\u0026#39;service\u0026#39;, {}) network_fields = promote_network_details(finding_service) api_fields = promote_api_details(finding_service) resource_fields = promote_resource_details(finding.get(\u0026#39;resource\u0026#39;, {})) created_at_str = finding.get(\u0026#39;createdAt\u0026#39;) event_last_seen_str = finding_service.get(\u0026#39;eventLastSeen\u0026#39;) dt_obj = datetime.now() if event_last_seen_str: try: dt_obj = datetime.strptime(event_last_seen_str, \u0026#39;%Y-%m-%dT%H:%M:%S.%fZ\u0026#39;) except ValueError: try: dt_obj = datetime.strptime(event_last_seen_str, \u0026#39;%Y-%m-%dT%H:%M:%SZ\u0026#39;) except ValueError: pass elif created_at_str: try: dt_obj = datetime.strptime(created_at_str, \u0026#39;%Y-%m-%dT%H:%M:%S.%fZ\u0026#39;) except ValueError: try: dt_obj = datetime.strptime(created_at_str, \u0026#39;%Y-%m-%dT%H:%M:%SZ\u0026#39;) except ValueError: pass processed_record = { \u0026#39;finding_id\u0026#39;: finding.get(\u0026#39;id\u0026#39;), \u0026#39;finding_type\u0026#39;: finding_type, \u0026#39;title\u0026#39;: finding.get(\u0026#39;title\u0026#39;), \u0026#39;severity\u0026#39;: finding.get(\u0026#39;severity\u0026#39;), \u0026#39;account_id\u0026#39;: finding.get(\u0026#39;accountId\u0026#39;), \u0026#39;region\u0026#39;: finding.get(\u0026#39;region\u0026#39;), \u0026#39;created_at\u0026#39;: created_at_str, \u0026#39;event_last_seen\u0026#39;: event_last_seen_str, **network_fields, **api_fields, **resource_fields, \u0026#39;date\u0026#39;: dt_obj.strftime(\u0026#39;%Y-%m-%d\u0026#39;), \u0026#39;service_raw\u0026#39;: json.dumps(finding_service), \u0026#39;resource_raw\u0026#39;: json.dumps(finding.get(\u0026#39;resource\u0026#39;, {})), \u0026#39;metadata_raw\u0026#39;: json.dumps(finding.get(\u0026#39;metadata\u0026#39;, {})), } processed_findings.append(processed_record) return processed_findings def save_processed_data(processed_events, source_key): if not processed_events: return first_event = processed_events[0] date_str = first_event.get(\u0026#39;date\u0026#39;, datetime.now().strftime(\u0026#39;%Y-%m-%d\u0026#39;)) original_filename = source_key.split(\u0026#39;/\u0026#39;)[-1].replace(\u0026#39;.gz\u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#39;.json\u0026#39;, \u0026#39;\u0026#39;) output_key = f\u0026#34;processed-guardduty/date={date_str}/{original_filename}_processed.jsonl.gz\u0026#34; json_lines = \u0026#34;\u0026#34; for event in processed_events: event_to_dump = event.copy() json_lines += json.dumps(event_to_dump) + \u0026#34;\\n\u0026#34; compressed_data = gzip.compress(json_lines.encode(\u0026#39;utf-8\u0026#39;)) s3_client.put_object( Bucket=DESTINATION_BUCKET, Key=output_key, Body=compressed_data, ContentType=\u0026#39;application/jsonl\u0026#39;, ContentEncoding=\u0026#39;gzip\u0026#39; ) print(f\u0026#34;Saved processed data to: s3://{DESTINATION_BUCKET}/{output_key}\u0026#34;) def lambda_handler(event, context): for record in event[\u0026#39;Records\u0026#39;]: bucket = record[\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] key = unquote_plus(record[\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;]) print(f\u0026#34;Processing GuardDuty finding file: s3://{bucket}/{key}\u0026#34;) try: processed_findings = process_guardduty_log(bucket, key) save_processed_data(processed_findings, key) print(f\u0026#34;Successfully processed {len(processed_findings)} findings from {key}\u0026#34;) except Exception as e: print(f\u0026#34;Error processing {key}: {str(e)}\u0026#34;) raise e return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;GuardDuty findings processed successfully\u0026#39;) } "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.7-dashboard-setup/5.7.2-setup-lambda/5.7.2.2-create-lambda-function/","title":"Lambda setup","tags":[],"description":"","content":"In this guide, you will setup a Lambda using Python to execute query using Athena service.\nCreate Lambda Function Open the Lambda Console\nNavigate to https://console.aws.amazon.com/lambda/ Or: AWS Management Console ‚Üí Services ‚Üí Lambda Create Function:\nClick the Create Function In the create setting use the following setting: Choose Author from scratch Name: dashboard-query Runtime: Python 3.12 Architecture: x86_64 Change default execution role: Use an existing role Choose dashboard-query-role Click Create Add code:\nIn the code editor copy and paste the codes below then click Deply: import boto3 import time import os import json athena = boto3.client(\u0026#39;athena\u0026#39;) RESOURCE_MAP = { \u0026#39;/logs/cloudtrail\u0026#39;: { \u0026#39;db\u0026#39;: \u0026#39;security_logs\u0026#39;, \u0026#39;table\u0026#39;: \u0026#39;processed_cloudtrail\u0026#39; }, \u0026#39;/logs/guardduty\u0026#39;: { \u0026#39;db\u0026#39;: \u0026#39;security_logs\u0026#39;, \u0026#39;table\u0026#39;: \u0026#39;processed_guardduty\u0026#39; }, \u0026#39;/logs/vpc\u0026#39;: { \u0026#39;db\u0026#39;: \u0026#39;security_logs\u0026#39;, \u0026#39;table\u0026#39;: \u0026#39;vpc_logs\u0026#39; }, \u0026#39;/logs/eni_logs\u0026#39;:{ \u0026#39;db\u0026#39;: \u0026#39;security_logs\u0026#39;, \u0026#39;table\u0026#39;: \u0026#39;eni_flow_logs\u0026#39; } } OUTPUT_BUCKET_NAME = os.environ.get(\u0026#34;ATHENA_OUTPUT_BUCKET\u0026#34;) REGION = os.environ.get(\u0026#34;REGION\u0026#34;) OUTPUT_BUCKET = f\u0026#39;s3://{OUTPUT_BUCKET_NAME}/\u0026#39; def lambda_handler(event, context): print(\u0026#34;Received event:\u0026#34;, json.dumps(event)) resource_path = event.get(\u0026#39;resource\u0026#39;) config = RESOURCE_MAP.get(resource_path) if not config: return api_response(400, {\u0026#39;error\u0026#39;: f\u0026#39;Unknown resource path: {resource_path}\u0026#39;}) database_name = config[\u0026#39;db\u0026#39;] table_name = config[\u0026#39;table\u0026#39;] query_params = event.get(\u0026#39;queryStringParameters\u0026#39;, {}) or {} if config[\u0026#39;table\u0026#39;] == \u0026#39;processed_cloudtrail\u0026#39;: query_string = f\u0026#34;\u0026#34;\u0026#34;SELECT * FROM {table_name} where \u0026#34;date\u0026#34; \u0026gt;= cast((current_date - interval \u0026#39;3\u0026#39; day) as varchar) order by eventtime desc\u0026#34;\u0026#34;\u0026#34; elif config[\u0026#39;table\u0026#39;] == \u0026#39;processed_guardduty\u0026#39;: query_string = f\u0026#34;\u0026#34;\u0026#34;SELECT * FROM {table_name} where \u0026#34;date\u0026#34; \u0026gt;= cast((current_date - interval \u0026#39;3\u0026#39; day) as varchar) order by date desc\u0026#34;\u0026#34;\u0026#34; elif config[\u0026#39;table\u0026#39;] == \u0026#39;vpc_logs\u0026#39;: query_string = f\u0026#34;\u0026#34;\u0026#34;SELECT * FROM {table_name} where \u0026#34;date\u0026#34; \u0026gt;= cast((current_date - interval \u0026#39;3\u0026#39; day) as varchar) order by timestamp desc\u0026#34;\u0026#34;\u0026#34; elif config[\u0026#39;table\u0026#39;] == \u0026#39;eni_flow_logs\u0026#39;: query_string = f\u0026#34;\u0026#34;\u0026#34;SELECT * FROM {table_name} where \u0026#34;date\u0026#34; \u0026gt;= cast((current_date - interval \u0026#39;3\u0026#39; day) as varchar) order by timestamp_str desc\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;Querying DB: {database_name}, Table: {table_name}, Output: {OUTPUT_BUCKET}\u0026#34;) try: response = athena.start_query_execution( QueryString=query_string, QueryExecutionContext={\u0026#39;Database\u0026#39;: database_name}, ResultConfiguration={\u0026#39;OutputLocation\u0026#39;: OUTPUT_BUCKET} ) query_execution_id = response[\u0026#39;QueryExecutionId\u0026#39;] status = \u0026#39;RUNNING\u0026#39; while status in [\u0026#39;RUNNING\u0026#39;, \u0026#39;QUEUED\u0026#39;]: response = athena.get_query_execution(QueryExecutionId=query_execution_id) status = response[\u0026#39;QueryExecution\u0026#39;][\u0026#39;Status\u0026#39;][\u0026#39;State\u0026#39;] if status in [\u0026#39;FAILED\u0026#39;, \u0026#39;CANCELLED\u0026#39;]: reason = response[\u0026#39;QueryExecution\u0026#39;][\u0026#39;Status\u0026#39;].get(\u0026#39;StateChangeReason\u0026#39;, \u0026#39;Unknown\u0026#39;) return api_response(500, {\u0026#39;error\u0026#39;: f\u0026#39;Query Failed: {reason}\u0026#39;}) time.sleep(1) results = athena.get_query_results(QueryExecutionId=query_execution_id) return api_response(200, results) except Exception as e: print(f\u0026#34;Error: {str(e)}\u0026#34;) return api_response(500, {\u0026#39;error\u0026#39;: str(e)}) def api_response(code, body): return { \u0026#34;statusCode\u0026#34;: code, \u0026#34;headers\u0026#34;: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Access-Control-Allow-Origin\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Access-Control-Allow-Methods\u0026#34;: \u0026#34;GET, OPTIONS\u0026#34; }, \u0026#34;body\u0026#34;: json.dumps(body) } "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.3-foundation-setup/5.3.2-set-up-s3-buckets-policies/","title":"Set up S3 buckets policies","tags":[],"description":"","content":"In this section, you will configure the bucket policy for the primary log bucket to allow CloudTrail, GuardDuty, and CloudWatch Logs to write logs.\nConfigure Bucket Policy Navigate to the primary log bucket: In S3 Console, click on incident-response-log-list-bucket-ACCOUNT_ID-REGION Open the Permissions tab:\nClick on the \u0026ldquo;Permissions\u0026rdquo; tab Scroll to Bucket policy:\nScroll down to the \u0026ldquo;Bucket policy\u0026rdquo; section Click \u0026ldquo;Edit\u0026rdquo; Paste the bucket policy: Copy the following JSON policy Important: Replace ACCOUNT_ID and REGION with your actual values in the policy { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowGuardDutyPutObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;guardduty.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:PutObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceAccount\u0026#34;: \u0026#34;ACCOUNT_ID\u0026#34; }, \u0026#34;ArnLike\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:guardduty:REGION:ACCOUNT_ID:detector/*\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowGuardDutyGetBucketLocation\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;guardduty.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceAccount\u0026#34;: \u0026#34;ACCOUNT_ID\u0026#34; }, \u0026#34;ArnLike\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:guardduty:REGION:ACCOUNT_ID:detector/*\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCloudWatchLogsGetBucketAcl\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;logs.REGION.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceAccount\u0026#34;: \u0026#34;ACCOUNT_ID\u0026#34; }, \u0026#34;ArnLike\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:logs:REGION:ACCOUNT_ID:log-group:*\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCloudWatchLogsPutObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;logs.REGION.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:PutObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceAccount\u0026#34;: \u0026#34;ACCOUNT_ID\u0026#34; }, \u0026#34;ArnLike\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:logs:REGION:ACCOUNT_ID:log-group:*\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCloudTrailAclCheck\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudtrail.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceAccount\u0026#34;: \u0026#34;ACCOUNT_ID\u0026#34; }, \u0026#34;ArnLike\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:cloudtrail:REGION:ACCOUNT_ID:trail/*\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCloudTrailWrite\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudtrail.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:PutObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/AWSLogs/ACCOUNT_ID/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;s3:x-amz-acl\u0026#34;: \u0026#34;bucket-owner-full-control\u0026#34;, \u0026#34;aws:SourceAccount\u0026#34;: \u0026#34;ACCOUNT_ID\u0026#34; }, \u0026#34;ArnLike\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:cloudtrail:REGION:ACCOUNT_ID:trail/*\u0026#34; } } } ] } Click \u0026ldquo;Save changes\u0026rdquo;\nVerify policy is saved: You should see the policy displayed in the Bucket policy section\n"},{"uri":"https://quanntm1206.github.io/AWS-FCJ/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Compute \u0026amp; Storage Mastery: Deep dive into EC2 families, Pricing models, and Storage solutions (EBS, EFS, FSx). Database \u0026amp; Migration: Understanding RDS integration and Lift-and-Shift migration tools (MGN). Advanced Lab 5: Deploying a 2-tier application (Node.js + RDS) on Amazon Linux 2023 with strict troubleshooting. Cost \u0026amp; Security: Implementing IAM policies for cost control and resource restriction. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon (15/9) Module 3: EC2 Deep Dive, Storage \u0026amp; Migration - Analyze Instance families (Intel/AMD vs Graviton/ARM) and Nitro Hypervisor benefits. - Compare Pricing Models: On-Demand, Reserved, Savings Plans, and Spot Instances (Auto Scaling integration). - Storage: Differentiate EBS (Persistent) vs Instance Store (Ephemeral/NVMe). Learn use cases for EFS (Linux shared) vs FSx (Windows/HPC). - Migration: Study AWS MGN (Application Migration Service) for continuous replication and cutover. 15/09/2025 16/09/2025 Module 03-01 Module 03-01-01 Module 03-01-02 Module 03-01-03 Module 03-01-04 Module 03-01-05 Module 03-01-06 Module 03-01-07 Module 03-02 Wed (17/9) EC2 Management \u0026amp; Security: - Practice: Create Custom AMI, EBS Snapshots, and test Instance Recovery. - Cost Control: Configure IAM Policies to restrict Regions, Instance Types, and enforce IP/Time-based rules. 17/09/2025 17/09/2025 Thu (18/9) Lab 5: Amazon Relational Database Service (Amazon RDS) - Design VPC network and Security Groups (EC2 \u0026amp; RDS communication). - Provision RDS (MySQL) and Amazon Linux 2023 instance (switched to t3.micro due to t2.micro unavailability). - Troubleshoot Customer Gateway timeouts during SSH setup. - App Deployment: Setup Node.js User Management App. - Troubleshooting: + Fix npm start error (updated package.json). + Resolve MySQL installation failure on AL2023 by connecting directly to RDS endpoint. + Create DB Users and grant permissions via SQL commands. - Explore Amazon Lightsail for simplified virtual server needs. - Review deployment procedures for both Windows Server 2022 and Linux. - Cleanup resources to prevent billing leakage. 18/09/2025 20/09/2025 https://000005.awsstudygroup.com/ Week 2 Achievements: Successfully Completed Lab 5 (2-Tier Architecture):\nDeployed a fully functional Node.js application connected to an RDS MySQL database. Verified full application functionality (Register/Login/Data Retrieval) after resolving connectivity issues. Solved Compatibility Issues on Amazon Linux 2023:\nAdapted to infrastructure constraints by upgrading from t2.micro to t3.micro. Identified that mysql-community-server and mariadb105 were unstable/missing on AL2023. Re-architected the database connection to point directly to the RDS Endpoint, bypassing local proxy issues. Advanced EC2 \u0026amp; Database Management:\nMastered the creation of Custom AMIs and EBS Snapshots for backup/recovery. Successfully managed RDS Database users: Created specific app users (appuser) and granted granular privileges (GRANT ALL) for security. Applied strict IAM Policies to limit resource creation (Region/Type restrictions), ensuring zero accidental costs. "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Summary Report: ‚ÄúAWS Cloud Mastery Series #2 - DevOps on AWS‚Äù Event Objectives To introduce AWS DevOps Services, with a specific focus on CI/CD Pipelines. To provide a comprehensive overview of Infrastructure as Code (IaC) and its associated tools. To explore the various Container Services available within the AWS ecosystem. To ensure robust Monitoring and Observability capabilities using AWS Services. Speakers Truong Quang Tinh ‚Äì AWS Community Builder, Platform Engineer - TymeX Bao Huynh ‚Äì AWS Community Builder Nguyen Khanh Phuc Thinh ‚Äì AWS Community Builder Tran Dai Vi ‚Äì AWS Community Builder Huynh Hoang Long ‚Äì AWS Community Builder Pham Hoang Quy ‚Äì AWS Community Builder Nghiem Le ‚Äì AWS Community Builder Dinh Le Hoang Anh - Cloud Engineer Trainee, First Cloud AI Journey Key Highlights DevOps Mindset - Culture: This concept encompasses collaboration, automation, continuous learning, and consistent measurement. - DevOps Roles: Key positions include DevOps Engineer, Cloud Engineer, Platform Engineer, and Site Reliability Engineer (SRE). - Success Metrics: + Ensuring deployment health. + Enhancing agility. + Maintaining system stability. + Optimizing the customer experience. + Justifying technology investments.\nDO DON\u0026rsquo;T Start with Fundamentals Stay in \u0026ldquo;Tutorial Hell\u0026rdquo; Learn by Building Real Projects Copy-paste blindly Document Everything Compare Your Progress to Others Master one thing at a time Give Up After Failures Soft Skills Enhancement - Continuous Integration: A practice wherein team members frequently integrate their work, aiming to facilitate efficient Continuous Delivery and Deployment.\nInfrastructure as Code (IaC) - Benefits: This approach offers automation, scalability, reproducibility, and significantly improved collaboration.\nAWS CloudFormation This constitutes AWS\u0026rsquo;s native IaC solution. It utilizes templates written in YAML or JSON to automatically provision every component of the AWS infrastructure.\n- Stack: A collection of AWS resources defined within a template, which CloudFormation allows users to create, update, or delete as a single unit.\n- CloudFormation Template: A YAML or JSON file that acts as a blueprint for configuring and deploying resources, effectively defining the AWS infrastructure.\n- Workflow: Create a template -\u0026gt; Store it in an S3 Bucket or local storage -\u0026gt; Use CloudFormation to create Stacks based on the template -\u0026gt; CloudFormation provisions the resources.\n- Drift Detection: This feature identifies discrepancies between the actual infrastructure and the Stack configuration. It allows administrators to update the Stack or revert changes, which is essential for maintaining version control.\nAWS Cloud Development Kit (CDK) An open-source software development framework that supports IaC using standard programming languages, including Python, Java, C#.Net, TypeScript/JavaScript, and Go.\n- Construct: These are the basic building blocks, comprised of components representing AWS resources and their configurations. There are three construct levels: + L1 Construct: Low-level resources that map directly to a single AWS CloudFormation resource. + L2 Construct: Provides a higher-level abstraction through an intuitive, intent-based API, encapsulating best practices and security defaults. + L3 Construct: Represents complete architectural patterns involving multiple resources, offering opinionated implementations for rapid deployment.\nAWS Amplify An AWS platform designed to facilitate the building, deploying, and scaling of web and mobile applications. It utilizes CloudFormation under the hood, deploying Stacks to build infrastructure programmatically.\nTerraform An IaC tool wherein infrastructure is defined through Terraform code. Users plan and subsequently apply the infrastructure across multiple cloud platforms, such as Azure, AWS, and Google Cloud.\n- Strength: Its primary advantage lies in multi-cloud support and state tracking using a unified configuration.\nHow to Choose IaC Tools? - Criteria: + Are you planning to utilize a single cloud provider or a multi-cloud strategy? + Is your primary role that of a Developer or Operations? + Does the specific cloud ecosystem fully support the tool?\nContainer Services on AWS Dockerfile A Dockerfile delineates the procedure for building a container image. It describes the environment, dependencies, build steps, and final runtime configuration, ensuring that the application runs consistently across any system supporting Docker.\n- Images: A packaged blueprint of an application, built from a Dockerfile using a layered file system. These are utilized to instantiate containers consistently across diverse environments.\n- Workflow: A Dockerfile is employed to build a Docker Image, which can then run a Container and be pushed to ECR or Docker Hub.\nAmazon ECR A fully managed container registry that simplifies the storage, management, and secure sharing of Docker container images. It serves as AWS\u0026rsquo;s proprietary, secure, and scalable private container registry.\n- Features: + Image Scanning + Immutable Tags + Lifecycle Policies + Encryption \u0026amp; IAM\n- Orchestration: This involves managing numerous container processes, including restarting containers, automatically scaling up under high load, efficiently distributing traffic, and managing where containers are placed and executed.\nKubernetes An open-source system that automates deployment, scaling, healing, and load balancing. - Components: + Master Node: The Control Plane that manages worker nodes and pods. + Worker Node: Runs application workloads inside pods. + Pod: The smallest deployable unit, which can contain one or more containers. + Service\nComparison: ECS vs EKS\nFeature Amazon ECS (Elastic Container Service) Amazon EKS (Elastic Kubernetes Service) Core Technology AWS-native container orchestration Kubernetes-based (open-source standard) Complexity Simpler, easier to operate Highly flexible but more complex Knowledge Required No Kubernetes knowledge needed Requires Kubernetes knowledge (pods, deployments, etc.) AWS Integration Deep AWS integration (ALB, IAM, CloudWatch, etc.) Standard Kubernetes integration Use Case/Benefits Great for fast deployments \u0026amp; lower ops overhead Multi-cluster, multi-cloud portability Ecosystem/Community AWS-native tools and community Larger ecosystem \u0026amp; community tools Summary ECS = easier, faster to run, lower operational overhead EKS = more flexibility, more control, more complexity App Runner This service is suitable for the rapid deployment of web applications and REST APIs, making it ideal for small to medium production workloads.\nMonitoring \u0026amp; Observability CloudWatch Monitors AWS resources and applications running on AWS in real-time. Provides comprehensive observability. Enables alarms and automated responses. Offers dashboards to assist with operational and cost optimization. - CloudWatch metrics: Performance data collected from systems on AWS or on-premise (via CloudWatch Agent), integrating seamlessly with EventBridge, Auto Scaling, and DevOps workflows.\nAWS X-Ray - Distributed Tracing: Tracks requests end-to-end and maps paths between visited services. It necessitates adding an SDK to the code to generate trace IDs.\n- Performance Insight: Facilitates root cause analysis for latency and errors by deducing insights from traces and providing Real User Monitoring.\nEvent Experience This event proved pivotal for our project, as it directly addressed our plan to implement IaC using CDK, replacing our current \u0026ldquo;ClickOps\u0026rdquo; approach to enhance maintainability and reproducibility. Additionally, the insights regarding CloudWatch significantly bolstered our data monitoring strategy.\nThe speakers addressed several key questions from our team:\nQ: To date, our project has been built entirely using ClickOps, and we plan to transition to CDK. Is there a tool available that can scan and convert our existing infrastructure into CDK or CloudFormation templates, avoiding the need to rebuild from scratch?\nA: Regrettably, there is currently no tool capable of fully automating this conversion. Consequently, the team will need to reconstruct the infrastructure from scratch. However, should you discover a tool that assists with this, please share it with the community.\nQ: We noticed that AWS X-Ray, when used with CloudWatch, appears similar to CloudTrail in its tracing method. Could you explain the key differences?\nA: X-Ray is integrated with CloudWatch and is specifically used to trace the path through resources and services that the system interacts with. In contrast, CloudTrail is primarily utilized to log and audit actions taken by AWS users (API calls).\nQ: Our project relies on GuardDuty Findings. Do you have any experience with reliably triggering Findings for demo scenarios?\nA: In my experience, GuardDuty Findings can be triggered by port scanning activities, although other methods certainly exist.\nA: GuardDuty can also be configured with a threat list containing custom rules to trigger findings upon activities related to specific malicious domains or IPs.\nThis event also marked the debut presentation for some of the speakers:\nThe DevOps and IaC sections were delivered with high proficiency. The Monitoring \u0026amp; Observability section was slightly less polished, and the speaker\u0026rsquo;s nervousness was perceptible; however, they still delivered significant value. Some event photos "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"OSPAR 2025 report: 170 services in scope based on enhanced OSPAR v2.0 guidelines We‚Äôre pleased to announce the completion of our annual AWS Outsourced Service Provider‚Äôs Audit Report (OSPAR) audit cycle on August 7, 2025, based on the newly enhanced version 2.0 guidelines (OSPAR v2.0). AWS is the first global cloud service provider in Singapore to obtain the report using the new OSPAR v2.0 guidelines.\nABS Guidelines and OSPAR v2.0 Standards The Association of Banks in Singapore (ABS) established the Guidelines on Control Objectives and Procedures for Outsourced Service Providers (ABS Guidelines) to provide baseline controls criteria that outsourced service providers (OSPs) operating in Singapore should have in place.\nABS enhanced the ABS Guidelines to version 2.0, which OSPs‚Äîsuch as AWS‚Äîneed to comply with for the audit period commencing on or after January 1, 2025.\nThe enhanced ABS Guidelines integrate key elements from the Monetary Authority of Singapore (MAS) regulatory updates on:\nCyber hygiene Technology risk management Business continuity management Additionally, it includes new control domains such as data security, cryptography, and software application development and management.\nExpanded Scope: 170 Services The 2025 OSPAR certification cycle includes the addition of seven (7) new services in scope, bringing the total number of services in scope to 170 in the AWS Asia Pacific (Singapore) Region.\nNewly added services in scope include:\nAmazon DynamoDB Accelerator (DAX) Amazon Security Lake Amazon Verified Permissions AWS Payment Cryptography AWS Resource Explorer AWS Verified Access AWS Wickr Successfully completing the OSPAR assessment demonstrates that AWS continues to maintain a robust system of controls to meet these guidelines. This underscores our commitment to fulfill the security expectations for cloud service providers set by the financial services industry in Singapore.\nCustomers can use OSPAR to streamline their due diligence processes, thereby reducing the effort and costs associated with compliance. OSPAR remains a core assurance program for our financial services customers because it is closely aligned with local regulatory requirements from MAS.\nHow to Access the Report You can download the latest OSPAR report from AWS Artifact, a self-service portal for on-demand access to AWS compliance reports.\nSign in to AWS Artifact in the AWS Management Console. Search for the 2025 OSPAR report. The list of services in scope for OSPAR is available in the report and on the AWS Services in Scope by Compliance Program webpage. As always, we‚Äôre committed to bringing new services into the scope of our OSPAR program based on your architectural and regulatory needs. If you have questions about the OSPAR report, contact your AWS account team.\nSource: Joseph Goh | 16 SEP 2025\n"},{"uri":"https://quanntm1206.github.io/AWS-FCJ/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section lists and provides concise summaries of the blog posts that have been translated.\nBlog 1 - Democratizing Quantum Resources: U-Mich \u0026amp; AWS This blog introduces the collaboration between the University of Michigan and AWS to deploy QREAL, a cloud-accessible quantum testing platform. You will explore how AWS serverless technologies (Fargate, Lambda, RDS) facilitate the democratization of access to quantum resources for both educational and research purposes. The article elucidates the \u0026ldquo;fabless\u0026rdquo; model, which enables researchers to conduct remote experiments on actual hardware, while assisting students in visualizing complex concepts such as quantum entanglement.\nBlog 2 - Amazon MSK Replicator vs. MirrorMaker 2 This blog offers a strategic comparison between Amazon MSK Replicator and Apache Kafka MirrorMaker 2 regarding data replication. You will learn how to select the appropriate tool for disaster recovery (DR) and data migration scenarios. The post analyzes when to utilize the fully managed MSK Replicator (optimal for same-account MSK-to-MSK replication) versus MirrorMaker 2 (ideal for hybrid, cross-account, or custom policy scenarios), while also outlining multi-Region architectures.\nBlog 3 - OSPAR 2025 Report: New Services in Scope This announcement highlights AWS‚Äôs completion of the OSPAR 2025 audit under the newly enhanced v2.0 guidelines, encompassing 170 services within the Singapore region. You will read about the stringent compliance standards established by the Association of Banks in Singapore (ABS) and the Monetary Authority of Singapore (MAS). The article also details seven newly added in-scope services, such as Amazon Security Lake and AWS Payment Cryptography, ensuring that financial institutions can innovate securely.\n"},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.7-dashboard-setup/5.7.3-setup-api-gateway/","title":"API Gateway Setup","tags":[],"description":"","content":"In this guide, you will setup an API Gateway to route api call from dashboard to Lambda.\nCreate API Gateway Open the API Gateway Console\nNavigate to https://console.aws.amazon.com/apigateway/ Or: AWS Management Console ‚Üí Services ‚Üí API Gateway Create API:\nClick Create API Choose REST API and click Build Use this setting for creation: Choose New API Name: dashboard-api API endpoint type: Regional Security policy: SecurityPolicy_TLS13_1_3_2025_09 Endpoint access mode: Basic IP address type: IPv4 Create Resources:\nEnable CORS for the root resource Click Create resource and name it logs Then click on /logs resource that just created and click Create Resource to create child resource of /logs Name it cloudtrail and enable CORS Repeat this three more times for eni_logs, guardduty and vpc Create methods:\nClick on /cloudtrail that just created and click Cretae method\nIn method creation, use this setting:\nMethod type: GET Intergration type: Lambda function Enable Lambda proxy intergration choose Buffered Lambda function: select your region search for dashboard-query and choose it Timout: 29000 Repeat this three more time for eni_logs, guardduty and vpc\nDeploy API:\nClick the Deploy API on the right corner In deploy API, use this setting: Stage: New stage Name: prod Click Deploy "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.11-appendices/5.11.3-cloudwatch-etl/","title":"CloudWatch ETL Code","tags":[],"description":"","content":"import json import boto3 import gzip import re import os from datetime import datetime, timezone s3 = boto3.client(\u0026#34;s3\u0026#34;) firehose= boto3.client(\u0026#34;firehose\u0026#34;) # -------------------------------------------------- # CONFIG # -------------------------------------------------- SOURCE_PREFIX = \u0026#34;exportedlogs/vpc-dns-logs/\u0026#34; FIREHOSE_STREAM_NAME = os.environ.get(\u0026#34;FIREHOSE_STREAM_NAME\u0026#34;) VPC_RE = re.compile(r\u0026#34;/(vpc-[0-9A-Za-z\\-]+)\u0026#34;) ISO_TS_RE = re.compile(r\u0026#34;^\\d{4}-\\d{2}-\\d{2}T\u0026#34;) def read_gz(bucket, key): obj = s3.get_object(Bucket=bucket, Key=key) with gzip.GzipFile(fileobj=obj[\u0026#34;Body\u0026#34;]) as f: return f.read().decode(\u0026#34;utf-8\u0026#34;, errors=\u0026#34;replace\u0026#34;) def flatten_once(d): out = {} for k, v in (d or {}).items(): if isinstance(v, dict): for k2, v2 in v.items(): out[f\u0026#34;{k}_{k2}\u0026#34;] = v2 else: out[k] = v return out def safe_int(x): try: return int(x) except: return None def parse_dns_line(line): raw = line.strip() if not raw: return None json_part = raw prefix_ts = None if ISO_TS_RE.match(raw): try: prefix_ts, rest = raw.split(\u0026#34; \u0026#34;, 1) json_part = rest except: pass if not json_part.startswith(\u0026#34;{\u0026#34;): idx = json_part.find(\u0026#34;{\u0026#34;) if idx != -1: json_part = json_part[idx:] try: obj = json.loads(json_part) except: return None flat = flatten_once(obj) if prefix_ts: flat[\u0026#34;_prefix_ts\u0026#34;] = prefix_ts return flat def lambda_handler(event, context): print(f\u0026#34;Received S3 Event. Records: {len(event.get(\u0026#39;Records\u0026#39;, []))}\u0026#34;) firehose_records = [] for record in event.get(\u0026#34;Records\u0026#34;, []): if \u0026#34;s3\u0026#34; not in record: continue bucket = record[\u0026#34;s3\u0026#34;][\u0026#34;bucket\u0026#34;][\u0026#34;name\u0026#34;] key = record[\u0026#34;s3\u0026#34;][\u0026#34;object\u0026#34;][\u0026#34;key\u0026#34;] if not key.startswith(SOURCE_PREFIX) or not key.endswith(\u0026#34;.gz\u0026#34;): print(f\u0026#34;Skipping file: {key}\u0026#34;) continue print(f\u0026#34;Processing S3 file: {key}\u0026#34;) # Extract VPC ID from file path vpc_id_match = VPC_RE.search(key) vpc_id = vpc_id_match.group(1) if vpc_id_match else \u0026#34;unknown\u0026#34; # Read and process file content content = read_gz(bucket, key) if not content: continue for line in content.splitlines(): r = parse_dns_line(line) if not r: continue # Create flattened JSON record out = { \u0026#34;version\u0026#34;: r.get(\u0026#34;version\u0026#34;), \u0026#34;account_id\u0026#34;: r.get(\u0026#34;account_id\u0026#34;), \u0026#34;region\u0026#34;: r.get(\u0026#34;region\u0026#34;), \u0026#34;vpc_id\u0026#34;: r.get(\u0026#34;vpc_id\u0026#34;, vpc_id), \u0026#34;query_timestamp\u0026#34;: r.get(\u0026#34;query_timestamp\u0026#34;), \u0026#34;query_name\u0026#34;: r.get(\u0026#34;query_name\u0026#34;), \u0026#34;query_type\u0026#34;: r.get(\u0026#34;query_type\u0026#34;), \u0026#34;query_class\u0026#34;: r.get(\u0026#34;query_class\u0026#34;), \u0026#34;rcode\u0026#34;: r.get(\u0026#34;rcode\u0026#34;), \u0026#34;answers\u0026#34;: json.dumps(r.get(\u0026#34;answers\u0026#34;), ensure_ascii=False), \u0026#34;srcaddr\u0026#34;: r.get(\u0026#34;srcaddr\u0026#34;), \u0026#34;srcport\u0026#34;: safe_int(r.get(\u0026#34;srcport\u0026#34;)), \u0026#34;transport\u0026#34;: r.get(\u0026#34;transport\u0026#34;), \u0026#34;srcids_instance\u0026#34;: r.get(\u0026#34;srcids_instance\u0026#34;), \u0026#34;timestamp\u0026#34;: (r.get(\u0026#34;query_timestamp\u0026#34;) or r.get(\u0026#34;timestamp\u0026#34;) or r.get(\u0026#34;_prefix_ts\u0026#34;)) } # Add newline for JSONL format json_row = json.dumps(out, ensure_ascii=False) + \u0026#34;\\n\u0026#34; firehose_records.append({\u0026#39;Data\u0026#39;: json_row}) # Send to Firehose in batches of 500 if firehose_records: total_records = len(firehose_records) print(f\u0026#34;Sending {total_records} records to Firehose...\u0026#34;) batch_size = 500 for i in range(0, total_records, batch_size): batch = firehose_records[i:i + batch_size] try: response = firehose.put_record_batch( DeliveryStreamName=FIREHOSE_STREAM_NAME, Records=batch ) if response[\u0026#39;FailedPutCount\u0026#39;] \u0026gt; 0: print(f\u0026#34;Warning: {response[\u0026#39;FailedPutCount\u0026#39;]} records failed\u0026#34;) except Exception as e: print(f\u0026#34;Firehose error: {e}\u0026#34;) return {\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;, \u0026#34;total_records\u0026#34;: len(firehose_records)} "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.3-foundation-setup/5.3.3-create-iam-roles-and-policies/5.3.3.3-create-iam-policy/","title":"Create IAM Policy","tags":[],"description":"","content":"Create IAM Quarantine Policy Create IrQuarantineIAMPolicy Navigate to IAM Console ‚Üí Policies ‚Üí Create policy\nPolicy JSON:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Policy name: IrQuarantineIAMPolicy Description: Deny-all policy for quarantining compromised IAM users "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.3-foundation-setup/5.3.3-create-iam-roles-and-policies/","title":"Create IAM Roles and Policies","tags":[],"description":"","content":"In this section, you will create 17 IAM roles with their associated policies for Lambda functions, Firehose streams, Step Functions, and other services.\nOverview of IAM Roles Lambda Execution Roles (9 roles):\nCloudTrailETLLambdaServiceRole GuardDutyETLLambdaServiceRole CloudWatchETLLambdaServiceRole CloudWatchENIETLLambdaServiceRole CloudWatchExportLambdaServiceRole ParseFindingsLambdaServiceRole IsolateEC2LambdaServiceRole QuarantineIAMLambdaServiceRole AlertDispatchLambdaServiceRole Service Roles (6 roles): 10. CloudTrailFirehoseRole 11. CloudWatchFirehoseRole 12. StepFunctionsRole 13. IncidentResponseStepFunctionsEventRole 14. FlowLogsIAMRole 15. GlueCloudWatchRole\nIAM Policy (1 policy): 16. IrQuarantineIAMPolicy\nContent Create Lambda Execution Roles Create Service Roles Create IAM Policy "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.5-processing-setup/5.5.3-create-lambda-function-etl-processing/","title":"Create Lambda Function - ETL Processing","tags":[],"description":"","content":"Create Lambda Functions - ETL Processing In this section, you will create 5 Lambda functions that process logs and send them to Kinesis Firehose or S3.\nincident-response-cloudtrail-etl Runtime: Python 3.12 Handler: CloudTrailETL.lambda_handler Role: CloudTrailETLLambdaServiceRole Timeout: 300s, Memory: 128MB Env: FIREHOSE_STREAM_NAME=cloudtrail-firehose-stream Code: cloudtrail-etl incident-response-guardduty-etl Runtime: Python 3.12 Handler: guardduty_etl.lambda_handler Role: GuardDutyETLLambdaServiceRole Timeout: 300s, Memory: 128MB Env: DESTINATION_BUCKET, S3_LOCATION_GUARDDUTY, DATABASE_NAME, TABLE_NAME_GUARDDUTY Code: guardduty-etl cloudwatch-etl-lambda Runtime: Python 3.12 Handler: cloudwatch_etl.lambda_handler Role: CloudWatchETLLambdaServiceRole Env: FIREHOSE_STREAM_NAME=vpc-dns-firehose-stream Code: cloudwatch-etl cloudwatch-eni-etl-lambda Runtime: Python 3.12 Handler: cloudwatch_eni_etl.lambda_handler Role: CloudWatchENIETLLambdaServiceRole Env: FIREHOSE_STREAM_NAME=vpc-flow-firehose-stream Code: cloudwatch-eni-etl cloudwatch-export-lambda Runtime: Python 3.12 Handler: cloudwatch_autoexport.lambda_handler Role: CloudWatchExportLambdaServiceRole Env: DESTINATION_BUCKET=incident-response-log-list-bucket-ACCOUNT_ID-REGION Code: cloudwatch-autoexport Configure CloudWatch Logs Subscription Filter Configure Subscription Filter Open the CloudWatch Console.\nIn the left navigation pane, select Log Management.\nClick on the centralized log group: /aws/incident-response/centralized-logs.\nCreate Subscription Filter:\nClick the \u0026ldquo;Subscription filters\u0026rdquo; tab. Click \u0026ldquo;Create Lambda subscription filter\u0026rdquo;. Configure Destination:\nDestination Lambda function: Select the function cloudwatch-export-lambda. Log format: Select \u0026ldquo;Other\u0026rdquo;. (This ensures the raw log data is passed efficiently for Lambda processing). Configure Log Format and Filter:\nSubscription filter name: Enter a descriptive name, e.g., VPC-Log-Export-Filter. Filter pattern: Leave this field blank. (Ensures all logs in the group are processed). Click \u0026ldquo;Start streaming\u0026rdquo;.\nConfigure S3 Event Notifications S3 Console ‚Üí incident-response-log-list-bucket-ACCOUNT_ID-REGION ‚Üí Properties ‚Üí Event notifications\nCreate 4 notifications with Event types/Object creation/‚úÖAll object create events:\nCloudTrailETLTrigger: Prefix AWSLogs/ACCOUNT_ID/CloudTrail/ ‚Üí Lambda incident-response-cloudtrail-etl VPCDNSLogsTrigger: Prefix exportedlogs/vpc-dns-logs/ ‚Üí Lambda cloudwatch-etl-lambda VPCFlowLogsTrigger: Prefix exportedlogs/vpc-flow-logs/ ‚Üí Lambda cloudwatch-eni-etl-lambda GuardDutyFindingsTrigger: Prefix AWSLogs/ACCOUNT_ID/GuardDuty/ ‚Üí Lambda incident-response-guardduty-etl "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.3-foundation-setup/","title":"Foundation Setup","tags":[],"description":"","content":"This initial Foundation Setup phase establishes the core prerequisites for the Auto Incident Response System, concentrating on the deployment of dedicated storage and essential security authorization. This mandates the creation of five secure Amazon S3 buckets for centralized log ingestion and processing, applying a necessary Bucket Policy for secure log delivery, and defining 17 IAM roles and a quarantine policy to enforce least-privilege access across all integrated AWS services.\nContent Set up Amazon S3 Bucket Configure S3 Bucket Policy for Primary Log Bucket Create IAM Roles and Policies "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Security Best Practices: Transitioning from long-term Access Keys to IAM Roles (IMDSv2) for EC2 security. Hybrid Storage \u0026amp; Migration: Implementing Hybrid Cloud solutions using Storage Gateway and VM Import/Export. Data Protection Strategy: Automating centralized backups with AWS Backup and mastering Disaster Recovery (RTO/RPO). Advanced Storage: Mastering S3 Storage Classes, Lifecycle Management, and the Snow Family. Incident Management: Real-world experience in troubleshooting account suspension and working with AWS Support. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon (22/9) Lab 48: Granting authorization for an application to access AWS services with an IAM role. - Scenario 1: Configure AWS CLI using Access Key/Secret Key (Simulating legacy method). - Scenario 2 (Best Practice): Create IAM Role with S3 Policy and attach to EC2 to leverage Instance Metadata (IMDSv2). - Audit: Review CloudTrail for credential usage. 22/09/2025 22/09/2025 https://000048.awsstudygroup.com/ Tue (23/9) Lab 24: Using File Storage Gateway - Deploy File Gateway Appliance on EC2. - Configure NFS/SMB file shares mapping directly to an S3 Bucket. - Verification: Mount file shares on a client machine and test read/write latency. 23/09/2025 23/09/2025 https://000024.awsstudygroup.com/ Wed (24/9) Lab 25: Amazon FSx for Windows File Server - Lab 25: Troubleshoot and fix CloudFormation templates for stack deployment. - Incident: AWS Account suspended unexpectedly. - Action: Submitted Support Ticket #1 regarding account status and billing verification. 24/09/2025 24/09/2025 https://000025.awsstudygroup.com/ Thu (25/9) Module 4: S3 Core Concepts \u0026amp; Hybrid Storage - Architecture: S3 Buckets, Objects, and 11-nines Durability (Multi-AZ). - Storage Classes: Analyze cost/performance trade-offs: Standard, Intelligent-Tiering, Standard-IA, One Zone-IA, and Glacier/Deep Archive. - Snow Family \u0026amp; Gateway: Study offline data migration tools (Snowball Edge) and Storage Gateway Types (File/Volume/Tape). - Disaster Recovery: Define RTO/RPO and compare DR strategies (Pilot Light vs Warm Standby). 25/09/2025 25/09/2025 Module 04-01 Module 04-02 Module 04-03 Module 04-04 Fri (26/9) Lab 13: Deploy AWS Backup to the System - Automation: Create a centralized Backup Plan for EBS, RDS, and EFS resources. - Notifications: Configure SNS topics to alert administrators on backup status. - Validation: Perform a \u0026ldquo;Test Restore\u0026rdquo; to verify data integrity and RTO capabilities. 26/09/2025 26/09/2025 https://000013.awsstudygroup.com/ Sat (27/9) Lab 14: VM Import/Export - Preparation: Setup VMWare Workstation and AWS CLI environment. - Migration: Export local VM image and upload to S3 Bucket. - Deployment: Import VM image as an AMI and launch a new EC2 instance (Simulating \u0026ldquo;Lift and Shift\u0026rdquo; migration). 27/09/2025 27/09/2025 https://000014.awsstudygroup.com/ Week 3 Achievements: Implemented Least Privilege Security Model:\nSuccessfully migrated an application from using hard-coded Access Keys to using IAM Roles. Verified that using IAM Roles eliminates the risk of credential leakage and simplifies key rotation. Hybrid Cloud \u0026amp; Migration Competency:\nConfigured Storage Gateway to extend on-premise storage to S3 seamlessly. Successfully executed a Lift-and-Shift Migration (Lab 14) by importing a local Virtual Machine into AWS EC2, validating the path for migrating legacy workloads. Automated Data Protection \u0026amp; Resilience:\nDeployed a centralized AWS Backup strategy (Lab 13) covering multiple resource types (EBS, RDS). Validated Disaster Recovery readiness by performing successful restore tests and setting up SNS Notifications for proactive monitoring. Incident Management Experience:\nGained practical experience in dealing with critical Account Suspension issues. Learned the procedure to effectively communicate with AWS Support, providing necessary logs and verification documents to restore service. "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Summary Report: ‚ÄúAWS Cloud Mastery Series #3: AWS Well-Architected ‚Äì Security Pillar Workshop‚Äù Event Objectives Introduction to AWS Cloud Club: Establishing the community\u0026rsquo;s mission and scope. Pillar 1: Identity and Access Management (IAM): Mastering access control and authorization. Pillar 2: Detection and Continuous Monitoring: Implementing observability and automated alerting. Pillar 3: Infrastructure Protection: Securing network boundaries and compute resources. Pillar 4: Data Protection: Ensuring data confidentiality and integrity through encryption and governance. Pillar 5: Incident Response: Establishing protocols for preparing, responding to, and recovering from security incidents. Speakers Le Vu Xuan An - AWS Cloud Club Captain HCMUTE Tran Duc Anh - AWS Cloud Club Captain SGU Tran Doan Cong Ly - AWS Cloud Club Captain PTIT Danh Hoang Hieu Nghi - AWS Cloud Club Captain HUFLIT Huynh Hoang Long - AWS Community Builder Dinh Le Hoang Anh - AWS Community Builder Nguyen Tuan Thinh - Cloud Engineer Trainee Nguyen Do Thanh Dat - Cloud Engineer Trainee Van Hoang Kha - Cloud Security Engineer, AWS Community Builder Thinh Lam - FCJ Member Viet Nguyen - FCJ Member Mendel Grabski (Long) - Ex-Head of Security \u0026amp; DevOps, Cloud Security Solution Architect Tinh Truong - Platform Engineer at TymeX, AWS Community Builder Key Highlights AWS Cloud Club The session commenced with an introduction to the AWS Cloud Club, an initiative designed to:\nFacilitate the exploration and cultivation of cloud computing skills. Foster technical leadership within the student community. Build meaningful, global professional connections. The club provides members with hands-on AWS experiences, mentorship from seasoned AWS professionals, and long-term career support. The AWS Cloud Clubs associated with FCJA include chapters at HCMUTE, SGU, PTIT, and HUFLIT.\nBenefits: Members gain significant opportunities to build practical skills, engage deeply with the community, and access pathways for career advancement.\nIdentity \u0026amp; Access Management (IAM) IAM serves as the fundamental AWS service responsible for controlling secure access. It manages Users, Groups, Roles, and Permissions, ensuring robust authentication and authorization mechanisms across the environment.\nBest Practices:\nLeast Privilege Principle: Granting only the specific permissions necessary to perform a task. Root Access: Deleting root access keys immediately following account creation to prevent compromise. Policy Precision: Avoiding the use of wildcards (\u0026quot;*\u0026quot;) in Actions or Resources to minimize the attack surface. Single Sign-On (SSO): Utilizing SSO for seamless multi-account integration and centralized access management. Service Control Policies (SCPs): These are organization-level policies that define the maximum available permissions for all accounts within an organization. It is crucial to note that SCPs act strictly as filters; they do not grant permissions themselves.\nPermission Boundaries: These allow administrators to set the maximum permissions that an identity-based policy can grant to a specific User or Role, providing a safety net against privilege escalation.\nMulti-Factor Authentication (MFA):\nFeature TOTP (Time-based One-Time Password) FIDO2 (Fast Identity Online 2) Mechanism Shared secret Public-key cryptography User Interaction Requires manually typing a 6-digit code Requires a simple touch or biometric scan Cost Free (Software-based) Variable cost (Hardware-key) Recovery Flexible backups and recovery options Strict backups and typically zero recovery if lost Credential Rotation with AWS Secrets Manager: The Credential Updater utilizes Secrets Manager functions in a cycle: Create Secret -\u0026gt; Set Secret (e.g., every 7 days) -\u0026gt; Test Secret -\u0026gt; Finish Secret. Rotation events can be precisely triggered via an EventBridge Schedule. The process concludes by deprecating the previous secret, ensuring invalidation of old credentials. Detection and Continuous Monitoring Multi-Layer Security Visibility:\nManagement Events: Monitors API calls and console actions across all organizational accounts. Data Events: Tracks S3 object access and Lambda executions at scale. Network Activity Events: Integrates VPC Flow Logs for granular network-level monitoring. Organization Coverage: Ensures unified logging ingestion across all member accounts and regions. Alerting \u0026amp; Automation with EventBridge:\nReal-time Events: CloudTrail events flow directly into EventBridge for immediate processing. This forms the foundation of Event-Driven Architecture (EDA), allowing systems to react instantaneously to state changes. Automated Alerting: Detects and notifies on suspicious activities across the entire organization. Cross-account Event Routing: Facilitates centralized event processing and automated response by routing events based on rules to targets in different accounts or regions. Integration \u0026amp; Workflows: Supports seamless integration with Lambda, SNS, and SQS to trigger automated security workflows. Detection-as-Code:\nCloudTrail Lake Queries: Involves creating and executing SQL-based detection rules for advanced threat hunting. Version-Controlled Logic: Detection rules and logic are tracked, versioned, and managed via code repositories. Automated Deployment: Trails and detection rules are automatically deployed across all relevant organization accounts via CI/CD pipelines. Infrastructure-as-Code (IaC): Utilizes tools like CloudFormation or Terraform to automate the setup of the organization\u0026rsquo;s logging and event trails. GuardDuty GuardDuty is an always-on, intelligent threat detection solution that continuously monitors for malicious activity and unauthorized behavior.\nHow GuardDuty Works: It relies on the continuous analysis of the Three Pillars of Detection: Data Source What It Monitors Real-World Example CloudTrail Events IAM actions, permission changes, API calls An attacker disables logging to conceal their tracks. VPC Flow Logs Network traffic to/from your resources An EC2 instance sending data to a known botnet C2 server. DNS Logs DNS queries from your infrastructure Malware-infected queries attempting to resolve cryptomining sites. Advanced Protection Plans: GuardDuty offers specialized detection add-ons for comprehensive coverage:\nS3 Protection: Detects abnormal S3 access patterns and scans for malware in S3 objects upon upload. EKS Protection: Monitors Kubernetes audit logs for unauthorized access and correlates findings with S3 to map the full attack path. Malware Protection: Automatically scans EBS volumes of EC2 instances when a compromise is suspected. RDS Protection: Analyzes login activity logs for databases (Aurora/RDS) to detect brute-force attacks. Lambda Protection: Monitors network logs from Lambda function invocations to detect if a compromised function is communicating with malicious IPs. Runtime Monitoring: Achieved using a GuardDuty Agent installed on EC2/EKS/ECS Fargate to monitor running processes, file access patterns, and system calls. Compliance Standards:\nAWS Foundational Security Best Practices: A standard developed by AWS, covering a wide range of services. CIS AWS Foundations Benchmark: A consensus-based guide developed by AWS and industry professionals, focusing on Identity (IAM), Logging \u0026amp; Monitoring, and Networking. Compliance Enforcement with Detection-as-Code:\nIaC Tool: AWS CloudFormation is used to deploy configurations. Compliance Engine: AWS CloudFormation pushes configuration checks to AWS Security Hub CSPM. Compliance Standards Applied: Security Hub performs checks against listed standards (AWS Foundational, CIS, PCI DSS, NIST). Resources Covered: Primarily Amazon S3, Amazon EC2, and Amazon RDS. Network Security Controls Attack Vectors: Threats are categorized into Ingress Attacks (e.g., DDoS, SQL injection), Egress Attacks (e.g., data exfiltration, DNS tunneling), and Inside Attacks (e.g., lateral movement).\nSecurity Groups (SG): Act as stateful firewalls at the instance/interface level. They only support \u0026ldquo;allow\u0026rdquo; rules and include an implicit \u0026ldquo;deny all\u0026rdquo; default.\nNetwork ACLs (NACLs): Operate at the subnet level as an additional layer of defense. They are stateless and use numbered rules to explicitly ALLOW or DENY traffic.\nAWS TGW Security Group Referencing: Allows Transit Gateway (TGW) VPCs to define inbound rules using only SG references, simplifying management in complex topologies.\nRoute 53 Resolver: Routes DNS queries to Private DNS (private hosted zones), VPC DNS, or Public DNS based on rules.\nAWS Network Firewall:\nUse Cases: Egress filtering (blocking malicious domains/protocols), environment segmentation (VPC to VPC), and intrusion prevention (IDS/IPS rules). Active Defense: Can automatically block malicious traffic using Amazon Threat Intelligence, where GuardDuty findings are marked for automated blocking actions. Data Protection \u0026amp; Governance Encryption (KMS): Data is encrypted using a Data Key, which is in turn protected by a Customer Master Key (CMK) (Envelope Encryption). KMS policies enforce a second layer of security with Condition keys to define precisely when encryption/decryption is permitted.\nCertificate Management (ACM): Provides free public certificates and automatically renews them 60 days before expiration. DNS Validation is the recommended method for ownership verification.\nSecrets Manager: Addresses the security risk of hardcoded credentials. It uses a 4-step Lambda logic to perform automatic credential rotation without inducing downtime.\nAPI Service Security (S3 \u0026amp; DynamoDB): S3 requires TLS 1.2+ and bucket policies with aws:SecureTransport for enforcement. DynamoDB is secure by default, mandating HTTPS.\nDatabase Service Security (RDS): Requires client-side trust in the AWS Root CA Bundle to verify server identity, and server-side enforcement (e.g., setting rds.force_ssl=1 for PostgreSQL).\nIncident Response \u0026amp; Prevention Prevention Best Practices: Key preventative measures include using temporary credentials, ensuring S3 buckets are never directly exposed to the internet, placing sensitive services within private subnets, managing all resources through Infrastructure as Code, and utilizing double-gate verification for high-risk changes (PR approval, pipeline deployment).\nIncident Response Process: A structured 5-step approach:\nPreparation Detection \u0026amp; Analysis Containment (isolate resources, revoke credentials) Eradication \u0026amp; Recovery Post-Incident Activity (lessons learned and documentation). Event Experience This workshop was highly pertinent to our team, as the content aligned directly with our ongoing project focused on Automated Incident Response and Forensics.\nThe speakers addressed several critical questions from our team:\nQ: Our team\u0026rsquo;s project is an Automated Incident Response and Forensics tool utilizing GuardDuty as the primary trigger. However, our testing indicates that GuardDuty can take up to 5 minutes to generate a finding after an incident occurs. Are there solutions to mitigate this latency? A: The 5-minute delay for GuardDuty to generate findings is largely inherent to its configuration, as it must ingest and process a massive volume of security data to accurately determine threats. To reduce latency, you might consider integrating third-party security services, such as Open Clarity Free, for near real-time findings. Additionally, leveraging CloudTrail directly can help detect specific anomalies and unusual user behavior more rapidly than waiting for the aggregated GuardDuty finding. Furthermore, Mr. Mendel Grabski graciously offered his support and mentorship when we discussed the specifics of our project following the event.\nSome event photos Picture of all Attendees\nGroup Picture With Speaker Mendel Grabski and Speaker Van Hoang Kha\n"},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.7-dashboard-setup/5.7.4-setup-cloudfront/","title":"Cloudfront Setup","tags":[],"description":"","content":"In this guide, you will setup a Cloudfront for cache, routing and web accessing.\nCreate Cloudfront Distribution Open the Cloudfront Console\nNavigate to https://console.aws.amazon.com/cloudfront/ Or: AWS Management Console ‚Üí Services ‚Üí Cloudfront Create Distribution:\nClick the Create distribution button In distribution creation, use this setting: Choose a plan: Free plan Name: Static Dashboard Website CloudFront Origin type: Amazom S3 S3 Origin: Choose the static-dashboard-bucket Keep the rest like default Enable security: Use this if you choose free plan Review and click Create distribution General setting:\nAfter creation complete, on your Cloudfront General tab click on Edit At the Default root object enter index.html Description: Static Dashboard Distribution Click Save change Create API Gateway origin:\nClick Origins on the menu tabs Then click Create origin In orogin creation, use this setting: Origin domain: choose dashboard-api Protocol: HTTPS only HTTPS port: 443 Minimum Origin SSL protocol: TLSv1.2 Origin path: /prod Click Create origin Create behaviors for API Gateway:\nClick Behaviors on the menu tabs Then click Create behavior In behavior creation, use this setting: Path pattern: /logs/* Origin and origin groups: choose dashboard-api Leave the rest setting like default Click Create behavior Update S3 policy to work with Cloudfront:\nClick Origins on the menu tabs, choose the s3-static-dashboard origin name Click Edit At Origin access controll section press Go to S3 bucket permissions Check if your S3 permission look like this, if don\u0026rsquo;t then copy and paste it to your S3 permission (Change the ACCOUNT_ID, ACCOUNT_REGION and CLOUDFRONT_ID to your): { \u0026#34;Version\u0026#34;: \u0026#34;2008-10-17\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;PolicyForCloudFrontPrivateContent\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCloudFrontServicePrincipal\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudfront.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::s3-static-dashboard-[ACCOUNT_ID]-[ACCOUNT_REGION]/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;ArnLike\u0026#34;: { \u0026#34;AWS:SourceArn\u0026#34;: \u0026#34;arn:aws:cloudfront::[ACCOUNT_ID]:distribution/[CLOUDFRONT_ID]\u0026#34; } } } ] } Click Save change Create error pages:\nClick Error pages on the menu tabs Click Create custom error page In custom error page creation, use this setting: HTTP error code: 403: Forbident Error caching minimum TTL: 300 Customize error response: Yes Response page path: /index.html HTTP Response code: 200: OK Repeat this for 404 code "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.11-appendices/5.11.4-cloudwatch-eni-etl/","title":"CloudWatch ENI ETL Code","tags":[],"description":"","content":" import json import boto3 import gzip import os from datetime import datetime s3 = boto3.client(\u0026#34;s3\u0026#34;) firehose = boto3.client(\u0026#34;firehose\u0026#34;) # -------------------------------------------------- # CONFIGURATION # -------------------------------------------------- FIREHOSE_STREAM_NAME = os.environ.get(\u0026#34;FIREHOSE_STREAM_NAME\u0026#34;) # ----------------------------- UTILS ----------------------------- def read_gz(bucket, key): obj = s3.get_object(Bucket=bucket, Key=key) with gzip.GzipFile(fileobj=obj[\u0026#34;Body\u0026#34;]) as f: return f.read().decode(\u0026#34;utf-8\u0026#34;, errors=\u0026#34;replace\u0026#34;) def safe_int(x): try: return int(x) except: return None def parse_flow_log_line(line): parts = line.strip().split(\u0026#39; \u0026#39;) if len(parts) \u0026lt; 14: return None try: start_timestamp = safe_int(parts[10]) time_str = None if start_timestamp: dt_object = datetime.fromtimestamp(start_timestamp) time_str = dt_object.strftime(\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;) record = { \u0026#34;version\u0026#34;: safe_int(parts[0]), # C·ªôt 1: version (int) \u0026#34;account_id\u0026#34;: parts[1], # C·ªôt 2: account_id (STRING) \u0026#34;interface_id\u0026#34;: parts[2], # C·ªôt 3: eni-... \u0026#34;srcaddr\u0026#34;: parts[3], \u0026#34;dstaddr\u0026#34;: parts[4], \u0026#34;srcport\u0026#34;: safe_int(parts[5]), \u0026#34;dstport\u0026#34;: safe_int(parts[6]), \u0026#34;protocol\u0026#34;: safe_int(parts[7]), \u0026#34;packets\u0026#34;: safe_int(parts[8]), \u0026#34;bytes\u0026#34;: safe_int(parts[9]), \u0026#34;start_time\u0026#34;: start_timestamp, # C·ªôt 11 \u0026#34;end_time\u0026#34;: safe_int(parts[11]), \u0026#34;action\u0026#34;: parts[12], \u0026#34;log_status\u0026#34;: parts[13], \u0026#34;timestamp_str\u0026#34;: time_str } return record except Exception as e: print(f\u0026#34;Error parsing line: {e}\u0026#34;) return None def lambda_handler(event, context): print(f\u0026#34;Received S3 Event. Records: {len(event.get(\u0026#39;Records\u0026#39;, []))}\u0026#34;) firehose_records = [] # Duy·ªát qua c√°c file S3 g·ª≠i v·ªÅ for record in event.get(\u0026#34;Records\u0026#34;, []): if \u0026#34;s3\u0026#34; not in record: continue bucket = record[\u0026#34;s3\u0026#34;][\u0026#34;bucket\u0026#34;][\u0026#34;name\u0026#34;] key = record[\u0026#34;s3\u0026#34;][\u0026#34;object\u0026#34;][\u0026#34;key\u0026#34;] # Ch·ªâ x·ª≠ l√Ω file .gz if not key.endswith(\u0026#34;.gz\u0026#34;): print(f\u0026#34;Skipping non-gz: {key}\u0026#34;) continue print(f\u0026#34;Processing: {key}\u0026#34;) # ƒê·ªçc n·ªôi dung content = read_gz(bucket, key) if not content: continue # Parse t·ª´ng d√≤ng log for line in content.splitlines(): rec = parse_flow_log_line(line) if not rec: continue # Chuy·ªÉn th√†nh JSON string v√† th√™m xu·ªëng d√≤ng (\\n) json_row = json.dumps(rec) + \u0026#34;\\n\u0026#34; firehose_records.append({\u0026#39;Data\u0026#39;: json_row}) # ƒê·∫©y sang Firehose (Batching 500 d√≤ng) if firehose_records: total = len(firehose_records) print(f\u0026#34;Flushing {total} records to Firehose...\u0026#34;) batch_size = 500 for i in range(0, total, batch_size): batch = firehose_records[i:i + batch_size] try: response = firehose.put_record_batch( DeliveryStreamName=FIREHOSE_STREAM_NAME, Records=batch ) if response[\u0026#39;FailedPutCount\u0026#39;] \u0026gt; 0: print(f\u0026#34;Warning: {response[\u0026#39;FailedPutCount\u0026#39;]} records failed.\u0026#34;) except Exception as e: print(f\u0026#34;Firehose API Error: {e}\u0026#34;) return {\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;, \u0026#34;count\u0026#34;: len(firehose_records)} "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.4-monitoring-setup/","title":"Monitoring Setup","tags":[],"description":"","content":"This Monitoring Setup phase activates and configures the three core log sources for threat detection. It involves enabling CloudTrail for comprehensive management and data events, activating GuardDuty to export security findings to the primary S3 bucket, and setting up VPC Flow Logs on your network to send all traffic metadata to the dedicated CloudWatch Log Group. This ensures a constant, centralized stream of log data is available for processing and automated response.\nCreate CloudWatch Log Group Open CloudWatch Console ‚Üí Log Management ‚Üí Create log group Configure:\nLog group name: /aws/incident-response/centralized-logs Retention: 90 days KMS key: None Click \u0026ldquo;Create\u0026rdquo;\nEnable AWS CloudTrail Open CloudTrail Console ‚Üí Trail ‚Üí Create trail Trail attributes:\nTrail name: incident-responses-cloudtrail-ACCOUNT_ID-REGION Storage location: Use existing S3 bucket S3 bucket: Choose your incident-response-log-list-bucket-ACCOUNT_ID-REGION Log file SSE-KMS encryption: Disable Log file validation: Enabled Click next Choose log events:\nEvents Choose Management events, Data events Management events: All (Read + Write) Data events: S3 - Log all events Click next till step 4 and Create Trail Advanced event selectors: Exlcude log buckets:\nClick the Trail then scroll down to Data Event and click Edit Setup like picture with the under format: -arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/\n-arn:aws:s3:::processed-guardduty-findings-ACCOUNT_ID-REGION/\n-arn:aws:s3:::processed-cloudtrail-logs-ACCOUNT_ID-REGION\n-arn:aws:s3:::athena-query-results-ACCOUNT_ID-REGION/\n-arn:aws:s3:::processed-cloudwatch-logs-ACCOUNT_ID-REGION/\nSave change Enable Amazon GuardDuty Open GuardDuty Console ‚Üí Get Started ‚Üí Enable GuardDuty\nConfigure settings:\nFinding export frequency: Update CWE and S3 every 15 minutes S3 export: incident-response-log-list-bucket-ACCOUNT_ID-REGION KMS encryption: Choose or create KMS key Enable VPC Flow Logs Open VPC Console ‚Üí Your VPCs ‚Üí Select your VPC\nActions ‚Üí Create flow log\nConfigure:\nFilter: All Aggregation interval: 10 minutes Destination: CloudWatch Logs Log group: /aws/incident-response/centralized-logs IAM role: FlowLogsIAMRole Log format: Default Create flow log\nEnable VPC DNS Query Logging Configure Resolver Query Logging Open the Amazon Route 53 Console.\nIn the left navigation pane, select VPC Resolver -\u0026gt; Query logging.\nClick \u0026ldquo;Configure query logging\u0026rdquo;.\nConfigure:\nName: Enter a descriptive name, e.g., IR-DNS-Query-Log-Config. Destination for query logs: CloudWatch Logs log group Log group: Select \u0026ldquo;Existing log group\u0026rdquo; and choose: /aws/incident-response/centralized-logs Click \u0026ldquo;Configure query logging\u0026rdquo;.\n"},{"uri":"https://quanntm1206.github.io/AWS-FCJ/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Observability \u0026amp; Monitoring: Master the \u0026ldquo;Three Pillars of Observability\u0026rdquo; on AWS: Logs (CloudTrail), Metrics (CloudWatch), and Traces/Traffic (VPC Flow Logs). Incident Response Automation: Understand how to link CloudWatch Alarms with SNS to trigger automated alerts. Security Analytics: Learn to query audit logs using Amazon Athena and detect threats with GuardDuty. Serverless Basics: Introduction to AWS Lambda and Systems Manager. Project Initiation: Begin researching project architecture and translating technical engineering blogs. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon (29/9) Incident Response Essentials: - CloudTrail: Configured trails for auditing user activity and API calls. - CloudWatch: Setup Metrics and Alarms (CPU/Network thresholds). - VPC Flow Logs: Captured IP traffic for network troubleshooting. - SNS: Setup notification topics for alarm triggers. 29/09/2025 29/09/2025 CloudWatch Metrics VPC Flow Logs CloudWatch Alarms SNS Setup CloudTrail Basics Tue (30/9) Advanced Analytics \u0026amp; Serverless: - Athena: Queried CloudTrail logs in S3 using SQL to investigate specific events. - GuardDuty: Explored intelligent threat detection mechanisms. - Compute: Introduction to AWS Lambda (Serverless) and Systems Manager (Ops automation). 30/09/2025 30/09/2025 AWS Lambda Intro Systems Manager GuardDuty Deep Dive CloudTrail with Athena Wed (1/10) Self-study \u0026amp; Review: - Consolidate knowledge on Monitoring tools. - Prepare environment for upcoming Project. 01/10/2025 01/10/2025 Thu (2/10) Project Architecture \u0026amp; Translation: - Project: Research and analyze the proposed architecture for the internship project. - Blog 1: Translated \u0026ldquo;Democratizing quantum resources: University of Michigan and AWS collaborate on a remote access quantum testbed\u0026rdquo;. 02/10/2025 02/10/2025 Original Blog My Translation Fri (3/10) Event: AI-Driven Development Life Cycle: Reimagining Software Engineering - Generative AI is turning developers into editors, shifting the main job from writing code to just reviewing it. - AI-DLC is where AI executes the coding and testing phases, shifting the human role from writing syntax to defining business intent and validating the AI\u0026rsquo;s output. Kiro and Amazon Q Developer showcase. 03/10/2025 03/10/2025 Sat (4/10) Advanced Data Streaming (Blog 2): - Topic: Apache Kafka Disaster Recovery \u0026amp; Migration. - Task: Translated Blog \u0026ldquo;Amazon MSK Replicator and MirrorMaker2\u0026rdquo;. - Key Learnings: Replication strategies, Active-Active vs Active-Passive setups. 04/10/2025 04/10/2025 Original Blog My Translation Week 4 Achievements: Mastered AWS Observability Suite:\nDifferentiated the distinct roles of CloudTrail (Auditing/Who did what?) vs CloudWatch (Performance/How is it running?). Understood the cost/retention limitations of CloudTrail (90 days default) and the necessity of S3 export for long-term compliance. Security \u0026amp; Forensic Analysis:\nLearned to use Amazon Athena to run SQL queries directly on log data in S3, significantly reducing the time needed to investigate security incidents. Configured SNS to decouple monitoring from alerting, allowing multiple subscribers (Email, Lambda, SMS) to react to a single alarm. Technical Translation \u0026amp; Research:\nDeepened technical vocabulary through translating complex topics (Quantum Computing \u0026amp; Kafka Replication). Gained insights into hybrid architectures (MSK Replicator) and academic cloud collaborations. "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I actively participated in five major events. Each occasion proved to be an invaluable experience, offering novel insights, practical knowledge, and networking opportunities, alongside memorable moments and rewards.\nEvent 1 Event Name: AI-Driven Development Life Cycle: Reimagining Software Engineering\nDate: October 03, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS\nDate: November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AWS Cloud Mastery Series #2 - DevOps on AWS\nDate: November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 4 Event Name: AWS Cloud Mastery Series #3: AWS Well-Architected ‚Äì Security Pillar Workshop\nDate: November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 5 Event Name: BUILDING AGENTIC AI - Context Optimization with Amazon Bedrock\nDate: November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee, Hackathon Participant\n"},{"uri":"https://quanntm1206.github.io/AWS-FCJ/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"Summary Report: ‚ÄúBUILDING AGENTIC AI - Context Optimization with Amazon Bedrock‚Äù Event Objectives To conduct a technical deep-dive into the AWS Bedrock Agent Core. To demonstrate the construction of Agentic Workflows within the AWS ecosystem. To introduce Diaflow as an AI automation platform. To introduce CloudThinker and its capabilities in Cloud Operations. To explore Agentic Orchestration and Context Optimization strategies using Amazon Bedrock. To facilitate practical learning through the CloudThinker Hack: Hands-on Workshop. Speakers Nguyen Gia Hung - Head of Solutions Architect, AWS Kien Nguyen - AWS Startup Solutions Architect Viet Pham - CEO \u0026amp; Founder, Diaflow Thang Ton - Co-Founder \u0026amp; COO, CloudThinker Henry Bui - Head of Engineering, CloudThinker Van Hoang Kha - AWS Community Leader Nhat Tran - CTO, CloudThinker Key Highlights Amazon Bedrock AgentCore The Evolution to Agentic AI: The adoption of Agentic AI systems by enterprises is accelerating rapidly. AWS positions itself as the optimal environment for building and scaling these AI Agents.\nAWS Ecosystem Provision:\nFrameworks: Supports integration with Strands, LangGraph, OpenAI, etc. Applications: Offerings include Kiro and AWS QuickSuite. Foundation: Built upon Amazon SageMaker and Amazon Bedrock Models. Core Components of Amazon Bedrock AgentCore:\nRuntime: A secure, serverless agent deployment environment that scales workloads and accelerates time-to-market. Identity: Enterprise-grade identity and access management (IAM) that accelerates development while securing access for AI Agents. Gateway: Provides unified, secure access to tools, featuring intelligent tool discovery capabilities. Memory: Facilitates intelligent, context-aware memory retention, simplifying the management of enterprise-grade data with deep customization options. Browser: A scalable, serverless cloud browser runtime offering enterprise-grade security and observability. Code Interpreter: A secure, sandboxed environment for code execution, enabling large-scale data processing with ease. Observability: Delivers complete visibility into agent performance to maintain quality and trust while integrating seamlessly with other observability tools. Diaflow: AI Automation Platform Diaflow is an AI Automation Platform for Businesses designed to unlock team potential by providing flexible, no-code tools to automate critical workflows rapidly.\nMission: To transform manual processes into intelligent workflows in minutes, addressing the statistic that 90% of businesses waste over 20 hours per person per week on repetitive tasks, while simultaneously solving data privacy concerns. Adoption: Over 6,000 users globally across Retail, IT Services, Finance, Marketing, and Healthcare. Backing: Developed by experts from major tech institutions and backed by partners like NVIDIA, Microsoft, AWS, and Google. Presence: Operating in the USA, Switzerland, France, South Korea, Singapore, and Vietnam. Core Capabilities \u0026amp; Features:\nAI-Powered Automation: Reduces manual tasks by 80% by connecting Databases, Apps, Knowledge Bases, APIs, and Legacy systems. Autonomous Task Execution: Users describe a goal, and Diaflow plans and executes the necessary steps. Multi-Model Processing: Supports various LLMs including Claude, Gemini, and Deepseek. Enterprise-Grade Security: Compliant with HIPAA, SOC2, and GDPR. CloudThinker: Agentic Cloud Operations CloudThinker addresses common customer challenges such as exploding cloud costs, the complexity of cloud management exceeding capability, and reactive incident responses.\nCapabilities: Insights -\u0026gt; Reasoning -\u0026gt; Execution\nMulti-agent Collaboration Autonomous Operations Continuous Optimization \u0026amp; Learning Multi-cloud Capability The CloudThinker AI Agent Team:\nAlex: Cloud Engineer Kai: Kubernetes Engineer Anna: General Manager Oliver: Security Engineer Tony: Database Engineer Agentic Orchestration and Context Optimization AI Agent Architecture Chatbots vs. Agents: Unlike reactive chatbots that use rule-based decisions, Agents are proactive, capable of initiating actions, making dynamic decisions, and adapting through experience to handle multi-step tasks. Core Components: Planning: Deconstructs tasks into executable steps. Memory: Retains context and interaction history. Tools: Interfaces with external APIs, search engines, and code interpreters. Action: Executes the plan and formulates the final response. Getting Started with Agents ReAct Architecture: A paradigm of interleaving thought and action (User Input ‚Üí Reasoning/Thought ‚Üí Action/Tool Use ‚Üí Observation ‚Üí Final Answer). Tool Selection: Effective tools must possess Agent Context, support diverse workflows, enable intuitive problem-solving, and undergo rigorous real-world testing. Coordination Models: Solving Bottlenecks Single-Agent systems often face \u0026ldquo;context isolation\u0026rdquo; (handling 100k+ tokens) and sequential processing bottlenecks. Distributed orchestration solves this via two main models:\nFeature Network (Peer-to-Peer) Supervisor Structure Agents communicate directly with one another. A central Supervisor delegates tasks to specialized Worker agents. Pros * Flexible communication.\n* High fault tolerance.\n* Easier transition from single-agent modes. * Clear task delegation.\n* Centralized coordination.\n* Modular and scalable architecture. Cons * Complex debugging.\n* Ambiguity in ownership. * Potential bottleneck risk.\n* Inflexible switching to single-agent mode. Supervisor Variants:\nGroup Chat-based: Collaborative context, simple coordination. Used by CloudThinker. Supervisor as Tool (Subagents): Fine-grained control, less worker autonomy. Hierarchical: Multi-layer supervision, scalable for large organizations. Solving the 100:1 Agentic Cost Problem Agentic Architectures face a \u0026ldquo;Context Explosion\u0026rdquo; where the Input/Output token ratio can reach 100:1 (compared to 3:1 for chatbots). To combat this, four \u0026ldquo;Quick-win\u0026rdquo; techniques were presented, offering 80‚Äì95% cost savings and 3‚Äì5x latency reduction:\nPrompt Caching: Goal: 70‚Äì90% cost reduction. Method: A Three-Tier Architecture where System Prompt and Conversation History are cached, while the current turn remains dynamic. Context Compaction: Goal: 80% summarization cost reduction. Method: Cache-Preserving Summarization appends instructions at the payload level to summarize old messages without breaking cache keys. Tool Consolidation: Goal: 20% reduction in token count and hallucinations. Method: Consolidating CRUD operations into single interfaces and using Just-In-Time Schemas to fetch documentation only when required. Parallel Tool Calling: Goal: 30‚Äì40% reduction in round trips. Method: Providing explicit instructions to the LLM to parallelize tool calls and reasoning steps. Cross-Region Inference To avoid rate limits caused by high-volume tool calls (50‚Äì100 per task), workloads are automatically routed across global regions (US, EU, APAC). This eliminates single points of failure and provides a global profile for near-zero latency.\nKey Insight: The best multi-agent system self-realizes when NOT to use multiple agents.\nCloudThinker Hack: Hands-On Workshop The organizers provided attendees with access to CloudThinker instructions and a Free Standard Plan Code to analyze their personal AWS Accounts.\nEvent Experience This event was highly informative, significantly strengthening our understanding of Amazon Bedrock and introducing us to powerful tools like Diaflow and CloudThinker.\nSuccess Story: During the CloudThinker Hack workshop, our team utilized the tool to analyze our AWS Account\u0026rsquo;s infrastructure.\nThe Finding: CloudThinker successfully identified an abnormally high volume of S3 Bucket GET requests (over 3 million requests in a 2-week period). The Solution: The agent recommended using Amazon Data Firehose to consolidate logs before saving them to S3, rather than writing individual small files. The Result: Our team was awarded a prize for effectively utilizing CloudThinker to diagnose and resolve this issue. We received a CloudThinker T-Shirt and Keychain. Some event photos Picture of all attendees\nReceiving Prize From CloudThinker\nReceiving Prize From CloudThinker\n"},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.11-appendices/5.11.5-cloudwatch-autoexport/","title":"CloudWatch Autoexport Code","tags":[],"description":"","content":" import json import base64 import gzip from io import BytesIO import boto3 import os import time s3 = boto3.client(\u0026#39;s3\u0026#39;) # --- CONFIGURATION --- RAW_S3_BUCKET = os.environ.get(\u0026#34;DESTINATION_BUCKET\u0026#34;) # The log group pattern constant is no longer used for filtering, but is kept for reference. # VPC_DNS_LOG_PATTERN = \u0026#39;/aws/route53/query/\u0026#39; def is_vpc_dns_log(log_message): try: json_body = json.loads(log_message.strip()) if \u0026#39;query_name\u0026#39; in json_body and \u0026#39;query_type\u0026#39; in json_body: return True return False except Exception: return False def lambda_handler(event, context): try: compressed_payload = base64.b64decode(event[\u0026#39;awslogs\u0026#39;][\u0026#39;data\u0026#39;]) f = BytesIO(compressed_payload) decompressed_data = gzip.GzipFile(fileobj=f).read() log_data = json.loads(decompressed_data.decode(\u0026#39;utf-8\u0026#39;)) log_lines = [] for log_event in log_data.get(\u0026#39;logEvents\u0026#39;, []): log_lines.append(log_event.get(\u0026#39;message\u0026#39;, \u0026#39;\u0026#39;)) if not log_lines: print(f\u0026#34;Batch skipped: No log events found in payload. Log Group: {log_data.get(\u0026#39;logGroup\u0026#39;)}\u0026#34;) return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: \u0026#39;Log batch ignored (No events).\u0026#39;} is_dns_log = is_vpc_dns_log(log_lines[0]) if is_dns_log: key_prefix = \u0026#39;vpc-dns-logs\u0026#39; filename_prefix = \u0026#39;vpc-\u0026#39; # Add vpc- to the filename else: key_prefix = \u0026#39;vpc-flow-logs\u0026#39; filename_prefix = \u0026#39;eni-\u0026#39; # Keep filename blank for other logs output_content = \u0026#39;\\n\u0026#39;.join(log_lines) full_log_group_name = log_data.get(\u0026#39;logGroup\u0026#39;, \u0026#39;unknown-group\u0026#39;) log_group_name_safe = full_log_group_name.strip(\u0026#39;/\u0026#39;).replace(\u0026#39;/\u0026#39;, \u0026#39;_\u0026#39;) final_filename = f\u0026#34;{filename_prefix}{context.aws_request_id}.gz\u0026#34; s3_key = f\u0026#39;exportedlogs/{key_prefix}/{log_group_name_safe}/{final_filename}\u0026#39; buffer = BytesIO() with gzip.GzipFile(fileobj=buffer, mode=\u0026#39;w\u0026#39;) as gz: gz.write(output_content.encode(\u0026#39;utf-8\u0026#39;)) gzipped_data = buffer.getvalue() s3.put_object( Bucket=RAW_S3_BUCKET, Key=s3_key, Body=gzipped_data, ContentType=\u0026#39;application/x-gzip\u0026#39; ) num_logs = len(log_lines) print(f\u0026#34;Exported {num_logs} raw log lines to s3://{RAW_S3_BUCKET}/{s3_key}\u0026#34;) return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: f\u0026#39;Logs exported. {num_logs} events processed. Key Prefix: {key_prefix}\u0026#39;} except Exception as e: print(f\u0026#34;Error in CW Export Lambda: {e}\u0026#34;) raise e "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.7-dashboard-setup/5.7.5-setup-cognito/","title":"Cognito Setup","tags":[],"description":"","content":"In this guide, you will create a Cognito user pool for dashboard login.\nCreate Cognito User Pool Open the Amazon Cognito Console\nNavigate to https://console.aws.amazon.com/cognito/ Or: AWS Management Console ‚Üí Services ‚Üí Cognito Create user pool:\nClick Create user pool In user pool creation, use this setting: Application type: Single-page application (SPA) Application name: dashboard-user-pool-client Options for sign-in identifiers: Email and Username Self-registration: Enable self-registration Required attributes for sign-up: email Add a return URL: Go to Cloudfront, choose the one that you just created and copy the Distribution domain name and paste it here (Example: https://d2bvvvpr6s4eyd.cloudfront.net) Click Create user directory After create, scroll down and click Go to overview User pool App clients configuration:\nSelect App clients on the left menu panel Choose dashboard-user-pool-client In App client information section, click Edit Change the setting like the image below: Click Save change Managed login pages configuration:\nIn Managed login pages configuration section, click Edit Click Add sign-out URL at Allowed sign-out URLs section Copy the URL on the callbacks URL and paste to Allowed sign-out URLs Scroll down to OpenID Connect scopes add Profile to the scopes Click Save change Create a user:\nOn the left menu panel, select User option Click Create user Enter your user information Click Create user "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.5-processing-setup/","title":"Processing Setup","tags":[],"description":"","content":"This Processing Setup phase establishes the core data pipeline for structuring raw logs and preparing them for queryable analysis. It mandates the deployment of three Kinesis Data Firehose streams for buffering and delivering CloudTrail and VPC logs to target S3 buckets. Concurrently, you will configure the AWS Glue Database and four Athena tables via DDL to make the structured data queryable. This pipeline relies on five ETL Lambda functions triggered by S3 Event Notifications to perform the necessary data transformation upon log arrival.\nContent Create Kinesis Data Firehose Delivery Streams Create AWS Glue Database and Tables Create Lambda Functions - ETL Processing "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Cloud Compliance: Understanding financial security standards (OSPAR) and AWS compliance scope. Identity \u0026amp; Access Management (IAM): Troubleshooting Trust Relationships and mastering Permission Boundaries. Advanced Access Control: Implementing Attribute-Based Access Control (ABAC) and Context-Aware Security (IP/Time conditions). Data Security: Encrypting data at rest with AWS KMS and auditing access via CloudTrail \u0026amp; Athena. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon (6/10) Compliance Research (Blog 3): - Topic: OSPAR 2025 Report \u0026amp; AWS Services Scope. - Task: Translated Blog \u0026ldquo;OSPAR 2025 report now available with 170 services\u0026hellip;\u0026rdquo;. - Key Learnings: Understanding security guidelines for financial institutions using cloud services. 06/10/2025 06/10/2025 Original Blog My Translation Tue (7/10) Module 5 Theory: - Study IAM core concepts: Users, Groups, Roles, Policies. - Analyze Shared Responsibility Model \u0026amp; Policy Structure (Effect, Action, Resource, Condition). - Research \u0026ldquo;Principle of Least Privilege\u0026rdquo; best practices. 07/10/2025 07/10/2025 Module 05-01 Module 05-02 Module 05-03 Module 05-04 Module 05-05 Module 05-06 Module 05-07 Module 05-08 Wed (8/10) Lab 18 \u0026amp; Lab 22: Security Hub \u0026amp; IAM Troubleshooting - Lab 18: Enabled AWS Security Hub to automate security checks. - Lab 22: Fixed Role assumption failure by modifying the Trust Policy to allow the correct Principal. 08/10/2025 09/10/2025 https://000018.awsstudygroup.com/ https://000022.awsstudygroup.com/ Fri (10/10) Lab 27 \u0026amp; 28: Resource Tagging \u0026amp; Access Control - Lab 27: Organized EC2 instances using Tags and created Resource Groups for efficient filtering. - Lab 28 (ABAC): Configured IAM Policies to restrict EC2 actions based on Tags. + Test: Verified user access rights across Regions (Tokyo vs N. Virginia) based on Tag matching. 10/10/2025 10/10/2025 https://000027.awsstudygroup.com/ https://000028.awsstudygroup.com/ Sat (11/10) Lab 30, 33 \u0026amp; 44: Advanced Governance \u0026amp; Context Security - Lab 30: Implemented IAM Permission Boundaries to strictly limit maximum user privileges. - Lab 33: Encrypted S3 data using AWS KMS and audited decryption events via Athena. - Lab 44: Configured IAM Roles with Conditions to restrict access based on specific Source IPs and Time windows (e.g., Office hours only). 11/10/2025 12/10/2025 https://000030.awsstudygroup.com/ https://000033.awsstudygroup.com/ https://000044.awsstudygroup.com/ Week 5 Achievements: Implemented Context-Aware Security:\nMoved beyond standard permissions by applying IAM Conditions (Lab 44), successfully restricting Admin Roles to be assumable only from specific trusted IP addresses and within designated timeframes. Strengthened Security Governance:\nApplied IAM Permission Boundaries (Lab 30) to create a \u0026ldquo;ceiling\u0026rdquo; for permissions, preventing delegated administrators from escalating their own privileges. Implemented Attribute-Based Access Control (ABAC) (Lab 28) using Resource Tags for dynamic permission management. Data Encryption \u0026amp; Auditing:\nSecured sensitive data in S3 using AWS KMS (Lab 33) for encryption at rest. Integrated CloudTrail and Athena to perform forensic analysis, tracking exactly who accessed the encrypted data and when. "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/","title":"Workshop","tags":[],"description":"","content":"AWS Auto Incident Response System Setup Overview This guide provides a complete, step-by-step procedure for deploying our automated incident response and forensic system in AWS. This system leverages CloudTrail, GuardDuty, VPC Flow Logs, Kinesis Firehose, Glue, Athena, and Lambda functions orchestrated by AWS Step Functions to automatically detect, analyze, and quarantine compromised resources like EC2 instances and IAM users. Futher log forensics capacity is added by setting up a Security Dashboard hosted on S3 and accessed via CloudFront and Cognito, query log using API Gateway and Lambda.\nContent Overview Prerequisites Phase 1: Foundation Setup Phase 2: Monitoring Setup Phase 3: Processing Setup Phase 4: Automation Setup Phase 5: Dashboard Setup Verify Use CDK Cleanup Appendices "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at Amazon Web Services Vietnam Co., Ltd. from 09/09/2025 to 09/12/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in Cloud Computing \u0026amp; DevOps projects, specifically focusing on Serverless Log Analytics and Hybrid Cloud Architectures, through which I improved my skills in AWS Cloud Services implementation (EC2, S3, Lambda, Athena), System Monitoring (CloudWatch), and Infrastructure Automation.\nIn terms of work ethic, I always aim to complete tasks well and complied with workplace regulations. I consistently maintained a polite, respectful attitude towards everyone and actively engaged with colleagues to improve work efficiency. I prioritized attendance at the office and eagerly participated in all company technical workshops to broaden my industry knowledge.\nTo reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚òê ‚úÖ ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚úÖ ‚òê ‚òê 3 Proactiveness Taking initiative, seeking out tasks, and actively participating in company events ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚úÖ ‚òê ‚òê 6 Progressive mindset Willingness to receive feedback and improve oneself ‚úÖ ‚òê ‚òê 7 Communication Presenting ideas and reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, partners, and demonstrating excellent manners and business etiquette ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚òê ‚úÖ ‚òê 12 Overall General evaluation of the entire internship period ‚úÖ ‚òê ‚òê Needs Improvement Deepen Technical Expertise: Although I have grasped the fundamentals, I need to further strengthen my in-depth understanding of complex architectures and debugging skills to handle advanced technical issues more independently. Enhance Problem-Solving Thinking: I aim to improve my logical approach to troubleshooting, moving from trial-and-error to a more systematic root cause analysis method. Refine Communication Skills: I will focus on articulating technical concepts more clearly and confidently in professional contexts to better convey ideas during team discussions and presentations. "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.6-automation-setup/","title":"Automation Setup","tags":[],"description":"","content":"Phase 4: Automation Setup Create Isolation Security Group EC2 Console ‚Üí Security Groups ‚Üí Create security group Name: IR-Isolation-SG Description: Denies all inbound and outbound traffic for compromised instances VPC: Select your VPC Inbound rules: None (deny all) Outbound rules: Remove default (deny all) Create and note Security Group ID (e.g., sg-0078026b70389e7b3) Create SNS Topic SNS Console ‚Üí Create topic Type: Standard, Name: IncidentResponseAlerts Access policy: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;events.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sns:Publish\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:sns:ap-southeast-1:831981618496:IncidentResponseAlerts\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;AWSEvents_IncidentResponseAlert_Target0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;events.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;SNS:Publish\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:sns:ap-southeast-1:831981618496:IncidentResponseAlerts\u0026#34; } ] } Create Lambda Functions - Incident Response ir-parse-findings-lambda Handler: parse_findings.lambda_handler Role: ParseFindingsLambdaServiceRole Code: parse-findings ir-isolate-ec2-lambda Handler: isolate_ec2.lambda_handler Role: IsolateEC2LambdaServiceRole Env: ISOLATION_SG_ID=sg-XXXXXXX (from step 12) Code: isolate-ec2 ir-quarantine-iam-lambda Handler: quarantine_iam.lambda_handler Role: QuarantineIAMLambdaServiceRole Env: QUARANTINE_POLICY_ARN=arn:aws:iam::ACCOUNT_ID:policy/IrQuarantineIAMPolicy Code: quarantine-iam ir-alert-dispatch Handler: alert_dispatch.lambda_handler Role: AlertDispatchLambdaServiceRole Env: SENDER_EMAIL, RECIPIENT_EMAIL, SLACK_WEBHOOK_URL Add SNS trigger: Topic IncidentResponseAlerts Code: alert-dispatch Update SNS Topic Subscription SNS Console ‚Üí IncidentResponseAlerts ‚Üí Subscriptions Verify: Protocol=AWS Lambda, Endpoint=ir-alert-dispatch, Status=Confirmed Create Step Functions State Machine Step Functions Console ‚Üí Create state machine Type: Standard, Name: IncidentResponseStepFunctions Definition: Step Functions Definition Role: StepFunctionsRole Create Create EventBridge Rule EventBridge Console ‚Üí Rules ‚Üí Create rule Name: IncidentResponseAlert Event pattern: { \u0026#34;source\u0026#34;: [\u0026#34;aws.guardduty\u0026#34;], \u0026#34;detail-type\u0026#34;: [\u0026#34;GuardDuty Finding\u0026#34;] } Targets (2): SNS topic: IncidentResponseAlerts Step Functions: IncidentResponseStepFunctions with role IncidentResponseStepFunctionsEventRole Configure Athena Workgroup Athena Console ‚Üí Workgroups ‚Üí primary ‚Üí Edit Query result location: s3://athena-query-results-ACCOUNT_ID-REGION/ Save "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.11-appendices/5.11.6-parse-findings/","title":"Parse Findings Code","tags":[],"description":"","content":" import json import logging logger = logging.getLogger() logger.setLevel(logging.INFO) def lambda_handler(event, context): instance_ids = [] detail = event.get(\u0026#39;detail\u0026#39;, {}) region = event.get(\u0026#39;region\u0026#39;) or detail.get(\u0026#39;region\u0026#39;) or \u0026#39;ap-southeast-1\u0026#39; instance_id_primary = detail.get(\u0026#39;resource\u0026#39;, {}).get(\u0026#39;instanceDetails\u0026#39;, {}).get(\u0026#39;instanceId\u0026#39;) if instance_id_primary: instance_ids.append(instance_id_primary) # --- 2. Extract from the older/secondary \u0026#39;resources\u0026#39; array structure --- for r in detail.get(\u0026#34;resources\u0026#34;, []): if r.get(\u0026#34;type\u0026#34;) == \u0026#34;AwsEc2Instance\u0026#34;: id_from_details = r.get(\u0026#39;details\u0026#39;, {}).get(\u0026#39;instanceId\u0026#39;) if id_from_details: instance_ids.append(id_from_details) else: arn_id = r.get(\u0026#39;id\u0026#39;) if arn_id and arn_id.startswith(\u0026#39;arn:aws:ec2:\u0026#39;): instance_ids.append(arn_id.split(\u0026#39;/\u0026#39;)[-1]) unique_instance_ids = list(set([id for id in instance_ids if id])) return { \u0026#34;InstanceIds\u0026#34;: unique_instance_ids, \u0026#34;Region\u0026#34;: region } "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Database Fundamentals: Reviewing core concepts like Normalization, Indexing, and ACID vs. BASE models. Managed Relational Databases: Mastering Amazon RDS and Aurora architecture. Static Web Hosting: Deploying static websites on S3 and securing them with Public Access Block configurations. Content Delivery Network (CDN): Analyzing architectural constraints when integrating CloudFront with S3 Website Endpoints. Incident Management: Documenting infrastructure dependencies and resolving configuration conflicts. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon (13/10) Module 6 Theory: AWS Database Services - Core Concepts: RDBMS vs. NoSQL, PK/FK, Indexing, Partitioning. - RDS \u0026amp; Aurora: Automated backups, Multi-AZ for HA, Read Replicas. - Redshift \u0026amp; ElastiCache: Data Warehousing and In-memory caching strategies. 13/10/2025 13/10/2025 Module 06-01 Module 06-02 Module 06-03 Tue (14/10) Lab 43: Planned Execution \u0026amp; Incident Report - Activity: Scheduled execution of Lab 43. - Incident: Documentation portal service unavailable (HTTP 5xx/Timeout). - Action: Flagged as \u0026ldquo;External Dependency Failure\u0026rdquo;. Documented the outage and pivoted to self-study on related AWS concepts pending resolution. 14/10/2025 14/10/2025 https://000043.awsstudygroup.com/ Wed (15/10) Lab 57: S3 Static Website \u0026amp; CloudFront Integration - Part 1 (Success): Completed S3 setup, enabled Static Website Hosting, configured Bucket Policy for public read access (Steps 1-6). - Part 2 (Critical Issue): Attempted to accelerate site with CloudFront (Step 7.2). - Root Cause Analysis: Identified architectural conflict: The tutorial instructed using OAI/OAC with an S3 Website Endpoint, which is unsupported by AWS. - Resolution Strategy: Determined that S3 Website Endpoints must be treated as \u0026ldquo;Custom Origins\u0026rdquo; (without OAC), or switch to standard S3 Origin to use OAC. 15/10/2025 15/10/2025 https://000057.awsstudygroup.com/ Thu (16/10) Documentation Refinement - Task: Refactored weekly worklogs to align with professional reporting standards (Markdown/Hugo). - Goal: Improved readability, structure, and consistency across all previous reports. 16/10/2025 16/10/2025 Fri (17/10) Family matters 17/10/2025 18/10/2025 Week 6 Achievements: Deep Dive into CloudFront Architectures:\nSuccessfully deployed a Static Website on Amazon S3 with public read permissions. Crucial Learning: Discovered that Origin Access Control (OAC) is NOT compatible with S3 Website Endpoints. Analyzed the difference between S3 REST API Endpoints (supports OAC/OAI for security) vs S3 Website Endpoints (requires Custom Origin config). Incident \u0026amp; Risk Management:\nDemonstrated adaptability by handling external documentation failures (Lab 43) without halting progress. Validated lab instructions against official AWS Documentation to identify critical configuration errors before deployment. Professional Documentation:\nStandardized the entire internship worklog, ensuring a clean, maintainable, and professional presentation format using Markdown. "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.7-dashboard-setup/","title":"Dashboard Setup","tags":[],"description":"","content":"This guide will show you how to setup the security dashboard. The security dashboard will be using S3 to contain the web files and folder, Lambda to query data using Athena, API Gateway to routing api to Lambda and Cloudfront to caching and access to the web using it\u0026rsquo;s URL.\nContent Setup S3 Setup Lambda Setup API Gateway Setup Cloudfront Setup Cognito "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.11-appendices/5.11.7-isolate-ec2/","title":"Isolate EC2 Code","tags":[],"description":"","content":" import json import boto3 import os from botocore.exceptions import ClientError ISOLATION_SG_ID = os.getenv(\u0026#39;ISOLATION_SG_ID\u0026#39;) def lambda_handler(event, context): print(\u0026#34;=== ISOLATE EVENT RECEIVED ===\u0026#34;) print(json.dumps(event, indent=2)) instance_id = event.get(\u0026#39;InstanceId\u0026#39;) region = event.get(\u0026#39;Region\u0026#39;, \u0026#39;ap-southeast-1\u0026#39;) if not instance_id or not ISOLATION_SG_ID: print(\u0026#34;[ERROR] Missing InstanceId or IsolationSGId in input. Cannot isolate.\u0026#34;) return {\u0026#34;status\u0026#34;: \u0026#34;isolation_failed\u0026#34;, \u0026#34;InstanceId\u0026#34;: instance_id, \u0026#34;error\u0026#34;: \u0026#34;Missing input data\u0026#34;} try: ec2 = boto3.client(\u0026#39;ec2\u0026#39;, region_name=region) response = ec2.describe_instances(InstanceIds=[instance_id]) instance = response[\u0026#39;Reservations\u0026#39;][0][\u0026#39;Instances\u0026#39;][0] current_sgs = [sg[\u0026#39;GroupId\u0026#39;] for sg in instance.get(\u0026#39;SecurityGroups\u0026#39;, [])] if ISOLATION_SG_ID in current_sgs: print(f\u0026#34;[INFO] {instance_id} already has isolation SG {ISOLATION_SG_ID}\u0026#34;) return { **event, \u0026#34;status\u0026#34;: \u0026#34;already_isolated\u0026#34;, \u0026#34;InstanceId\u0026#34;: instance_id, \u0026#34;Region\u0026#34;: region, \u0026#34;IsolationSG\u0026#34;: None } print(f\u0026#34;[ACTION] Isolating {instance_id} in {region} with SG {ISOLATION_SG_ID}\u0026#34;) ec2.modify_instance_attribute( InstanceId=instance_id, Groups=[ISOLATION_SG_ID] ) print(f\u0026#34;[SUCCESS] {instance_id} isolated with SG {ISOLATION_SG_ID}\u0026#34;) return { **event, \u0026#34;status\u0026#34;: \u0026#34;isolation_complete\u0026#34;, \u0026#34;InstanceId\u0026#34;: instance_id, \u0026#34;Region\u0026#34;: region, \u0026#34;IsolationSG\u0026#34;: ISOLATION_SG_ID } except ClientError as e: error_code = e.response.get(\u0026#39;Error\u0026#39;, {}).get(\u0026#39;Code\u0026#39;) print(f\u0026#34;[ERROR] Isolation FAILED for {instance_id} ({error_code}): {str(e)}\u0026#34;) return { \u0026#34;status\u0026#34;: \u0026#34;isolation_failed\u0026#34;, \u0026#34;InstanceId\u0026#34;: instance_id, \u0026#34;error\u0026#34;: str(e) } except Exception as e: print(f\u0026#34;[ERROR] Isolation FAILED (General) for {instance_id}: {str(e)}\u0026#34;) raise e "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/7-feedback/","title":"Sharing &amp; Feedback","tags":[],"description":"","content":"General Evaluation 1. Work Environment: The working environment at FCJ is characterized by its professionalism and openness. The atmosphere effectively alleviates pressure for newcomers, with team members providing enthusiastic professional support even outside of standard working hours. The workspace is optimally arranged, facilitating sustained high concentration.\n2. Support from Mentors / Team Admins Mentors act as pivotal navigators in my development. Team Admins are swift and efficient regarding administrative procedures and workflows, ensuring that the internship process remains seamless and uninterrupted.\n3. Relevance to Academic Major The practical work serves to validate the foundational knowledge I acquired in university. More importantly, I have gained exposure to new domains and deployment workflows that were not covered in the standard university curriculum.\n4. Learning Opportunities \u0026amp; Skill Development Beyond technical expertise, I have significantly improved my professional communication skills, specifically learning to present issues in an articulate and concise manner when working with Senior members. I have also refined my professional demeanor and adopted appropriate corporate etiquette.\n5. Culture \u0026amp; Team Spirit The company culture prioritizes mutual respect and efficiency. During critical project phases, everyone focuses on providing maximum support to achieve common goals. This result-oriented spirit has allowed me to integrate rapidly into the team.\n6. Policies / Benefits for Interns The flexible working mechanism empowers me to proactively manage my time and workflow. The most valuable aspect has been the opportunity to participate in internal knowledge-sharing and presentation sessions, which provide insightful content that is directly applicable to my projects.\nSuggestions \u0026amp; Aspirations I hope that the notifications regarding examinations be communicated with greater clarity to ensure that we can equip ourselves with the necessary preparation. "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Advanced Networking: Mastering VPC connectivity (Peering vs. Transit Gateway) and Hybrid DNS resolution. Security Best Practices: Implementing IAM Roles for EC2 to enforce \u0026ldquo;Least Privilege\u0026rdquo; access. Hybrid Storage: Deploying storage solutions for Windows workloads (FSx) and extending on-premise storage to cloud (Storage Gateway). Data Protection \u0026amp; Migration: Automating centralized backups and executing \u0026ldquo;Lift-and-Shift\u0026rdquo; VM migrations. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon (20/10) Networking Basics \u0026amp; Security - Lab 19 (VPC Peering): Established direct network connection between two VPCs, configured Route Tables, and enabled Cross-Peer DNS Resolution. - Lab 48 (IAM Roles): Replaced hard-coded Access Keys with IAM Roles attached to EC2 instances to secure application access to S3. 22/09/2025 22/09/2025 https://000019.awsstudygroup.com/ https://000048.awsstudygroup.com/ Tue (21/10) Lab 20: Advanced Networking with Transit Gateway - Architecture: Implemented a Hub-and-Spoke topology connecting 4 VPCs using a single Transit Gateway (TGW). - Routing: Configured TGW Route Tables to centralize traffic management, replacing complex mesh peering connections. 23/09/2025 23/09/2025 https://000020.awsstudygroup.com/ Wed (22/10) Lab 10: Hybrid Connectivity with Route 53 Resolver - Hybrid DNS: Configured Inbound and Outbound Endpoints to resolve domain names between AWS VPCs and On-premise networks. - Integration: Deployed Microsoft Active Directory to simulate on-premise DNS resolution. 24/09/2025 24/09/2025 https://000010.awsstudygroup.com/ Thu (23/10) Lab 25: Amazon FSx for Windows File Server - Deployment: Created a fully managed Native Windows File System integrated with Microsoft Active Directory. - Features: Mapped SMB file shares to Windows instances and enabled Data Deduplication/Shadow Copies for storage efficiency. 25/09/2025 25/09/2025 https://000025.awsstudygroup.com/ Fri (24/10) Hybrid Storage \u0026amp; Backup Automation - Lab 24 (Storage Gateway): Deployed File Gateway to map local NFS/SMB shares directly to S3 buckets for limitless cloud storage extension. - Lab 13 (AWS Backup): Created a centralized Backup Plan to automate protection for EBS, RDS, and EFS resources with SNS notifications. 26/09/2025 26/09/2025 https://000024.awsstudygroup.com/ https://000013.awsstudygroup.com/ Sat (25/10) Lab 14: Migration Strategy (Lift \u0026amp; Shift) - VM Import/Export: Exported a local Virtual Machine (VMware), uploaded image to S3, and converted it into an Amazon Machine Image (AMI). - Validation: Launched a new EC2 instance from the imported AMI, verifying the workload migration capability. 27/09/2025 27/09/2025 https://000014.awsstudygroup.com/ Week 3 Achievements: Designed Scalable Network Architectures:\nTransitioned from simple VPC Peering (Lab 19) to scalable Transit Gateway architectures (Lab 20), significantly reducing routing complexity for multi-VPC environments. Solved the Hybrid DNS challenge using Route 53 Resolver, enabling seamless service discovery between Cloud and On-premise. Mastered Storage \u0026amp; Data Protection:\nImplemented specialized storage for Windows workloads using Amazon FSx and extended on-prem capacity using Storage Gateway. Established a robust Disaster Recovery foundation by automating backups via AWS Backup. Executed Cloud Migration:\nSuccessfully performed a \u0026ldquo;Lift and Shift\u0026rdquo; migration (Lab 14), validating the technical path for moving legacy virtual machines to AWS EC2 without re-architecting. "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.11-appendices/5.11.8-quarantine-iam/","title":"Quarantine IAM Code","tags":[],"description":"","content":" import json import boto3 import os QUARANTINE_POLICY_ARN = os.environ.get(\u0026#34;QUARANTINE_POLICY_ARN\u0026#34;) def lambda_handler(event, context): print(\u0026#34;=== EVENT RECEIVED ===\u0026#34;) print(json.dumps(event, indent=2)) try: finding = event.get(\u0026#39;detail\u0026#39;, {}) user_name = ( finding.get(\u0026#39;resource\u0026#39;, {}) .get(\u0026#39;accessKeyDetails\u0026#39;, {}) .get(\u0026#39;userName\u0026#39;) ) if not user_name: print(\u0026#34;[WARNING] No IAM user found in this finding. Skipping.\u0026#34;) return {\u0026#34;status\u0026#34;: \u0026#34;no_user\u0026#34;} print(f\u0026#34;[ACTION] Quarantining IAM User \u0026#39;{user_name}\u0026#39;...\u0026#34;) iam = boto3.client(\u0026#39;iam\u0026#39;) # Ki·ªÉm tra n·∫øu policy ƒë√£ ƒë∆∞·ª£c g√°n attached_policies = iam.list_attached_user_policies(UserName=user_name)[\u0026#39;AttachedPolicies\u0026#39;] policy_arns = [p[\u0026#39;PolicyArn\u0026#39;] for p in attached_policies] if QUARANTINE_POLICY_ARN in policy_arns: print(f\u0026#34;[INFO] Policy {QUARANTINE_POLICY_ARN} is already attached to user {user_name}.\u0026#34;) else: iam.attach_user_policy( UserName=user_name, PolicyArn=QUARANTINE_POLICY_ARN ) print(f\u0026#34;[SUCCESS] Policy attached. User {user_name} is now quarantined.\u0026#34;) except Exception as e: print(f\u0026#34;[ERROR] Failed to quarantine user: {str(e)}\u0026#34;) raise e return {\u0026#34;status\u0026#34;: \u0026#34;processed\u0026#34;, \u0026#34;action\u0026#34;: \u0026#34;iam_quarantined\u0026#34;} "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.8-verify-setup/","title":"Verify Setup","tags":[],"description":"","content":"After all the setup phase, please refer to the checklist to ensure complete resources creation\nVerify Setup Complete Verification Checklist:\nIncident Response and Forensics:\n‚úÖ S3 Buckets: All 5 created with versioning/encryption ‚úÖ IAM Roles: All 17 roles with correct policies ‚úÖ CloudTrail: Logging enabled ‚úÖ GuardDuty: Enabled with S3 export ‚úÖ VPC Flow Logs: Active ‚úÖ Lambda Functions: All 9 deployed ‚úÖ Firehose Streams: All 3 active ‚úÖ Glue Tables: All 4 created ‚úÖ S3 Events: All 4 triggers configured ‚úÖ SNS Topic: Created with subscription ‚úÖ Step Functions: Active ‚úÖ EventBridge Rule: Enabled with 2 targets Security Dashboard:\n‚úÖ S3 Buckets: Bucket is created with dashboard file stored and enabled hosting ‚úÖ Query Lambda: Lambda is created with the appropriate roles ‚úÖ API Gateway: API Gateway is created with the correct API and resources ‚úÖ CloudFront: Distribution is created with API and S3 origins configured ‚úÖ Cognito: Linked to CloudFront distribution and created user in user pool End-to-End Test\nGenerate sample GuardDuty findings: 1.1 GuardDuty Console ‚Üí Settings ‚Üí Generate sample findings (200+ findings) or 1.2 Trigger single finding via CloudShell (Dectector Id is in GuardDuty Console ‚Üí Settings ) aws guardduty create-sample-findings --detector-id [$dectector-id] --finding-types \u0026#34;Recon:EC2/PortProbeUnprotectedPort\u0026#34; Monitor workflow: Check EventBridge, SNS, Step Functions, Lambda logs Verify alerts: Check email and Slack Query data in Athena: "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Advanced Security: Implementing layered security with encryption (KMS, ACM) and edge protection (WAF, Shield). Serverless Data Lake: Structuring a serverless analytics architecture using Amazon S3 for storage and Amazon Athena for querying. Global Networking: Optimizing content delivery and latency using Route 53, Global Accelerator, and CloudFront. Cost Governance: Mastering cost analysis tools (Cost Explorer, Budgets) and multi-account management strategies. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon (27/10) Advanced Security Services - Review: Comprehensive consolidation of core AWS services (EC2, VPC, IAM, S3, RDS). - Data Encryption: Configured AWS KMS (Key Management Service) for encryption at rest and AWS ACM for SSL/TLS certificates. 20/10/2025 20/10/2025 EC2 User Guide VPC User Guide IAM User Guide S3 User Guide RDS User Guide\nKMS Developer Guide ACM User Guide Tue (28/10) Edge Protection: Deployed AWS WAF and AWS Shield to mitigate DDoS/Web exploits. - Threat Detection: Analyzed Amazon GuardDuty findings and integrated AWS Secrets Manager for credential rotation. Project R\u0026amp;D: Serverless Data Analysis - Storage Layer: Structured Amazon S3 buckets as a Data Lake for log aggregation. - Query Engine: Utilized Amazon Athena to run standard SQL queries directly on S3 data without provisioning servers. - Observability: Configured CloudWatch for monitoring metrics and logs. 21/10/2025 21/10/2025 WAF Developer Guide AMS User Guide (GuardDuty) RDS Guide (Secrets Manager)\nS3 User Guide S3 API Reference Wed (29/10) Networking \u0026amp; Compute Scaling - Global Traffic: Configured Amazon Route 53 (DNS Failover) and AWS Global Accelerator for low-latency user access. - CDN: Optimized content delivery with Amazon CloudFront. - Serverless Compute: Deployed functions on AWS Lambda and containers on AWS Fargate. - Storage Performance: Compared EBS Provisioned IOPS vs Amazon EFS throughput modes. 22/10/2025 22/10/2025 Route 53 Guide Global Accelerator Guide [CloudFront Guide]( "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.11-appendices/5.11.9-alert-dispatch/","title":"Alert Dispatch Code","tags":[],"description":"","content":" import os import json import logging import urllib.request import boto3 from botocore.exceptions import ClientError import html # --- Telegram ENV --- # BOT_TOKEN = os.environ.get(\u0026#39;BOT_TOKEN\u0026#39;) # CHAT_ID = os.environ.get(\u0026#39;CHAT_ID\u0026#39;) # MESSAGE_THREAD_ID = os.environ.get(\u0026#39;MESSAGE_THREAD_ID\u0026#39;) # --- Slack ENV --- SLACK_WEBHOOK_URL = os.environ.get(\u0026#34;SLACK_WEBHOOK_URL\u0026#34;) # --- SES ENV --- SENDER_EMAIL = os.environ.get(\u0026#39;SENDER_EMAIL\u0026#39;) RECIPIENT_EMAIL = os.environ.get(\u0026#39;RECIPIENT_EMAIL\u0026#39;) # Can now be \u0026#34;a@b.com, c@d.com\u0026#34; AWS_REGION = os.environ.get(\u0026#39;AWS_REGION\u0026#39;, \u0026#39;ap-southeast-1\u0026#39;) # --- Setup --- # TELEGRAM_URL = f\u0026#34;https://api.telegram.org/bot{BOT_TOKEN}/sendMessage\u0026#34; if BOT_TOKEN else None logger = logging.getLogger() logger.setLevel(logging.INFO) # Initialize SES Client ses_client = boto3.client(\u0026#39;ses\u0026#39;, region_name=AWS_REGION) # ==================================================================== # SEND TO TELEGRAM # ==================================================================== # def send_to_telegram(finding, chat_id, thread_id): # logger.info(\u0026#34;Formatting message for Telegram...\u0026#34;) # severity_num = finding.get(\u0026#39;severity\u0026#39;, 0) # if severity_num \u0026gt;= 7.0: # severity = \u0026#34;üî¥ HIGH\u0026#34; # elif severity_num \u0026gt;= 4.0: # severity = \u0026#34;üü† MEDIUM\u0026#34; # else: # severity = \u0026#34;üîµ LOW\u0026#34; # title = finding.get(\u0026#39;title\u0026#39;, \u0026#39;N/A\u0026#39;) # description = finding.get(\u0026#39;description\u0026#39;, \u0026#39;N/A\u0026#39;) # account_id = finding.get(\u0026#39;accountId\u0026#39;, \u0026#39;N/A\u0026#39;) # region = finding.get(\u0026#39;region\u0026#39;, \u0026#39;N/A\u0026#39;) # finding_type = finding.get(\u0026#39;type\u0026#39;, \u0026#39;N/A\u0026#39;) # message_text = ( # f\u0026#34;üö® *GuardDuty Finding* üö®\\n\\n\u0026#34; # f\u0026#34;*Severity:* {severity}\\n\u0026#34; # f\u0026#34;*Account:* {account_id}\\n\u0026#34; # f\u0026#34;*Region:* {region}\\n\u0026#34; # f\u0026#34;*Title:* {title}\\n\u0026#34; # f\u0026#34;*Description:* {description}\\n\\n\u0026#34; # f\u0026#34;*Finding Type:* `{finding_type}`\u0026#34; # ) # payload = {\u0026#39;chat_id\u0026#39;: chat_id, \u0026#39;text\u0026#39;: message_text, \u0026#39;parse_mode\u0026#39;: \u0026#39;Markdown\u0026#39;} # if thread_id: # payload[\u0026#39;message_thread_id\u0026#39;] = thread_id # try: # req = urllib.request.Request( # TELEGRAM_URL, # data=json.dumps(payload).encode(\u0026#39;utf-8\u0026#39;), # headers={\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;} # ) # with urllib.request.urlopen(req) as response: # logger.info(\u0026#34;Telegram response: \u0026#34; + response.read().decode(\u0026#39;utf-8\u0026#39;)) # except Exception as e: # logger.error(f\u0026#34;TELEGRAM FAILED: {e}\u0026#34;) # ==================================================================== # SEND TO SLACK # ==================================================================== def send_to_slack(finding): if not SLACK_WEBHOOK_URL: logger.warning(\u0026#34;Slack ENV missing. Skipping.\u0026#34;) return severity_num = finding.get(\u0026#34;severity\u0026#34;, 0) title = finding.get(\u0026#34;title\u0026#34;, \u0026#34;No Title\u0026#34;) description = finding.get(\u0026#34;description\u0026#34;, \u0026#34;No Description\u0026#34;) region = finding.get(\u0026#34;region\u0026#34;, \u0026#34;N/A\u0026#34;) account_id = finding.get(\u0026#34;accountId\u0026#34;, \u0026#34;N/A\u0026#34;) finding_type = finding.get(\u0026#34;type\u0026#34;, \u0026#34;N/A\u0026#34;) if severity_num \u0026gt;= 7: color = \u0026#34;#ff0000\u0026#34; sev = \u0026#34;üî¥ HIGH\u0026#34; elif severity_num \u0026gt;= 4: color = \u0026#34;#ffa500\u0026#34; sev = \u0026#34;üü† MEDIUM\u0026#34; else: color = \u0026#34;#007bff\u0026#34; sev = \u0026#34;üîµ LOW\u0026#34; payload = { \u0026#34;text\u0026#34;: f\u0026#34;üö® {sev} ‚Äì {title}\u0026#34;, \u0026#34;attachments\u0026#34;: [{ \u0026#34;color\u0026#34;: color, \u0026#34;blocks\u0026#34;: [ {\u0026#34;type\u0026#34;: \u0026#34;header\u0026#34;, \u0026#34;text\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;plain_text\u0026#34;, \u0026#34;text\u0026#34;: f\u0026#34;üö® GuardDuty Finding: {title}\u0026#34;}}, {\u0026#34;type\u0026#34;: \u0026#34;section\u0026#34;, \u0026#34;fields\u0026#34;: [ {\u0026#34;type\u0026#34;: \u0026#34;mrkdwn\u0026#34;, \u0026#34;text\u0026#34;: f\u0026#34;*Severity:*\\n{sev}\u0026#34;}, {\u0026#34;type\u0026#34;: \u0026#34;mrkdwn\u0026#34;, \u0026#34;text\u0026#34;: f\u0026#34;*Region:*\\n{region}\u0026#34;} ]}, {\u0026#34;type\u0026#34;: \u0026#34;section\u0026#34;, \u0026#34;text\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;mrkdwn\u0026#34;, \u0026#34;text\u0026#34;: f\u0026#34;*Description:*\\n{description}\u0026#34;}}, {\u0026#34;type\u0026#34;: \u0026#34;divider\u0026#34;}, {\u0026#34;type\u0026#34;: \u0026#34;context\u0026#34;, \u0026#34;elements\u0026#34;: [ {\u0026#34;type\u0026#34;: \u0026#34;mrkdwn\u0026#34;, \u0026#34;text\u0026#34;: f\u0026#34;*Account:* `{account_id}`\u0026#34;}, {\u0026#34;type\u0026#34;: \u0026#34;mrkdwn\u0026#34;, \u0026#34;text\u0026#34;: f\u0026#34;*Type:* `{finding_type}`\u0026#34;} ]} ] }] } try: req = urllib.request.Request( SLACK_WEBHOOK_URL, data=json.dumps(payload).encode(\u0026#34;utf-8\u0026#34;), headers={\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;} ) with urllib.request.urlopen(req) as response: logger.info(\u0026#34;Slack response: \u0026#34; + response.read().decode(\u0026#34;utf-8\u0026#34;)) except Exception as e: logger.error(f\u0026#34;SLACK FAILED: {e}\u0026#34;) # ==================================================================== # SEND TO SES EMAIL (UPDATED FOR MULTIPLE RECIPIENTS) # ==================================================================== def send_to_ses(finding): if not SENDER_EMAIL or not RECIPIENT_EMAIL: logger.warning(\u0026#34;SES Env vars missing. Skipping Email.\u0026#34;) return logger.info(\u0026#34;Formatting message for SES Email...\u0026#34;) recipient_list = [email.strip() for email in RECIPIENT_EMAIL.split(\u0026#39;,\u0026#39;)] severity_num = finding.get(\u0026#34;severity\u0026#34;, 0) title = finding.get(\u0026#34;title\u0026#34;, \u0026#34;No Title\u0026#34;) description = finding.get(\u0026#34;description\u0026#34;, \u0026#34;No Description\u0026#34;) region = finding.get(\u0026#34;region\u0026#34;, \u0026#34;N/A\u0026#34;) account_id = finding.get(\u0026#34;accountId\u0026#34;, \u0026#34;N/A\u0026#34;) finding_type = finding.get(\u0026#34;type\u0026#34;, \u0026#34;N/A\u0026#34;) finding_id = finding.get(\u0026#34;id\u0026#34;, \u0026#34;N/A\u0026#34;) if severity_num \u0026gt;= 7: color = \u0026#34;#ff0000\u0026#34; sev = \u0026#34;HIGH\u0026#34; elif severity_num \u0026gt;= 4: color = \u0026#34;#ffa500\u0026#34; sev = \u0026#34;MEDIUM\u0026#34; else: color = \u0026#34;#007bff\u0026#34; sev = \u0026#34;LOW\u0026#34; html_body = f\u0026#34;\u0026#34;\u0026#34; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;style\u0026gt; body {{ font-family: Arial, sans-serif; line-height: 1.6; color: #333; }} .container {{ width: 100%; max-width: 600px; margin: 0 auto; border: 1px solid #ddd; border-radius: 8px; overflow: hidden; }} .header {{ background-color: {color}; color: white; padding: 15px; text-align: center; }} .content {{ padding: 20px; }} .footer {{ background-color: #f4f4f4; padding: 10px; text-align: center; font-size: 12px; color: #666; }} .label {{ font-weight: bold; color: #555; }} \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;header\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;üö® GuardDuty Alert: {sev}\u0026lt;/h2\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;content\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;{title}\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;{description}\u0026lt;/p\u0026gt; \u0026lt;hr\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span class=\u0026#34;label\u0026#34;\u0026gt;Account ID:\u0026lt;/span\u0026gt; {account_id}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span class=\u0026#34;label\u0026#34;\u0026gt;Region:\u0026lt;/span\u0026gt; {region}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span class=\u0026#34;label\u0026#34;\u0026gt;Type:\u0026lt;/span\u0026gt; {finding_type}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span class=\u0026#34;label\u0026#34;\u0026gt;Finding ID:\u0026lt;/span\u0026gt; {finding_id}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;footer\u0026#34;\u0026gt; Generated by AWS Lambda Alert Dispatch \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \u0026#34;\u0026#34;\u0026#34; try: response = ses_client.send_email( Source=SENDER_EMAIL, Destination={\u0026#39;ToAddresses\u0026#39;: recipient_list}, # Uses the list now Message={ \u0026#39;Subject\u0026#39;: {\u0026#39;Data\u0026#39;: f\u0026#34;GuardDuty Alert [{sev}]: {title}\u0026#34;, \u0026#39;Charset\u0026#39;: \u0026#39;UTF-8\u0026#39;}, \u0026#39;Body\u0026#39;: {\u0026#39;Html\u0026#39;: {\u0026#39;Data\u0026#39;: html_body, \u0026#39;Charset\u0026#39;: \u0026#39;UTF-8\u0026#39;}} } ) logger.info(f\u0026#34;SES Email sent to {len(recipient_list)} recipients! MessageId: {response[\u0026#39;MessageId\u0026#39;]}\u0026#34;) except ClientError as e: logger.error(f\u0026#34;SES FAILED: {e.response[\u0026#39;Error\u0026#39;][\u0026#39;Message\u0026#39;]}\u0026#34;) # ==================================================================== # MAIN HANDLER # ==================================================================== def lambda_handler(event, context): logger.info(f\u0026#34;Event received: {json.dumps(event)}\u0026#34;) try: sns_message_raw = event[\u0026#34;Records\u0026#34;][0][\u0026#34;Sns\u0026#34;][\u0026#34;Message\u0026#34;] message_data = json.loads(sns_message_raw) # Normalization Logic finding = {} if \u0026#34;detail-type\u0026#34; in message_data and message_data[\u0026#34;detail-type\u0026#34;] == \u0026#34;GuardDuty Finding\u0026#34;: detail = message_data[\u0026#34;detail\u0026#34;] finding = { \u0026#34;severity\u0026#34;: detail.get(\u0026#34;severity\u0026#34;, 0), \u0026#34;title\u0026#34;: detail.get(\u0026#34;title\u0026#34;, \u0026#34;GuardDuty Finding\u0026#34;), \u0026#34;description\u0026#34;: detail.get(\u0026#34;description\u0026#34;, \u0026#34;No description provided\u0026#34;), \u0026#34;accountId\u0026#34;: detail.get(\u0026#34;accountId\u0026#34;, \u0026#34;N/A\u0026#34;), \u0026#34;region\u0026#34;: detail.get(\u0026#34;region\u0026#34;, \u0026#34;N/A\u0026#34;), \u0026#34;type\u0026#34;: detail.get(\u0026#34;type\u0026#34;, \u0026#34;N/A\u0026#34;), \u0026#34;id\u0026#34;: detail.get(\u0026#34;id\u0026#34;, \u0026#34;N/A\u0026#34;) } elif \u0026#34;AlarmName\u0026#34; in message_data: state = message_data.get(\u0026#34;NewStateValue\u0026#34;) severity = 8 if state == \u0026#34;ALARM\u0026#34; else 0 finding = { \u0026#34;severity\u0026#34;: severity, \u0026#34;title\u0026#34;: f\u0026#34;CloudWatch Alarm: {message_data.get(\u0026#39;AlarmName\u0026#39;)}\u0026#34;, \u0026#34;description\u0026#34;: message_data.get(\u0026#34;NewStateReason\u0026#34;, \u0026#34;State change detected\u0026#34;), \u0026#34;accountId\u0026#34;: message_data.get(\u0026#34;AWSAccountId\u0026#34;, \u0026#34;N/A\u0026#34;), \u0026#34;region\u0026#34;: message_data.get(\u0026#34;Region\u0026#34;, \u0026#34;N/A\u0026#34;), \u0026#34;type\u0026#34;: \u0026#34;CloudWatch Alarm\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;N/A\u0026#34; } else: finding = { \u0026#34;severity\u0026#34;: 0, \u0026#34;title\u0026#34;: \u0026#34;Unknown Alert\u0026#34;, \u0026#34;description\u0026#34;: f\u0026#34;Raw Payload: {json.dumps(message_data)}\u0026#34;, \u0026#34;accountId\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Unknown\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;N/A\u0026#34; } except Exception as e: logger.error(f\u0026#34;FATAL: Could not parse incoming SNS event: {e}\u0026#34;) return {\u0026#34;statusCode\u0026#34;: 500} # --- Send Telegram --- # if BOT_TOKEN and CHAT_ID: # send_to_telegram(finding, CHAT_ID, MESSAGE_THREAD_ID) # --- Send Slack --- if SLACK_WEBHOOK_URL: send_to_slack(finding) # --- Send SES Email --- if SENDER_EMAIL and RECIPIENT_EMAIL: send_to_ses(finding) return {\u0026#34;statusCode\u0026#34;: 200, \u0026#34;body\u0026#34;: \u0026#34;Dispatch complete\u0026#34;} "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.9-use-cdk/","title":"Use CDK","tags":[],"description":"","content":"Overview We have provided CDK stack to create all of the infrastructure required for this workshop.\nTo get the files please go to this Github Link and clone or download all the files to a folder\nSetup Guide Before deploying the CDK stack, you must configure your local environment to authenticate with your AWS account using the AWS Command Line Interface (CLI).\nInstall the AWS CLI.\nObtain Credentials: You need an Access Key ID and a Secret Access Key from an IAM user with deployment permissions.\nRun the Configuration Command: Open your terminal and execute aws configure.\n$ aws configure When prompted, enter your credentials and desired settings. The Default region name should match the region where you plan to deploy the stack (e.g., ap-southeast-1):\nPrompt Example Value AWS Access Key ID AKIA... AWS Secret Access Key wJalr... Default region name ap-southeast-1 Default output format json Verify Configuration: Test your setup by fetching your user identity. A successful output confirms you are authenticated.\n$ aws sts get-caller-identity Prerequisites Ensure the following tools and services are installed and configured on your system:\nPython 3.8+ and pip: Required for executing the CDK application and building Lambda function assets. Node.js and npm: Required for running the AWS CDK CLI and building the React dashboard. AWS CDK Toolkit: Install the CDK CLI globally: $ npm install -g aws-cdk Set Up Python Environment The infrastructure definition is written in Python. A dedicated virtual environment is used to manage project dependencies.\nCreate the Virtual Environment:\n$ python -m venv .venv Activate the Virtual Environment:\nOperating System Command macOS / Linux source .venv/bin/activate Windows (Command Prompt) .venv\\Scripts\\activate.bat Windows (PowerShell) .venv\\Scripts\\Activate.ps1 Install Python Dependencies:\n$ pip install -r requirements.txt Step to build the dashboard In the project folder location, check inside the react folder. If the dist folder already exists, you do not need to build. Otherwise, please follow the steps below. If you are on cmd use this command to move to react folder:\n$ cd react And use this command to list all content in react:\n$ ls Prerequisites Ensure you have Node.js and npm installed. You can check the current version by running:\n$ npm --version If the command is not recognized, please download and install Node.js from nodejs.org\nInstall dependencies Run the following command to install all necessary libraries:\n$ npm install Build the Project After the installation is complete, run the build command:\n$ npm run build Upon completion, a dist folder will be generated containing index.html and the assets folder.\nConfigure Deployment Context The stack utilizes context variables. These variables are read from cdk.context.json or provided via command-line flags.\nVariable Name Description Required if functionality is desired Default Value (in cdk.context.json) vpc_ids A list of VPC IDs for Flow Logs and DNS Query Logging. Yes [] alert_email A list of email addresses for alert notifications (requires SES). Yes [] sender_email The verified SES sender email address. Yes (if alert_email is set) \u0026quot;\u0026quot; slack_webhook_url The Slack webhook URL for sending alerts. No \u0026quot;\u0026quot; Example\n{ \u0026#34;vpc_ids\u0026#34;: [ \u0026#34;vpc-a1b2c3d4e5f6g7h8i\u0026#34; ], \u0026#34;alert_email\u0026#34;: [ \u0026#34;admin@example.com\u0026#34; ], \u0026#34;sender_email\u0026#34;: \u0026#34;alerts@your-domain.com\u0026#34;, \u0026#34;slack_webhook_url\u0026#34;: \u0026#34;\u0026#34; } Deploy the Stacks Before processing further, if inside the /react folder, enter this command to go back to the main folder:\n$ cd.. CDK Bootstrapping: If you have not used the AWS CDK in your target AWS account and region previously, run the bootstrap command once to provision necessary resources (e.g., S3 deployment bucket).\n$ cdk bootstrap (Optional) Synthesize and Diff: Review the proposed CloudFormation changes before deployment:\n$ cdk synth --all $ cdk diff --all Execute Deployment: Run the deployment command and approve any requested IAM security changes when prompted.\n$ cdk deploy --all The deployment is complete when the CDK CLI reports success for the stack: AwsIncidentResponseAutomationCdkStack and DashboardCdkStack\nIMPORTANT NOTE: After the deployment is complete, you should verify the email in SES. Create a user in Cognito to be able to log in to the Dashboard. Access the Security Group and remove the default outbound rule from the QuarantineSecurityGroup "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Project Architecture: Designing a Serverless Log Analytics pipeline using AWS managed services. Data Ingestion: Automating log extraction from Amazon CloudWatch to Amazon S3 using AWS Lambda. ETL Automation: Implementing Extract-Transform-Load (ETL) logic to normalize raw JSON logs. Data Analytics: Enabling interactive SQL queries on S3 data using Amazon Athena. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon (3/11) Project R\u0026amp;D: Log Analytics Architecture\n- Requirement Analysis: Defined the flow for collecting and analyzing application logs.\n- Architecture Design: Designed a pipeline: CloudWatch Logs (Source) to AWS Lambda (Collector) to Amazon S3 (Data Lake) to Amazon Athena (Query Engine).\n- Format Strategy: Decided on JSON format for raw logs to ensure schema flexibility. 03/11/2025 03/11/2025 S3 User Guide\nModule 04-02 - S3 Concepts\nCloudWatch Metrics Tue (4/11) Data Ingestion Implementation (Part 1)\n- Lambda Dev: Developed a Python/Node.js Lambda function to interface with CloudWatch Logs API.\n- Logic: Implemented logic to \u0026ldquo;crawl\u0026rdquo; (fetch) log streams and aggregate them.\n- Storage: Configured S3 Bucket policy to allow Lambda write access. 04/11/2025 04/11/2025 S3 API Reference\nCloudWatch for VPC Wed (5/11) Data Ingestion Implementation (Part 2)\n- Formatting: Structured the fetched logs into valid JSON objects.\n- Persistence: Successfully stored raw log files into the S3 bucket (Raw Zone).\n- Testing: Verified log integrity by manually inspecting S3 objects. 05/11/2025 05/11/2025 S3 User Guide Thu (6/11) ETL Processing Development (Part 1)\n- Lambda ETL: Started developing a second Lambda function triggered by S3 Event Notifications (ObjectCreate).\n- Transformation: Implemented logic to parse raw JSON files, clean data, and flatten nested structures. 06/11/2025 06/11/2025 Athena User Guide Fri (7/11) ETL Processing \u0026amp; Athena Integration (Part 2)\n- Loading: Configured the Lambda to output processed data to a target S3 bucket (Clean Zone).\n- Athena Integration: Defined the Table Schema (DDL) in Athena to map to the processed JSON data.\n- Validation: Executed SQL queries in Athena to verify data accuracy and structure. 07/11/2025 07/11/2025 VPC Flow Logs \u0026amp; Athena\nAthena Solutions Week 9 Achievements: Built a Serverless ETL Pipeline:\nSuccessfully automated the flow of data from operational logs (CloudWatch) to an analytical data lake (S3) without provisioning any servers (using AWS Lambda). Implemented ETL (Extract, Transform, Load) processes to convert unstructured raw logs into structured, queryable JSON datasets. Mastered Data Analytics on AWS:\nConfigured Amazon Athena to treat files in S3 as a relational database, enabling SQL-based analysis on log data. Applied S3 Best Practices by organizing data into \u0026ldquo;Raw\u0026rdquo; and \u0026ldquo;Processed\u0026rdquo; zones to maintain data lake hygiene. CloudWatch Integration:\nDeepened understanding of programmatic access to CloudWatch Logs beyond the Console UI. "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.10-cleanup/","title":"Cleanup","tags":[],"description":"","content":"Congratulations on completing this workshop! In this workshop, you have created an Automated Incident Response and Forensics System and familiarized with Lambda, Step Functions, EventBridge, Glue, Athena, CloudFront, Cognito, S3 Buckets\nCleanup Guide: Cleanup Guide for Manual Infrastructure Setup Clean Guide for CDK Setup "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.10-cleanup/5.10.1-manual-cleanup/","title":"Manual Cleanup","tags":[],"description":"","content":"Clean up (Manual Infrastructure Setup) Phase 1: Automation and Monitoring Cleanup The goal here is to stop all active processes and delete the monitoring and core automation resources (EventBridge, Step Functions, SNS, GuardDuty, Flow Logs, CloudTrail).\n1. Delete Incident Response Automation 1.1 Delete EventBridge Rule\nGo to EventBridge Console ‚Üí Rules. Select the rule: IncidentResponseAlert. Click \u0026ldquo;Delete\u0026rdquo;. 1.2 Delete Step Functions State Machine\nGo to Step Functions Console ‚Üí State Machines. Select the State Machine: IncidentResponseStepFunctions. Click \u0026ldquo;Delete\u0026rdquo;. 1.3 Delete SNS Topic and Subscription\nGo to SNS Console ‚Üí Topics ‚Üí IncidentResponseAlerts. First, delete the subscription associated with ir-alert-dispatch. Then, delete the topic itself by clicking \u0026ldquo;Delete topic\u0026rdquo;. 1.4 Delete GuardDuty Detector\nGo to GuardDuty Console ‚Üí Settings ‚Üí General. Click \u0026ldquo;Suspend\u0026rdquo; to stop processing, then click \u0026ldquo;Disable GuardDuty\u0026rdquo; (or \u0026ldquo;Delete detector\u0026rdquo;). 1.5 Disable VPC Flow Logs\nGo to VPC Console ‚Üí VPC Flow Logs. Select the flow log created (associated with YOUR_VPC_ID). Click \u0026ldquo;Delete flow log\u0026rdquo;. 1.6 Delete CloudTrail Trail\nGo to CloudTrail Console ‚Üí Trails. Select the trail: incident-responses-cloudtrail-ACCOUNT_ID-REGION. Click \u0026ldquo;Delete\u0026rdquo;. Phase 2: Lambda and Compute Cleanup 2. Delete All Lambda Functions (9 Functions) Go to the Lambda Console and delete the following functions:\nincident-response-cloudtrail-etl incident-response-guardduty-etl cloudwatch-etl-lambda cloudwatch-eni-etl-lambda cloudwatch-export-lambda ir-parse-findings-lambda ir-isolate-ec2-lambda ir-quarantine-iam-lambda ir-alert-dispatch 3. Delete Isolation Security Group Go to EC2 Console ‚Üí Security Groups. Find and select the Security Group: IR-Isolation-SG (using ID sg-XXXXXXX). Click \u0026ldquo;Delete security group\u0026rdquo;. 4. Delete CloudWatch Log Groups Go to the CloudWatch Console ‚Üí Log Groups and delete:\nThe centralized log group: /aws/incident-response/centralized-logs. Any associated Lambda log groups for the 9 deleted functions (e.g., /aws/lambda/ir-parse-findings-lambda). Phase 3: Processing and Data Lake Cleanup 5. Delete Kinesis Data Firehose Streams Go to the Kinesis Console ‚Üí Delivery Streams and delete:\ncloudtrail-firehose-stream vpc-dns-firehose-stream vpc-flow-firehose-stream 6. Delete AWS Glue Tables and Database 6.1 Delete Glue Tables\nGo to Glue Console ‚Üí Tables. Select and delete: security_logs.processed_cloudtrail, security_logs.processed_guardduty, security_logs.vpc_logs, and security_logs.eni_flow_logs. 6.2 Delete Glue Database\nGo to Glue Console ‚Üí Databases. Select the database: security_logs and click \u0026ldquo;Delete\u0026rdquo;. 7. Delete IAM Roles and Policies 7.1 Delete IAM Policies\nGo to IAM Console ‚Üí Policies. Delete the custom managed policy: IrQuarantineIAMPolicy. Note: Inline policies created in the setup will be deleted automatically when the corresponding role is deleted. 7.2 Delete IAM Roles\nGo to IAM Console ‚Üí Roles. Delete the following 17 roles: Lambda Execution Roles: CloudTrailETLLambdaServiceRole, GuardDutyETLLambdaServiceRole, CloudWatchETLLambdaServiceRole, CloudWatchENIETLLambdaServiceRole, CloudWatchExportLambdaServiceRole, ParseFindingsLambdaServiceRole, IsolateEC2LambdaServiceRole, QuarantineIAMLambdaServiceRole, AlertDispatchLambdaServiceRole. Service Roles: CloudTrailFirehoseRole, CloudWatchFirehoseRole, StepFunctionsRole, IncidentResponseStepFunctionsEventRole, FlowLogsIAMRole, GlueCloudWatchRole. Phase 4: S3 Bucket Cleanup (Data Deletion) 8. Empty and Delete S3 Buckets This is the final step to ensure all storage charges are stopped.\nBucket Name Purpose incident-response-log-list-bucket-ACCOUNT_ID-REGION Primary Log Source (CloudTrail/GuardDuty/Exported CW) processed-cloudtrail-logs-ACCOUNT_ID-REGION Firehose Destination for CloudTrail logs processed-cloudwatch-logs-ACCOUNT_ID-REGION Firehose Destination for VPC DNS/Flow logs processed-guardduty-findings-ACCOUNT_ID-REGION ETL Destination for GuardDuty logs athena-query-results-ACCOUNT_ID-REGION Athena Query Results Storage Go to the S3 Console. For each of the 5 buckets: Click on the bucket name. Go to the \u0026ldquo;Objects\u0026rdquo; tab. Click \u0026ldquo;Empty\u0026rdquo; to clear all data. You must confirm the permanent delete by typing permanently delete. Go back to the S3 bucket list, select the bucket, and click \u0026ldquo;Delete\u0026rdquo;. "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.11-appendices/5.11.10-step-functions-state-machine-definition/","title":"Steps Functions Definition ASL Code","tags":[],"description":"","content":" { \u0026#34;Comment\u0026#34;: \u0026#34;Guardduty Incident Response Automation\u0026#34;, \u0026#34;StartAt\u0026#34;: \u0026#34;CheckFindingType\u0026#34;, \u0026#34;States\u0026#34;: { \u0026#34;CheckFindingType\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Choice\u0026#34;, \u0026#34;Choices\u0026#34;: [ { \u0026#34;Comment\u0026#34;: \u0026#34;Check if EC2\u0026#34;, \u0026#34;Variable\u0026#34;: \u0026#34;$.detail.resource.resourceType\u0026#34;, \u0026#34;StringEquals\u0026#34;: \u0026#34;Instance\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;ParseFindings\u0026#34; }, { \u0026#34;Comment\u0026#34;: \u0026#34;Check if IAM\u0026#34;, \u0026#34;Variable\u0026#34;: \u0026#34;$.detail.resource.resourceType\u0026#34;, \u0026#34;StringEquals\u0026#34;: \u0026#34;AccessKey\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;Quarantine_IAM_User\u0026#34; } ], \u0026#34;Default\u0026#34;: \u0026#34;NoActionNeeded\u0026#34; }, \u0026#34;ParseFindings\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::lambda:invoke\u0026#34;, \u0026#34;OutputPath\u0026#34;: \u0026#34;$.Payload\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;Payload.$\u0026#34;: \u0026#34;$\u0026#34;, \u0026#34;FunctionName\u0026#34;: \u0026#34;arn:aws:lambda:ap-southeast-1:831981618496:function:ir-parse-findings-lambda\u0026#34; }, \u0026#34;Retry\u0026#34;: [ { \u0026#34;ErrorEquals\u0026#34;: [ \u0026#34;Lambda.ServiceException\u0026#34;, \u0026#34;Lambda.AWSLambdaException\u0026#34;, \u0026#34;Lambda.SdkClientException\u0026#34;, \u0026#34;Lambda.TooManyRequestsException\u0026#34; ], \u0026#34;IntervalSeconds\u0026#34;: 1, \u0026#34;MaxAttempts\u0026#34;: 3, \u0026#34;BackoffRate\u0026#34;: 2, \u0026#34;JitterStrategy\u0026#34;: \u0026#34;FULL\u0026#34; } ], \u0026#34;Next\u0026#34;: \u0026#34;Isolate_EC2_Instance\u0026#34; }, \u0026#34;Isolate_EC2_Instance\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::lambda:invoke\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;FunctionName\u0026#34;: \u0026#34;arn:aws:lambda:ap-southeast-1:831981618496:function:ir-isolate-ec2-lambda\u0026#34;, \u0026#34;Payload\u0026#34;: { \u0026#34;InstanceId.$\u0026#34;: \u0026#34;$.InstanceIds[0]\u0026#34;, \u0026#34;Region.$\u0026#34;: \u0026#34;$.Region\u0026#34; } }, \u0026#34;Retry\u0026#34;: [ { \u0026#34;ErrorEquals\u0026#34;: [ \u0026#34;Lambda.TooManyRequestsException\u0026#34;, \u0026#34;Lambda.ServiceException\u0026#34;, \u0026#34;Lambda.AWSLambdaException\u0026#34;, \u0026#34;Lambda.SdkClientException\u0026#34; ], \u0026#34;IntervalSeconds\u0026#34;: 2, \u0026#34;MaxAttempts\u0026#34;: 3, \u0026#34;BackoffRate\u0026#34;: 2 } ], \u0026#34;Next\u0026#34;: \u0026#34;CheckIsolationStatus\u0026#34;, \u0026#34;OutputPath\u0026#34;: \u0026#34;$.Payload\u0026#34; }, \u0026#34;CheckIsolationStatus\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Choice\u0026#34;, \u0026#34;Choices\u0026#34;: [ { \u0026#34;Variable\u0026#34;: \u0026#34;$.IsolationSG\u0026#34;, \u0026#34;IsNull\u0026#34;: true, \u0026#34;Next\u0026#34;: \u0026#34;AlreadyIsolated\u0026#34; } ], \u0026#34;Default\u0026#34;: \u0026#34;EnableTerminationProtection\u0026#34; }, \u0026#34;AlreadyIsolated\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Succeed\u0026#34; }, \u0026#34;EnableTerminationProtection\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:ec2:modifyInstanceAttribute\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;InstanceId.$\u0026#34;: \u0026#34;$.InstanceId\u0026#34;, \u0026#34;DisableApiTermination\u0026#34;: { \u0026#34;Value\u0026#34;: true } }, \u0026#34;Next\u0026#34;: \u0026#34;CreateQuarantineTag\u0026#34;, \u0026#34;ResultPath\u0026#34;: null }, \u0026#34;CreateQuarantineTag\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:ec2:createTags\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;Resources.$\u0026#34;: \u0026#34;States.Array($.InstanceId)\u0026#34;, \u0026#34;Tags\u0026#34;: [ { \u0026#34;Key\u0026#34;: \u0026#34;Quarantine\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;True\u0026#34; }, { \u0026#34;Key\u0026#34;: \u0026#34;Security Group\u0026#34;, \u0026#34;Value.$\u0026#34;: \u0026#34;$.IsolationSG\u0026#34; } ] }, \u0026#34;Next\u0026#34;: \u0026#34;DescribeInstanceASG\u0026#34;, \u0026#34;ResultPath\u0026#34;: null }, \u0026#34;DescribeInstanceASG\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:autoscaling:describeAutoScalingInstances\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;InstanceIds.$\u0026#34;: \u0026#34;States.Array($.InstanceId)\u0026#34; }, \u0026#34;ResultPath\u0026#34;: \u0026#34;$.ASGInfo\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;CheckIfASGExists\u0026#34; }, \u0026#34;CheckIfASGExists\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Choice\u0026#34;, \u0026#34;Choices\u0026#34;: [ { \u0026#34;Variable\u0026#34;: \u0026#34;$.ASGInfo.AutoScalingInstances[0]\u0026#34;, \u0026#34;IsPresent\u0026#34;: true, \u0026#34;Next\u0026#34;: \u0026#34;UpdateASGConfiguration\u0026#34; } ], \u0026#34;Default\u0026#34;: \u0026#34;DescribeVolumes\u0026#34; }, \u0026#34;UpdateASGConfiguration\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:autoscaling:updateAutoScalingGroup\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;AutoScalingGroupName.$\u0026#34;: \u0026#34;$.ASGInfo.AutoScalingInstances[0].AutoScalingGroupName\u0026#34;, \u0026#34;MinSize\u0026#34;: 0 }, \u0026#34;ResultPath\u0026#34;: null, \u0026#34;Next\u0026#34;: \u0026#34;Wait for ASG\u0026#34; }, \u0026#34;Wait for ASG\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Wait\u0026#34;, \u0026#34;Seconds\u0026#34;: 10, \u0026#34;Next\u0026#34;: \u0026#34;DetachFromASG\u0026#34; }, \u0026#34;DetachFromASG\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:autoscaling:detachInstances\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;AutoScalingGroupName.$\u0026#34;: \u0026#34;$.ASGInfo.AutoScalingInstances[0].AutoScalingGroupName\u0026#34;, \u0026#34;InstanceIds.$\u0026#34;: \u0026#34;States.Array($.InstanceId)\u0026#34;, \u0026#34;ShouldDecrementDesiredCapacity\u0026#34;: false }, \u0026#34;Retry\u0026#34;: [ { \u0026#34;ErrorEquals\u0026#34;: [ \u0026#34;AutoScaling.ValidationException\u0026#34; ], \u0026#34;IntervalSeconds\u0026#34;: 15, \u0026#34;MaxAttempts\u0026#34;: 3, \u0026#34;BackoffRate\u0026#34;: 2 } ], \u0026#34;ResultPath\u0026#34;: null, \u0026#34;Next\u0026#34;: \u0026#34;DescribeVolumes\u0026#34; }, \u0026#34;DescribeVolumes\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:ec2:describeVolumes\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;Filters\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;attachment.instance-id\u0026#34;, \u0026#34;Values.$\u0026#34;: \u0026#34;States.Array($.InstanceId)\u0026#34; } ] }, \u0026#34;ResultPath\u0026#34;: \u0026#34;$.VolumeInfo\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;CreateSnapshots\u0026#34; }, \u0026#34;CreateSnapshots\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Map\u0026#34;, \u0026#34;ItemsPath\u0026#34;: \u0026#34;$.VolumeInfo.Volumes\u0026#34;, \u0026#34;MaxConcurrency\u0026#34;: 1, \u0026#34;Iterator\u0026#34;: { \u0026#34;StartAt\u0026#34;: \u0026#34;Wait before calling CreateSnapshot API\u0026#34;, \u0026#34;States\u0026#34;: { \u0026#34;Wait before calling CreateSnapshot API\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Wait\u0026#34;, \u0026#34;Seconds\u0026#34;: 15, \u0026#34;Next\u0026#34;: \u0026#34;CreateSnapshot\u0026#34; }, \u0026#34;CreateSnapshot\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:ec2:createSnapshot\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;VolumeId.$\u0026#34;: \u0026#34;$.VolumeId\u0026#34;, \u0026#34;Description.$\u0026#34;: \u0026#34;States.Format(\u0026#39;IR Snapshot for {} - {}\u0026#39;, $.Attachments[0].InstanceId, $.VolumeId)\u0026#34;, \u0026#34;TagSpecifications\u0026#34;: [ { \u0026#34;ResourceType\u0026#34;: \u0026#34;snapshot\u0026#34;, \u0026#34;Tags\u0026#34;: [ { \u0026#34;Key\u0026#34;: \u0026#34;Quarantine\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;True\u0026#34; } ] } ] }, \u0026#34;Retry\u0026#34;: [ { \u0026#34;ErrorEquals\u0026#34;: [ \u0026#34;Ec2.RequestLimitExceeded\u0026#34; ], \u0026#34;IntervalSeconds\u0026#34;: 60, \u0026#34;MaxAttempts\u0026#34;: 3, \u0026#34;BackoffRate\u0026#34;: 2 } ], \u0026#34;End\u0026#34;: true } } }, \u0026#34;End\u0026#34;: true }, \u0026#34;Quarantine_IAM_User\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Choice\u0026#34;, \u0026#34;Choices\u0026#34;: [ { \u0026#34;Variable\u0026#34;: \u0026#34;$.detail.resource.accessKeyDetails.userType\u0026#34;, \u0026#34;StringEquals\u0026#34;: \u0026#34;Root\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;RootUserDetected\u0026#34; } ], \u0026#34;Default\u0026#34;: \u0026#34;ExecuteIAMQuarantine\u0026#34; }, \u0026#34;RootUserDetected\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Succeed\u0026#34;, \u0026#34;Comment\u0026#34;: \u0026#34;Cannot quarantine root user\u0026#34; }, \u0026#34;ExecuteIAMQuarantine\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::lambda:invoke\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;FunctionName\u0026#34;: \u0026#34;arn:aws:lambda:ap-southeast-1:831981618496:function:ir-quarantine-iam-lambda\u0026#34;, \u0026#34;Payload.$\u0026#34;: \u0026#34;$\u0026#34; }, \u0026#34;Retry\u0026#34;: [ { \u0026#34;ErrorEquals\u0026#34;: [ \u0026#34;Lambda.TooManyRequestsException\u0026#34;, \u0026#34;Lambda.ServiceException\u0026#34;, \u0026#34;Lambda.AWSLambdaException\u0026#34;, \u0026#34;Lambda.SdkClientException\u0026#34; ], \u0026#34;IntervalSeconds\u0026#34;: 2, \u0026#34;MaxAttempts\u0026#34;: 3, \u0026#34;BackoffRate\u0026#34;: 2 } ], \u0026#34;End\u0026#34;: true }, \u0026#34;NoActionNeeded\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Succeed\u0026#34; } } } "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Cost Optimization: Refactoring the Data Pipeline from Athena ETL to AWS Lambda to reduce operational costs. ChatOps Implementation: Integrating AWS GuardDuty with Slack for real-time threat monitoring. Operational Excellence: Customizing notification payloads for better visibility and handling \u0026ldquo;Alert Fatigue\u0026rdquo; incidents. Data Engineering: resolving schema mismatch issues by standardizing data format to JSONL. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon (10/11) Pipeline Refactoring (Cost Optimization)\n- Decision: Migrated from Athena ETL to AWS Lambda for log processing to minimize costs.\n- Issue: Encountered Schema Mismatch (field mix-up) where raw logs could not be correctly parsed into the target structure.\n- Status: Investigation in progress. 10/11/2025 10/11/2025 Lambda Pricing\nAthena Pricing Tue (11/11) ChatOps Integration \u0026amp; Incident Handling\n- Setup: Configured AWS Chatbot to integrate SNS with Slack Channel.\n- Incident: Triggered \u0026ldquo;Alert Storm\u0026rdquo; (1000+ emails) due to teammate activating all GuardDuty Sample Findings simultaneously.\n- Response: Tuned SNS subscription filters to manage noise. 11/11/2025 11/11/2025 GuardDuty Findings Wed (12/11) Notification Payload Optimization (Lambda)\n- UX Audit: Noticed default Slack notifications lacked critical context (Severity Level) in the preview/collapsed view.\n- Coding: Updated Lambda logic to parse raw findings and construct a Custom Message Payload.\n- Result: Important fields (Severity, Region, Type) are now visible immediately without opening the app. 12/11/2025 12/11/2025 Thu (13/11) Data Pipeline Remediation (Part 1)\n- Debugging: Analyzed the \u0026ldquo;field mix-up\u0026rdquo; error from Monday.\n- Root Cause: Standard JSON arrays were causing ingestion issues with Athena\u0026rsquo;s SerDe.\n- Solution Strategy: Decided to convert the data stream format to JSONL (Newline Delimited JSON). 13/11/2025 13/11/2025 Fri (14/11) Data Pipeline Remediation (Part 2)\n- Implementation: Updated Lambda logic to serialize processed logs as .jsonl files.\n- Validation: Successfully pushed cleaned data to S3 and manually queried via Athena without schema errors.\n- Outcome: Pipeline is now functional and cost-effective. 14/11/2025 14/11/2025 Sat (15/11) Event: ‚ÄúAWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS‚Äù\n- Introducing AI/ML/GenAI on AWS. 15/11/2025 15/11/2025 Week 10 Achievements: Engineered a Cost-Effective Data Pipeline:\nSuccessfully replaced the expensive Athena ETL approach with a lightweight Lambda-based solution. Solved the complex JSON Parsing/Schema Mismatch issue by implementing JSONL serialization, ensuring seamless data ingestion into the Data Lake. Established Robust ChatOps:\nBuilt a real-time threat notification system: GuardDuty $\\to$ EventBridge $\\to$ SNS $\\to$ AWS Chatbot $\\to$ Slack. UX Optimization: Custom-formatted notifications to display Severity immediately, significantly reducing the \u0026ldquo;Mean Time To Know\u0026rdquo; (MTTK) for security alerts. Incident Management Experience:\nLearned a valuable lesson on Alert Fatigue when handling 1000+ sample findings, reinforcing the importance of proper SNS filtering and testing protocols. "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/5-workshop/5.11-appendices/","title":"Appendices","tags":[],"description":"","content":"Appendices Lambda Codes: CloudTrail ETL GuardDuty ETL CloudWatch ETL CloudWatch ENI ETL CloudWatch Auto Export Parse Findings Isolate EC2 Instance Quarantine IAM Alert Dispatch Step Functions ASL Code: Step Functions ASL Code "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: ETL Automation: Enhancing the \u0026ldquo;CloudWatch ETL Lambda\u0026rdquo; to support automated schema evolution (Auto-Create Table) and Event-driven execution. Pipeline R\u0026amp;D: Investigating methods to automate CloudWatch Log Exports (Batch Ingestion) vs Streaming. Architectural Trade-offs: Evaluating EventBridge Scheduler vs Subscription Filters for data ingestion. Incident Analysis: Troubleshooting compatibility issues between Real-time Streaming (Subscription Filters) and Batch Processing formats. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon (17/11) Event: ‚ÄúAWS Cloud Mastery Series #2 - DevOps on AWS‚Äù\n- Introduce AWS DevOps Services. ‚Äì CI/CD Pipeline.\n- Introduce Infrastructure as Code (IaC) and related tools.\n- Introduce Container Services on AWS. - Ensure Monitoring and Observability capability using AWS Services 17/11/2025 17/11/2025 Tue (18/11) ETL Pipeline Upgrade (Event-Driven Architecture)\n- Feature: Upgraded CloudWatch ETL Lambda code to automatically detect schema and execute DDL (Create Table) if the Athena table is missing.\n- Automation: Configured S3 Event Notifications to trigger this Lambda immediately upon object creation (s3:ObjectCreated:*), removing the need for manual invocation. 18/11/2025 18/11/2025 Wed (19/11) Ingestion Gap Analysis (Auto-Export Research)\n- Gap: Identified that CloudWatch Logs requires manual initiation for S3 exports.\n- Research: Investigated Amazon EventBridge Scheduler to trigger the create_export_task API periodically.\n- Status: Proof of Concept (PoC) phase. 19/11/2025 19/11/2025 Thu (20/11) Architectural Pivot \u0026amp; Failure Analysis\n- Pivot: Attempted to use CloudWatch Subscription Filters as a simpler alternative to EventBridge.\n- Failure: The pipeline broke because Subscription Filters stream data (base64 encoded real-time stream), whereas the current ETL Lambda expects batch log files (.gz).\n- Decision: Reverted to the Batch Export strategy. 20/11/2025 20/11/2025 Subscription Filters Fri (21/11) Ingestion Remediation (EventBridge Retry)\n- Action: Resumed testing with EventBridge Scheduler to trigger Log Exports.\n- Challenge: Encountered IAM permission and Lambda timeout issues during the export trigger execution.\n- Outcome: Solution is currently unstable/pending resolution. Continued debugging over the weekend. 21/11/2025 22/11/2025 Week 11 Achievements: Advanced Lambda \u0026amp; Automation:\nSuccessfully transformed the ETL process into a fully Event-Driven workflow. Now, data is processed instantly as soon as it arrives in the Data Lake (S3), reducing data latency. Implemented Automated Schema Management within Lambda (Auto-Create Athena Table), reducing operational overhead when deploying to new environments. Architectural Deep Dive (Streaming vs Batch):\nGained a profound understanding of the difference between Log Streaming (Subscription Filters - Kinesis/Lambda) and Log Export (S3 Batch). Validated that \u0026ldquo;Streaming\u0026rdquo; requires a completely different processing logic (Stream decoding) compared to \u0026ldquo;Batching\u0026rdquo; (File parsing), leading to informed architectural decisions. "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Pipeline Re-architecture: Finalizing the transition from Batch Export to Real-time Log Streaming (Subscription Filters). Network Security: Configuring Lambda networking (ENI) to ensure secure data ingestion within VPC. Advanced R\u0026amp;D (NLP): Deep diving into Natural Language Processing (Vector Spaces \u0026amp; Probabilistic Models) for semantic log analysis. Industry Engagement: Updating knowledge on AWS DevOps, IaC, and Container services via technical workshops. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon (25/11) Architectural Pivot (Batch to Streaming)\n- Failure Analysis: Abandoned the CloudWatch to S3 to Lambda (Batch) approach due to API limits and high latency.\n- Success: Implemented CloudWatch Subscription Filters to trigger Lambda directly (CloudWatch to Lambda to S3).\n- Benefit: Achieved near real-time data ingestion latency. 25/11/2025 25/11/2025 Subscription Filters\nReal-time Log Processing Tue (26/11) Stream Processing Logic (Lambda)\n- Data Handling: Updated Lambda code to handle the streaming payload (Base64 encoded \u0026amp; Gzip compressed) from CloudWatch.\n- Transformation: Parsed raw log events and formatted them into JSON lines before buffering. 26/11/2025 26/11/2025 Decompressing Log Data Wed (27/11) Network Optimization \u0026amp; NLP Part 1\n- Infrastructure: Configured Lambda ENI to securely access S3 buckets within the VPC (using Gateway Endpoints).\n- Coursework: Completed NLP: Classification and Vector Spaces. Studied how to represent text logs as vectors for machine learning models. 27/11/2025 27/11/2025 Lambda VPC Access\nNLP Course 1 Thu (28/11) Advanced Analytics R\u0026amp;D (NLP Part 2)\n- Coursework: Advanced to NLP: Probabilistic Models. Investigated Auto-correct and N-gram language models to detect anomalies in log sequences.\n- Application: Evaluated potential integration of probabilistic models for predicting system failures based on log patterns. 28/11/2025 28/11/2025 NLP Course 2 Fri (29/11) Workshop \u0026amp; Capstone Planning\n- Event: Attended \u0026ldquo;AWS Cloud Mastery Series #3 - Security Pillar Workshop\u0026rdquo;.\n- Key Topics: CI/CD Pipeline, Infrastructure as Code (IaC), Container Services, and Observability.\n- Capstone: Researched technical proposal structures and started drafting the Final Project Proposal. 29/11/2025 29/11/2025 Week 12 Achievements: Successfully Architected a Real-time Pipeline:\nValidated that the Streaming Pattern (Subscription Filter) is superior to the Batch Pattern (Export Task) for this specific use case. Reduced data ingestion latency from minutes/hours to milliseconds. Advanced Skill Acquisition (AI/ML):\nLaid a strong academic foundation in Natural Language Processing (Vector Spaces, Probabilistic Models), preparing for the implementation of \u0026ldquo;Smart Log Analysis\u0026rdquo; features. Broadened Cloud Competency:\nGained comprehensive insights into the DevOps ecosystem on AWS (CI/CD, IaC, Containers) through the Cloud Mastery workshop, supporting the operational strategy for the final project. "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/1-worklog/1.13-week13/","title":"Week 13 Worklog","tags":[],"description":"","content":"Week 13 Objectives: Advanced Analytics Optimization: Implementing Athena Partition Projection to automate partition management and improve query performance. Pipeline Refinement: Migrating from SQS polling to Amazon Kinesis Data Firehose to resolve high-concurrency invocation issues. AI/ML Deep Dive: Mastering Sequence Models and Attention Mechanisms in NLP; exploring Agentic AI with Amazon Bedrock. Capstone Preparation: Finalizing presentation materials for the internship defense. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon (01/12) Project Management \u0026amp; NLP Study (Part 3)\n- Milestone: Submitted Final Project Proposal to the team for peer review.\n- Coursework: Advanced to NLP: Sequence Models. Studied RNNs (Recurrent Neural Networks) for processing sequential log data. 01/12/2025 01/12/2025 NLP Course 3 Tue (02/12) Operational Enablement \u0026amp; Lambda Refactoring\n- Knowledge Sharing: Produced a Video Tutorial on creating Slack Webhooks for ChatOps.\n- Code Update: Reconfigured Lambda ENI logic to align output format with the new Athena schema requirements. 02/12/2025 02/12/2025 Wed (03/12) Pipeline Optimization (Firehose \u0026amp; Athena)\n- Problem: SQS-triggered Lambda incurred excessive GET requests/invocations.\n- Solution: Pivoted to Kinesis Data Firehose for batching and buffering data before S3 ingestion.\n- Athena Optimization: Implemented Partition Projection for VPC Logs to automate partition management without MSCK REPAIR TABLE. 03/12/2025 03/12/2025 Athena Partition Projection Thu (04/12) Workshop Content Development\n- Task: Drafted the \u0026ldquo;Data Module\u0026rdquo; for the team\u0026rsquo;s internal workshop.\n- Content: Detailed the end-to-end flow of log data from ingestion (Firehose) to analysis (Athena). 04/12/2025 04/12/2025 Fri (05/12) Industry Engagement: GenAI \u0026amp; Agentic AI\n- Event: Attended \u0026ldquo;BUILDING AGENTIC AI - Context Optimization with Amazon Bedrock\u0026rdquo;.\n- Key Takeaway: Learned about RAG (Retrieval-Augmented Generation) and context management for AI Agents.\n- Action: Continued developing workshop materials. 05/12/2025 05/12/2025 Amazon Bedrock Sat (06/12) Advanced AI Research \u0026amp; Final Prep\n- Coursework: Progressed to NLP: Attention Models (Transformers/Self-Attention).\n- Presentation: Designed the Final Defense Slides, visualizing the architectural evolution (Batch to Streaming) and key technical achievements. 06/12/2025 06/12/2025 [NLP Course 4](https://www.coursera.org/programs/fptu-fall-2025-zmahp/lear ttention-models-in-nlp) Week 13 Achievements: Solved High-Scale Data Ingestion Issues:\nDiagnosed the bottleneck with SQS polling (High GET requests) and successfully re-architected the pipeline using Kinesis Data Firehose, reducing API costs and improving throughput. Mastered Athena Advanced Features:\nSuccessfully implemented Partition Projection (via DDL TBLPROPERTIES), eliminating the operational overhead of manually adding partitions. This ensures queries on vpc-logs are always up-to-date and performant. Project Completion Readiness:\nBridged the gap between traditional NLP and modern Generative AI, while simultaneously finalizing the documentation and presentation materials for the project defense. "},{"uri":"https://quanntm1206.github.io/AWS-FCJ/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://quanntm1206.github.io/AWS-FCJ/tags/","title":"Tags","tags":[],"description":"","content":""}]